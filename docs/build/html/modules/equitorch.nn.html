

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>equitorch.nn package &mdash; equitorch  documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../_static/css/custom.css" />

  
      <script src="../_static/jquery.js?v=5d32c60e"></script>
      <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../_static/documentation_options.js?v=5929fcd5"></script>
      <script src="../_static/doctools.js?v=9bcbadda"></script>
      <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="equitorch.nn.functional package" href="equitorch.nn.functional.html" />
    <link rel="prev" title="equitorch.irreps package" href="equitorch.irreps.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            equitorch
              <img src="../_static/logo_.png" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="equitorch.html">equitorch package</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="equitorch.html#subpackages">Subpackages</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="equitorch.constants.html">equitorch.constants package</a></li>
<li class="toctree-l3"><a class="reference internal" href="equitorch.irreps.html">equitorch.irreps package</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">equitorch.nn package</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#subpackages">Subpackages</a></li>
<li class="toctree-l4"><a class="reference internal" href="#submodules">Submodules</a></li>
<li class="toctree-l4"><a class="reference internal" href="#module-equitorch.nn.activations">equitorch.nn.activations module</a></li>
<li class="toctree-l4"><a class="reference internal" href="#module-equitorch.nn.angular">equitorch.nn.angular module</a></li>
<li class="toctree-l4"><a class="reference internal" href="#module-equitorch.nn.cutoffs">equitorch.nn.cutoffs module</a></li>
<li class="toctree-l4"><a class="reference internal" href="#module-equitorch.nn.dropout">equitorch.nn.dropout module</a></li>
<li class="toctree-l4"><a class="reference internal" href="#module-equitorch.nn.init">equitorch.nn.init module</a></li>
<li class="toctree-l4"><a class="reference internal" href="#module-equitorch.nn.linears">equitorch.nn.linears module</a></li>
<li class="toctree-l4"><a class="reference internal" href="#module-equitorch.nn.norm">equitorch.nn.norm module</a></li>
<li class="toctree-l4"><a class="reference internal" href="#module-equitorch.nn.normalization">equitorch.nn.normalization module</a></li>
<li class="toctree-l4"><a class="reference internal" href="#module-equitorch.nn.others">equitorch.nn.others module</a></li>
<li class="toctree-l4"><a class="reference internal" href="#module-equitorch.nn.radials">equitorch.nn.radials module</a></li>
<li class="toctree-l4"><a class="reference internal" href="#module-equitorch.nn.rotations">equitorch.nn.rotations module</a></li>
<li class="toctree-l4"><a class="reference internal" href="#module-equitorch.nn.sphericals">equitorch.nn.sphericals module</a></li>
<li class="toctree-l4"><a class="reference internal" href="#module-equitorch.nn.tensor_products">equitorch.nn.tensor_products module</a></li>
<li class="toctree-l4"><a class="reference internal" href="#module-equitorch.nn.wigner_d">equitorch.nn.wigner_d module</a></li>
<li class="toctree-l4"><a class="reference internal" href="#module-equitorch.nn">Module contents</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="equitorch.ops.html">equitorch.ops package</a></li>
<li class="toctree-l3"><a class="reference internal" href="equitorch.structs.html">equitorch.structs package</a></li>
<li class="toctree-l3"><a class="reference internal" href="equitorch.transforms.html">equitorch.transforms package</a></li>
<li class="toctree-l3"><a class="reference internal" href="equitorch.utils.html">equitorch.utils package</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="equitorch.html#module-equitorch">Module contents</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../api.html">API</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">equitorch</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="equitorch.html">equitorch package</a></li>
      <li class="breadcrumb-item active">equitorch.nn package</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/modules/equitorch.nn.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="equitorch-nn-package">
<h1>equitorch.nn package<a class="headerlink" href="#equitorch-nn-package" title="Link to this heading"></a></h1>
<section id="subpackages">
<h2>Subpackages<a class="headerlink" href="#subpackages" title="Link to this heading"></a></h2>
<div class="toctree-wrapper compound">
<ul>
<li class="toctree-l1"><a class="reference internal" href="equitorch.nn.functional.html">equitorch.nn.functional package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="equitorch.nn.functional.html#submodules">Submodules</a></li>
<li class="toctree-l2"><a class="reference internal" href="equitorch.nn.functional.html#module-equitorch.nn.functional.activations">equitorch.nn.functional.activations module</a><ul>
<li class="toctree-l3"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.activations.gating"><code class="docutils literal notranslate"><span class="pre">gating()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="equitorch.nn.functional.html#module-equitorch.nn.functional.angular">equitorch.nn.functional.angular module</a><ul>
<li class="toctree-l3"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.angular.sincos"><code class="docutils literal notranslate"><span class="pre">sincos()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="equitorch.nn.functional.html#module-equitorch.nn.functional.cutoffs">equitorch.nn.functional.cutoffs module</a></li>
<li class="toctree-l2"><a class="reference internal" href="equitorch.nn.functional.html#module-equitorch.nn.functional.dropout">equitorch.nn.functional.dropout module</a><ul>
<li class="toctree-l3"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.dropout.irrep_wise_dropout"><code class="docutils literal notranslate"><span class="pre">irrep_wise_dropout()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="equitorch.nn.functional.html#module-equitorch.nn.functional.linears">equitorch.nn.functional.linears module</a><ul>
<li class="toctree-l3"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.linears.TensorProductU1UDummy"><code class="docutils literal notranslate"><span class="pre">TensorProductU1UDummy</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.linears.TensorProductU1UDummy.backward"><code class="docutils literal notranslate"><span class="pre">TensorProductU1UDummy.backward()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.linears.tensor_product_u1u"><code class="docutils literal notranslate"><span class="pre">tensor_product_u1u()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.linears.so3_linear_uu"><code class="docutils literal notranslate"><span class="pre">so3_linear_uu()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.linears.TensorProduct1UUDummy"><code class="docutils literal notranslate"><span class="pre">TensorProduct1UUDummy</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.linears.TensorProduct1UUDummy.backward"><code class="docutils literal notranslate"><span class="pre">TensorProduct1UUDummy.backward()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.linears.tensor_product_1uu"><code class="docutils literal notranslate"><span class="pre">tensor_product_1uu()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.linears.TensorProductUU1Dummy"><code class="docutils literal notranslate"><span class="pre">TensorProductUU1Dummy</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.linears.TensorProductUU1Dummy.backward"><code class="docutils literal notranslate"><span class="pre">TensorProductUU1Dummy.backward()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.linears.tensor_product_uu1"><code class="docutils literal notranslate"><span class="pre">tensor_product_uu1()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.linears.TensorProductU1VDummy"><code class="docutils literal notranslate"><span class="pre">TensorProductU1VDummy</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.linears.TensorProductU1VDummy.backward"><code class="docutils literal notranslate"><span class="pre">TensorProductU1VDummy.backward()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.linears.tensor_product_u1v"><code class="docutils literal notranslate"><span class="pre">tensor_product_u1v()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.linears.so3_linear_uv"><code class="docutils literal notranslate"><span class="pre">so3_linear_uv()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.linears.TensorProduct1VUDummy"><code class="docutils literal notranslate"><span class="pre">TensorProduct1VUDummy</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.linears.TensorProduct1VUDummy.backward"><code class="docutils literal notranslate"><span class="pre">TensorProduct1VUDummy.backward()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.linears.tensor_product_1vu"><code class="docutils literal notranslate"><span class="pre">tensor_product_1vu()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.linears.TensorProductVU1Dummy"><code class="docutils literal notranslate"><span class="pre">TensorProductVU1Dummy</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.linears.TensorProductVU1Dummy.backward"><code class="docutils literal notranslate"><span class="pre">TensorProductVU1Dummy.backward()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.linears.tensor_product_vu1"><code class="docutils literal notranslate"><span class="pre">tensor_product_vu1()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.linears.IrrepWiseLinear"><code class="docutils literal notranslate"><span class="pre">IrrepWiseLinear</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.linears.IrrepWiseLinear.backward"><code class="docutils literal notranslate"><span class="pre">IrrepWiseLinear.backward()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.linears.IrrepsLinear"><code class="docutils literal notranslate"><span class="pre">IrrepsLinear</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.linears.IrrepsLinear.backward"><code class="docutils literal notranslate"><span class="pre">IrrepsLinear.backward()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="equitorch.nn.functional.html#module-equitorch.nn.functional.norm">equitorch.nn.functional.norm module</a><ul>
<li class="toctree-l3"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.norm.SquaredNorm"><code class="docutils literal notranslate"><span class="pre">SquaredNorm</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.norm.SquaredNorm.backward"><code class="docutils literal notranslate"><span class="pre">SquaredNorm.backward()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.norm.squared_norm"><code class="docutils literal notranslate"><span class="pre">squared_norm()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.norm.Norm"><code class="docutils literal notranslate"><span class="pre">Norm</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.norm.Norm.backward"><code class="docutils literal notranslate"><span class="pre">Norm.backward()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.norm.norm"><code class="docutils literal notranslate"><span class="pre">norm()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.norm.ChannelMeanSquaredNorm"><code class="docutils literal notranslate"><span class="pre">ChannelMeanSquaredNorm</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.norm.ChannelMeanSquaredNorm.backward"><code class="docutils literal notranslate"><span class="pre">ChannelMeanSquaredNorm.backward()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.norm.channel_mean_squared_norm"><code class="docutils literal notranslate"><span class="pre">channel_mean_squared_norm()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.norm.BatchMeanSquaredNorm"><code class="docutils literal notranslate"><span class="pre">BatchMeanSquaredNorm</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.norm.BatchMeanSquaredNorm.backward"><code class="docutils literal notranslate"><span class="pre">BatchMeanSquaredNorm.backward()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.norm.batch_mean_squared_norm"><code class="docutils literal notranslate"><span class="pre">batch_mean_squared_norm()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="equitorch.nn.functional.html#module-equitorch.nn.functional.normalization">equitorch.nn.functional.normalization module</a><ul>
<li class="toctree-l3"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.normalization.batch_rms_norm"><code class="docutils literal notranslate"><span class="pre">batch_rms_norm()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.normalization.layer_rms_norm"><code class="docutils literal notranslate"><span class="pre">layer_rms_norm()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="equitorch.nn.functional.html#module-equitorch.nn.functional.rotations">equitorch.nn.functional.rotations module</a><ul>
<li class="toctree-l3"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.rotations.angles_to_matrix"><code class="docutils literal notranslate"><span class="pre">angles_to_matrix()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="equitorch.nn.functional.html#module-equitorch.nn.functional.sparse_product">equitorch.nn.functional.sparse_product module</a><ul>
<li class="toctree-l3"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.sparse_product.SparseMul"><code class="docutils literal notranslate"><span class="pre">SparseMul</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.sparse_product.SparseMul.backward"><code class="docutils literal notranslate"><span class="pre">SparseMul.backward()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.sparse_product.sparse_mul"><code class="docutils literal notranslate"><span class="pre">sparse_mul()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.sparse_product.SparseOuter"><code class="docutils literal notranslate"><span class="pre">SparseOuter</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.sparse_product.SparseOuter.backward"><code class="docutils literal notranslate"><span class="pre">SparseOuter.backward()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.sparse_product.sparse_outer"><code class="docutils literal notranslate"><span class="pre">sparse_outer()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.sparse_product.SparseInner"><code class="docutils literal notranslate"><span class="pre">SparseInner</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.sparse_product.SparseInner.backward"><code class="docutils literal notranslate"><span class="pre">SparseInner.backward()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.sparse_product.sparse_inner"><code class="docutils literal notranslate"><span class="pre">sparse_inner()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.sparse_product.SparseVecMat"><code class="docutils literal notranslate"><span class="pre">SparseVecMat</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.sparse_product.SparseVecMat.backward"><code class="docutils literal notranslate"><span class="pre">SparseVecMat.backward()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.sparse_product.sparse_vecmat"><code class="docutils literal notranslate"><span class="pre">sparse_vecmat()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.sparse_product.SparseVecSca"><code class="docutils literal notranslate"><span class="pre">SparseVecSca</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.sparse_product.SparseVecSca.backward"><code class="docutils literal notranslate"><span class="pre">SparseVecSca.backward()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.sparse_product.sparse_vecsca"><code class="docutils literal notranslate"><span class="pre">sparse_vecsca()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.sparse_product.SparseScaVec"><code class="docutils literal notranslate"><span class="pre">SparseScaVec</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.sparse_product.SparseScaVec.backward"><code class="docutils literal notranslate"><span class="pre">SparseScaVec.backward()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.sparse_product.sparse_scavec"><code class="docutils literal notranslate"><span class="pre">sparse_scavec()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.sparse_product.SparseMatTVec"><code class="docutils literal notranslate"><span class="pre">SparseMatTVec</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.sparse_product.SparseMatTVec.backward"><code class="docutils literal notranslate"><span class="pre">SparseMatTVec.backward()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.sparse_product.sparse_mat_t_vec"><code class="docutils literal notranslate"><span class="pre">sparse_mat_t_vec()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="equitorch.nn.functional.html#module-equitorch.nn.functional.sparse_scale">equitorch.nn.functional.sparse_scale module</a><ul>
<li class="toctree-l3"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.sparse_scale.SparseScale"><code class="docutils literal notranslate"><span class="pre">SparseScale</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.sparse_scale.SparseScale.backward"><code class="docutils literal notranslate"><span class="pre">SparseScale.backward()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.sparse_scale.sparse_scale"><code class="docutils literal notranslate"><span class="pre">sparse_scale()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="equitorch.nn.functional.html#module-equitorch.nn.functional.sphericals">equitorch.nn.functional.sphericals module</a><ul>
<li class="toctree-l3"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.sphericals.spherical_harmonics"><code class="docutils literal notranslate"><span class="pre">spherical_harmonics()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.sphericals.xyz_to_spherical"><code class="docutils literal notranslate"><span class="pre">xyz_to_spherical()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.sphericals.spherical_to_xyz"><code class="docutils literal notranslate"><span class="pre">spherical_to_xyz()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.sphericals.xyz_to_sincos"><code class="docutils literal notranslate"><span class="pre">xyz_to_sincos()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="equitorch.nn.functional.html#module-equitorch.nn.functional.tensor_products">equitorch.nn.functional.tensor_products module</a><ul>
<li class="toctree-l3"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.tensor_products.TensorProductUUUDummy"><code class="docutils literal notranslate"><span class="pre">TensorProductUUUDummy</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.tensor_products.TensorProductUUUDummy.backward"><code class="docutils literal notranslate"><span class="pre">TensorProductUUUDummy.backward()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.tensor_products.tensor_product_uuu"><code class="docutils literal notranslate"><span class="pre">tensor_product_uuu()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.tensor_products.TensorProductUVWDummy"><code class="docutils literal notranslate"><span class="pre">TensorProductUVWDummy</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.tensor_products.TensorProductUVWDummy.backward"><code class="docutils literal notranslate"><span class="pre">TensorProductUVWDummy.backward()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.tensor_products.tensor_product_uvw"><code class="docutils literal notranslate"><span class="pre">tensor_product_uvw()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.tensor_products.TensorDotUU"><code class="docutils literal notranslate"><span class="pre">TensorDotUU</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.tensor_products.TensorDotUU.backward"><code class="docutils literal notranslate"><span class="pre">TensorDotUU.backward()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.tensor_products.TensorDotUV"><code class="docutils literal notranslate"><span class="pre">TensorDotUV</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.tensor_products.TensorDotUV.backward"><code class="docutils literal notranslate"><span class="pre">TensorDotUV.backward()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="equitorch.nn.functional.html#module-equitorch.nn.functional.wigner_d">equitorch.nn.functional.wigner_d module</a><ul>
<li class="toctree-l3"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.wigner_d.sparse_wigner_rotation"><code class="docutils literal notranslate"><span class="pre">sparse_wigner_rotation()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.wigner_d.dense_wigner_rotation"><code class="docutils literal notranslate"><span class="pre">dense_wigner_rotation()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.wigner_d.wigner_d_matrix"><code class="docutils literal notranslate"><span class="pre">wigner_d_matrix()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.wigner_d.align_to_z_wigner_d"><code class="docutils literal notranslate"><span class="pre">align_to_z_wigner_d()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="equitorch.nn.functional.html#module-equitorch.nn.functional">Module contents</a><ul>
<li class="toctree-l3"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.tensor_product_u1u"><code class="docutils literal notranslate"><span class="pre">tensor_product_u1u()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.so3_linear_uu"><code class="docutils literal notranslate"><span class="pre">so3_linear_uu()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.tensor_product_1uu"><code class="docutils literal notranslate"><span class="pre">tensor_product_1uu()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.tensor_product_uu1"><code class="docutils literal notranslate"><span class="pre">tensor_product_uu1()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.tensor_product_u1v"><code class="docutils literal notranslate"><span class="pre">tensor_product_u1v()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.so3_linear_uv"><code class="docutils literal notranslate"><span class="pre">so3_linear_uv()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.tensor_product_1vu"><code class="docutils literal notranslate"><span class="pre">tensor_product_1vu()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.tensor_product_vu1"><code class="docutils literal notranslate"><span class="pre">tensor_product_vu1()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.irrep_wise_linear"><code class="docutils literal notranslate"><span class="pre">irrep_wise_linear()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.irreps_linear"><code class="docutils literal notranslate"><span class="pre">irreps_linear()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.so2_linear_uu"><code class="docutils literal notranslate"><span class="pre">so2_linear_uu()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.so2_linear_uv"><code class="docutils literal notranslate"><span class="pre">so2_linear_uv()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.spherical_harmonics"><code class="docutils literal notranslate"><span class="pre">spherical_harmonics()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.xyz_to_spherical"><code class="docutils literal notranslate"><span class="pre">xyz_to_spherical()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.spherical_to_xyz"><code class="docutils literal notranslate"><span class="pre">spherical_to_xyz()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.xyz_to_sincos"><code class="docutils literal notranslate"><span class="pre">xyz_to_sincos()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.batch_rms_norm"><code class="docutils literal notranslate"><span class="pre">batch_rms_norm()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.layer_rms_norm"><code class="docutils literal notranslate"><span class="pre">layer_rms_norm()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.sparse_scale"><code class="docutils literal notranslate"><span class="pre">sparse_scale()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.tensor_product_uuu"><code class="docutils literal notranslate"><span class="pre">tensor_product_uuu()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.tensor_product_uvw"><code class="docutils literal notranslate"><span class="pre">tensor_product_uvw()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.tensor_dot_uu"><code class="docutils literal notranslate"><span class="pre">tensor_dot_uu()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.tensor_dot_uv"><code class="docutils literal notranslate"><span class="pre">tensor_dot_uv()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.sparse_mul"><code class="docutils literal notranslate"><span class="pre">sparse_mul()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.sparse_outer"><code class="docutils literal notranslate"><span class="pre">sparse_outer()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.sparse_inner"><code class="docutils literal notranslate"><span class="pre">sparse_inner()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.sparse_vecmat"><code class="docutils literal notranslate"><span class="pre">sparse_vecmat()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.sparse_vecsca"><code class="docutils literal notranslate"><span class="pre">sparse_vecsca()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.sparse_scavec"><code class="docutils literal notranslate"><span class="pre">sparse_scavec()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.sparse_mat_t_vec"><code class="docutils literal notranslate"><span class="pre">sparse_mat_t_vec()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.sparse_wigner_rotation"><code class="docutils literal notranslate"><span class="pre">sparse_wigner_rotation()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.dense_wigner_rotation"><code class="docutils literal notranslate"><span class="pre">dense_wigner_rotation()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.wigner_d_matrix"><code class="docutils literal notranslate"><span class="pre">wigner_d_matrix()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.align_to_z_wigner_d"><code class="docutils literal notranslate"><span class="pre">align_to_z_wigner_d()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.gating"><code class="docutils literal notranslate"><span class="pre">gating()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.sincos"><code class="docutils literal notranslate"><span class="pre">sincos()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.squared_norm"><code class="docutils literal notranslate"><span class="pre">squared_norm()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.norm"><code class="docutils literal notranslate"><span class="pre">norm()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.channel_mean_squared_norm"><code class="docutils literal notranslate"><span class="pre">channel_mean_squared_norm()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.batch_mean_squared_norm"><code class="docutils literal notranslate"><span class="pre">batch_mean_squared_norm()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.irrep_wise_dropout"><code class="docutils literal notranslate"><span class="pre">irrep_wise_dropout()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.angles_to_matrix"><code class="docutils literal notranslate"><span class="pre">angles_to_matrix()</span></code></a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</section>
<section id="submodules">
<h2>Submodules<a class="headerlink" href="#submodules" title="Link to this heading"></a></h2>
</section>
<section id="module-equitorch.nn.activations">
<span id="equitorch-nn-activations-module"></span><h2>equitorch.nn.activations module<a class="headerlink" href="#module-equitorch.nn.activations" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="equitorch.nn.activations.Gate">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">equitorch.nn.activations.</span></span><span class="sig-name descname"><span class="pre">Gate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">irreps</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="equitorch.irreps.html#equitorch.irreps.Irreps" title="equitorch.irreps.Irreps"><span class="pre">Irreps</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">irrep_wise</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#equitorch.nn.activations.Gate" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Applies element-wise gates to equivariant features.</p>
<p>This module implements gating nonlinearities for features represented by <a class="reference internal" href="equitorch.irreps.html#equitorch.irreps.Irreps" title="equitorch.irreps.Irreps"><code class="xref py py-class docutils literal notranslate"><span class="pre">Irreps</span></code></a>.
It can operate in two primary modes based on how the gate values are provided:</p>
<ol class="arabic simple">
<li><p><strong>Separate Gates (``gate`` argument provided):</strong>
The module takes two distinct inputs: <code class="docutils literal notranslate"><span class="pre">input</span></code> (the features to be gated) and <code class="docutils literal notranslate"><span class="pre">gate</span></code> (the gate scalars).</p>
<ul class="simple">
<li><p>If <code class="docutils literal notranslate"><span class="pre">irrep_wise=True</span></code> (default): Each gate scalar in the <code class="docutils literal notranslate"><span class="pre">gate</span></code> tensor is applied to its corresponding
irrep block within the <code class="docutils literal notranslate"><span class="pre">input</span></code> features. The <code class="docutils literal notranslate"><span class="pre">gate</span></code> tensor should have a shape compatible with
<code class="docutils literal notranslate"><span class="pre">(...,</span> <span class="pre">num_gates,</span> <span class="pre">channels)</span></code>, where <code class="docutils literal notranslate"><span class="pre">num_gates</span></code> is the number of irreps in <code class="docutils literal notranslate"><span class="pre">irreps</span></code>.</p></li>
<li><p>If <code class="docutils literal notranslate"><span class="pre">irrep_wise=False</span></code>: A single gate scalar (or a set of scalars broadcastable across irreps)
is applied to all irrep blocks in the <code class="docutils literal notranslate"><span class="pre">input</span></code> features. The <code class="docutils literal notranslate"><span class="pre">gate</span></code> tensor should have a shape
compatible with <code class="docutils literal notranslate"><span class="pre">(...,</span> <span class="pre">1,</span> <span class="pre">channels)</span></code>.</p></li>
</ul>
</li>
<li><p><strong>Concatenated Input (``gate=None``):</strong>
The module takes a single <code class="docutils literal notranslate"><span class="pre">input</span></code> tensor where the features and their corresponding gate scalars
are concatenated along the spherical dimension (<code class="docutils literal notranslate"><span class="pre">dim=-2</span></code>). The last <code class="docutils literal notranslate"><span class="pre">num_gates</span></code> slices along this
dimension are interpreted as the gate scalars. The <code class="docutils literal notranslate"><span class="pre">input</span></code> tensor shape is expected to be
<code class="docutils literal notranslate"><span class="pre">(...,</span> <span class="pre">irreps.dim</span> <span class="pre">+</span> <span class="pre">num_gates,</span> <span class="pre">channels)</span></code>.
The module internally splits this tensor into features and gates, optionally applies an activation
function to the extracted gates, and then proceeds with the gating operation as described in mode 1.</p></li>
</ol>
<p>An optional activation function can be applied to the gate scalars before they modulate the features.</p>
<p class="rubric">Example</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">equitorch.irreps</span><span class="w"> </span><span class="kn">import</span> <span class="n">Irreps</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">equitorch.nn</span><span class="w"> </span><span class="kn">import</span> <span class="n">Gate</span>

<span class="n">irreps</span> <span class="o">=</span> <span class="n">Irreps</span><span class="p">(</span><span class="s2">&quot;1x0e + 2x1o&quot;</span><span class="p">)</span> <span class="c1"># Example: one scalar, two l=1 odd irreps</span>
<span class="n">gate_module</span> <span class="o">=</span> <span class="n">Gate</span><span class="p">(</span><span class="n">irreps</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Tanh</span><span class="p">())</span>

<span class="n">batch_size</span><span class="p">,</span> <span class="n">channels</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">8</span>
<span class="n">num_gates</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">irreps</span><span class="p">)</span> <span class="c1"># This will be 2 for the example irreps</span>

<span class="c1"># Mode 1: Separate input and gate tensors (irrep_wise=True)</span>
<span class="n">features</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">irreps</span><span class="o">.</span><span class="n">dim</span><span class="p">,</span> <span class="n">channels</span><span class="p">)</span>
<span class="n">gates</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_gates</span><span class="p">,</span> <span class="n">channels</span><span class="p">)</span>
<span class="n">output_separate</span> <span class="o">=</span> <span class="n">gate_module</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">gates</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Output shape (separate gates): </span><span class="si">{</span><span class="n">output_separate</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Mode 2: Concatenated input tensor</span>
<span class="c1"># irreps.dim for &quot;1x0e + 2x1o&quot; is 1*1 + 2*3 = 7</span>
<span class="c1"># num_gates is 2</span>
<span class="n">concatenated_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">irreps</span><span class="o">.</span><span class="n">dim</span> <span class="o">+</span> <span class="n">num_gates</span><span class="p">,</span> <span class="n">channels</span><span class="p">)</span>
<span class="n">output_concatenated</span> <span class="o">=</span> <span class="n">gate_module</span><span class="p">(</span><span class="n">concatenated_input</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Output shape (concatenated input): </span><span class="si">{</span><span class="n">output_concatenated</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>irreps</strong> (<a class="reference internal" href="equitorch.irreps.html#equitorch.irreps.Irreps" title="equitorch.irreps.Irreps"><em>Irreps</em></a>) – The irreducible representations of the feature part of the input tensor
(i.e., the part that will be gated).</p></li>
<li><p><strong>activation</strong> (<em>torch.nn.Module</em><em>, </em><em>optional</em>) – An activation function to be applied to the
gate scalars before the gating operation. Defaults to <code class="docutils literal notranslate"><span class="pre">None</span></code> (no activation).</p></li>
<li><p><strong>irrep_wise</strong> (<em>bool</em><em>, </em><em>optional</em>) – Determines how gates are applied.
If <code class="docutils literal notranslate"><span class="pre">True</span></code> (default), gates are applied irrep-by-irrep. This requires the <code class="docutils literal notranslate"><span class="pre">gate</span></code>
tensor (if provided separately) to have a shape like <code class="docutils literal notranslate"><span class="pre">(...,</span> <span class="pre">num_gates,</span> <span class="pre">channels)</span></code>.
If <code class="docutils literal notranslate"><span class="pre">False</span></code>, a single gate (or a broadcastable set) is applied across all irreps.
This requires the <code class="docutils literal notranslate"><span class="pre">gate</span></code> tensor (if provided separately) to have a shape like
<code class="docutils literal notranslate"><span class="pre">(...,</span> <span class="pre">1,</span> <span class="pre">channels)</span></code>. <code class="docutils literal notranslate"><span class="pre">num_gates</span></code> corresponds to <code class="docutils literal notranslate"><span class="pre">len(irreps)</span></code>.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="equitorch.nn.activations.Gate.irreps_info">
<span class="sig-name descname"><span class="pre">irreps_info</span></span><a class="headerlink" href="#equitorch.nn.activations.Gate.irreps_info" title="Link to this definition"></a></dt>
<dd><p>Cached information about the input feature irreps, used for efficient gating.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="equitorch.structs.html#equitorch.structs.IrrepsInfo" title="equitorch.structs.IrrepsInfo">IrrepsInfo</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="equitorch.nn.activations.Gate.num_gates">
<span class="sig-name descname"><span class="pre">num_gates</span></span><a class="headerlink" href="#equitorch.nn.activations.Gate.num_gates" title="Link to this definition"></a></dt>
<dd><p>The number of distinct gate scalars, equal to <code class="docutils literal notranslate"><span class="pre">len(irreps)</span></code>.
This dictates the expected size of the gate dimension in the <code class="docutils literal notranslate"><span class="pre">gate</span></code> tensor
or the number of gate slices in a concatenated input.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="id0">
<span class="sig-name descname"><span class="pre">irreps_info</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="equitorch.structs.html#equitorch.structs.IrrepsInfo" title="equitorch.structs.IrrepsInfo"><span class="pre">IrrepsInfo</span></a></em><a class="headerlink" href="#id0" title="Link to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="module-equitorch.nn.angular">
<span id="equitorch-nn-angular-module"></span><h2>equitorch.nn.angular module<a class="headerlink" href="#module-equitorch.nn.angular" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="equitorch.nn.angular.SinCos">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">equitorch.nn.angular.</span></span><span class="sig-name descname"><span class="pre">SinCos</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">max_m</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">with_ones</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">component_normalize</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#equitorch.nn.angular.SinCos" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Module wrapper for the <a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.angular.sincos" title="equitorch.nn.functional.angular.sincos"><code class="xref py py-func docutils literal notranslate"><span class="pre">sincos()</span></code></a> function.</p>
<p>Computes the sin/cos expansion of an angle (a):</p>
<div class="math notranslate nohighlight">
\[[1.0, \sin(a), \cos(a), \sin(2a), \cos(2a), \dots, \sin(\text{max_m} \cdot a), \cos(\text{max_m} \cdot a)]\]</div>
<p>or</p>
<div class="math notranslate nohighlight">
\[[1.0, \sqrt{2}\sin(a), \sqrt{2}\cos(a), \sqrt{2}\sin(2a), \sqrt{2}\cos(2a), \dots, \sqrt{2}\sin(\text{max_m} \cdot a), \sqrt{2}\cos(\text{max_m} \cdot a)]\]</div>
<p>The leading 1.0 is excluded if <cite>with_ones</cite> is <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>max_m</strong> (<em>int</em>) – The maximum multiple of the angle (a) to compute (sin) and (cos) for.</p></li>
<li><p><strong>with_ones</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether to include the leading 1.0 in the expansion. Defaults to <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p></li>
<li><p><strong>component_normalize</strong> (<em>bool</em><em>, </em><em>optional</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, multiplies the (sin) and (cos) values by (sqrt{2})
such that the expectation of the squared norm over ([0, 2pi]) is 1.
Defaults to <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
<section id="module-equitorch.nn.cutoffs">
<span id="equitorch-nn-cutoffs-module"></span><h2>equitorch.nn.cutoffs module<a class="headerlink" href="#module-equitorch.nn.cutoffs" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="equitorch.nn.cutoffs.PolynomialCutoff">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">equitorch.nn.cutoffs.</span></span><span class="sig-name descname"><span class="pre">PolynomialCutoff</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">r_max</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">r_min</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">p</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">6</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#equitorch.nn.cutoffs.PolynomialCutoff" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Polynomial cutoff, as proposed in <a class="reference external" href="https://arxiv.org/abs/2003.03123">DimeNet</a>.</p>
<p>The polynomial cutoff function is defined as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}f(r) = \begin{cases}
1, &amp; r &lt; r_{\text{min}} \\
1 - \frac{(p+1)(p+2)}{2}u^p + p(p+2)u^{p+1} - \frac{p(p+1)}{2}u^{p+2}, &amp; r_{\text{min}} \leq r \leq r_{\text{max}} \\
0, &amp; r &gt; r_{\text{max}}
\end{cases}\end{split}\]</div>
<p>where (u = frac{r - r_{text{min}}}{r_{text{max}} - r_{text{min}}}) and (r) is the input distance.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>r_max</strong> (<em>float</em>) – The cutoff distance (r_{text{max}}) where the function reaches zero.</p></li>
<li><p><strong>r_min</strong> (<em>float</em><em>, </em><em>optional</em>) – The starting distance (r_{text{min}}) where the function begins to decrease from 1.
Must be less than or equal to <code class="docutils literal notranslate"><span class="pre">r_max</span></code>. Defaults to <code class="docutils literal notranslate"><span class="pre">0.</span></code>.</p></li>
<li><p><strong>p</strong> (<em>float</em><em>, </em><em>optional</em>) – The power parameter (p) controlling the smoothness of the cutoff.
Must be greater than or equal to <code class="docutils literal notranslate"><span class="pre">2.0</span></code>. Defaults to <code class="docutils literal notranslate"><span class="pre">6.</span></code>.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="equitorch.nn.cutoffs.CosineCutoff">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">equitorch.nn.cutoffs.</span></span><span class="sig-name descname"><span class="pre">CosineCutoff</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">r_max</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">r_min</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#equitorch.nn.cutoffs.CosineCutoff" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>The cosine cutoff function.</p>
<p>The cosine cutoff function is defined as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}f(r) = \begin{cases}
1, &amp; r &lt; r_{\text{min}} \\
\frac{1}{2}\left[1 + \cos\left(\pi \cdot u\right)\right], &amp; r_{\text{min}} \leq r \leq r_{\text{max}} \\
0, &amp; r &gt; r_{\text{max}}
\end{cases}\end{split}\]</div>
<p>where (u = frac{r - r_{text{min}}}{r_{text{max}} - r_{text{min}}}) and (r) is the input distance.</p>
<p>This cutoff function smoothly decreases from 1 to 0 in the range
[r_{text{min}}, r_{text{max}}] using a cosine function.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>r_max</strong> (<em>float</em>) – The cutoff distance (r_{text{max}}) where the function reaches zero.</p></li>
<li><p><strong>r_min</strong> (<em>float</em><em>, </em><em>optional</em>) – The starting distance (r_{text{min}}) where the function begins to decrease from 1.
Must be less than <code class="docutils literal notranslate"><span class="pre">r_max</span></code>. Defaults to <code class="docutils literal notranslate"><span class="pre">0.</span></code>.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="equitorch.nn.cutoffs.MollifierCutoff">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">equitorch.nn.cutoffs.</span></span><span class="sig-name descname"><span class="pre">MollifierCutoff</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">r_max</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">r_min</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1e-07</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#equitorch.nn.cutoffs.MollifierCutoff" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>The mollifier cutoff function.</p>
<p>The mollifier cutoff function is defined as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}f(r) = \begin{cases}
1, &amp; r &lt; r_{\text{min}} \\
\exp\left(1 - \frac{1}{1 - u^2 + \epsilon}\right), &amp; r_{\text{min}} \leq r \leq r_{\text{max}} \\
0, &amp; r &gt; r_{\text{max}}
\end{cases}\end{split}\]</div>
<p>where (u = frac{r - r_{text{min}}}{r_{text{max}} - r_{text{min}}}) and (r) is the input distance.</p>
<p>This cutoff function smoothly decreases from 1 to 0 in the range
[r_{text{min}}, r_{text{max}}] using a mollifier (bump) function.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>r_max</strong> (<em>float</em>) – The cutoff distance (r_{text{max}}) where the function reaches zero.</p></li>
<li><p><strong>r_min</strong> (<em>float</em><em>, </em><em>optional</em>) – The starting distance (r_{text{min}}) where the function begins to decrease from 1.
Must be less than <code class="docutils literal notranslate"><span class="pre">r_max</span></code>. Defaults to <code class="docutils literal notranslate"><span class="pre">0.</span></code>.</p></li>
<li><p><strong>eps</strong> (<em>float</em><em>, </em><em>optional</em>) – Small epsilon value (epsilon) to prevent division by zero.
Defaults to <code class="docutils literal notranslate"><span class="pre">1e-7</span></code>.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
<section id="module-equitorch.nn.dropout">
<span id="equitorch-nn-dropout-module"></span><h2>equitorch.nn.dropout module<a class="headerlink" href="#module-equitorch.nn.dropout" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="equitorch.nn.dropout.Dropout">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">equitorch.nn.dropout.</span></span><span class="sig-name descname"><span class="pre">Dropout</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">p</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">irreps</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="equitorch.irreps.html#equitorch.irreps.Irreps" title="equitorch.irreps.Irreps"><span class="pre">Irreps</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">irrep_wise</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">work_on_eval</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#equitorch.nn.dropout.Dropout" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Apply dropout to equivariant features.</p>
<p>Can operate irrep-wise or on the entire feature vector (channel-wise).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>p</strong> (<em>float</em><em>, </em><em>optional</em>) – Probability of an element to be zeroed.
Default: 0.5</p></li>
<li><p><strong>irreps</strong> (<a class="reference internal" href="equitorch.irreps.html#equitorch.irreps.Irreps" title="equitorch.irreps.Irreps"><em>Irreps</em></a><em>, </em><em>optional</em>) – Irreps of the input tensor.
Required if <cite>irrep_wise</cite> is True.
Default: None</p></li>
<li><p><strong>irrep_wise</strong> (<em>bool</em><em>, </em><em>optional</em>) – If True, applies dropout independently
for each (irrep_instance, channel).
If False, applies standard 1D dropout
treating (irreps_dim, channels) as a
single feature dimension for dropout.
Default: True</p></li>
<li><p><strong>work_on_eval</strong> (<em>bool</em><em>, </em><em>optional</em>) – If True, dropout is applied even during
evaluation. Default: False</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
<section id="module-equitorch.nn.init">
<span id="equitorch-nn-init-module"></span><h2>equitorch.nn.init module<a class="headerlink" href="#module-equitorch.nn.init" title="Link to this heading"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="equitorch.nn.init.initialize_tensor_product">
<span class="sig-prename descclassname"><span class="pre">equitorch.nn.init.</span></span><span class="sig-name descname"><span class="pre">initialize_tensor_product</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">weight</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">feature_mode</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gain</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">channel_normed</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#equitorch.nn.init.initialize_tensor_product" title="Link to this definition"></a></dt>
<dd><p>Initialize weights for tensor product operations.</p>
<p>This function initializes weights for tensor product operations with different
feature modes. The initialization uses a uniform distribution with bounds
calculated based on the feature mode and whether channel normalization is used.</p>
<p>For ‘uvw’ mode:</p>
<div class="math notranslate nohighlight">
\[\begin{split}a = \begin{cases}
\sqrt{3} \cdot \text{gain}, &amp; \text{if channel_normed} = \text{True} \\
\sqrt{\frac{3}{\text{fan_in}}} \cdot \text{gain}, &amp; \text{otherwise}
\end{cases}\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\text{fan_in} = \text{weight.shape[-2]} \cdot \text{weight.shape[-3]}\)</span></p>
<p>For ‘uuu’ mode:</p>
<div class="math notranslate nohighlight">
\[a = \sqrt{3} \cdot \text{gain}\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>weight</strong> (<em>torch.Tensor</em>) – The weight tensor to initialize.</p></li>
<li><p><strong>feature_mode</strong> (<em>str</em>) – The feature mode for initialization. Must be one of [‘uvw’, ‘uuu’].</p></li>
<li><p><strong>gain</strong> (<em>float</em><em>, </em><em>optional</em>) – The gain factor to apply. Default is 1.</p></li>
<li><p><strong>channel_normed</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether channel normalization is used. Default is False.</p></li>
</ul>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>ValueError</strong> – If an unknown feature_mode is provided.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="equitorch.nn.init.initialize_so3_so2_linear">
<span class="sig-prename descclassname"><span class="pre">equitorch.nn.init.</span></span><span class="sig-name descname"><span class="pre">initialize_so3_so2_linear</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">weight</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">feature_mode</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gain</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">channel_normed</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#equitorch.nn.init.initialize_so3_so2_linear" title="Link to this definition"></a></dt>
<dd><p>Initialize weights for SO(3) or SO(2) linear operations.</p>
<p>This function initializes weights for SO(3) or SO(2) linear operations with different
feature modes. The initialization uses a uniform distribution with bounds
calculated based on the feature mode and whether channel normalization is used.</p>
<p>For ‘uv’ mode:</p>
<div class="math notranslate nohighlight">
\[\begin{split}a = \begin{cases}
\sqrt{3} \cdot \text{gain}, &amp; \text{if channel_normed} = \text{True} \\
\sqrt{\frac{3}{\text{fan_in}}} \cdot \text{gain}, &amp; \text{otherwise}
\end{cases}\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\text{fan_in} = \text{weight.shape[-2]}\)</span></p>
<p>For ‘uu’ mode:</p>
<div class="math notranslate nohighlight">
\[a = \sqrt{3} \cdot \text{gain}\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>weight</strong> (<em>torch.Tensor</em>) – The weight tensor to initialize.</p></li>
<li><p><strong>feature_mode</strong> (<em>str</em>) – The feature mode for initialization. Must be one of [‘uv’, ‘uu’].</p></li>
<li><p><strong>gain</strong> (<em>float</em><em>, </em><em>optional</em>) – The gain factor to apply. Default is 1.</p></li>
<li><p><strong>channel_normed</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether channel normalization is used. Default is False.</p></li>
</ul>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>ValueError</strong> – If an unknown feature_mode is provided.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="equitorch.nn.init.initialize_linear">
<span class="sig-prename descclassname"><span class="pre">equitorch.nn.init.</span></span><span class="sig-name descname"><span class="pre">initialize_linear</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">weight</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gain</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">channel_normed</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#equitorch.nn.init.initialize_linear" title="Link to this definition"></a></dt>
<dd><p>Initialize weights for standard linear operations.</p>
<p>This function initializes weights for standard linear operations using a uniform
distribution with bounds calculated based on whether channel normalization is used.</p>
<div class="math notranslate nohighlight">
\[\begin{split}a = \begin{cases}
\sqrt{3} \cdot \text{gain}, &amp; \text{if channel_normed} = \text{True} \\
\sqrt{\frac{3}{\text{fan_in}}} \cdot \text{gain}, &amp; \text{otherwise}
\end{cases}\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\text{fan_in} = \text{weight.shape[-2]}\)</span></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>weight</strong> (<em>torch.Tensor</em>) – The weight tensor to initialize.</p></li>
<li><p><strong>gain</strong> (<em>float</em><em>, </em><em>optional</em>) – The gain factor to apply. Default is 1.</p></li>
<li><p><strong>channel_normed</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether channel normalization is used. Default is False.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
<section id="module-equitorch.nn.linears">
<span id="equitorch-nn-linears-module"></span><h2>equitorch.nn.linears module<a class="headerlink" href="#module-equitorch.nn.linears" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="equitorch.nn.linears.SO3Linear">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">equitorch.nn.linears.</span></span><span class="sig-name descname"><span class="pre">SO3Linear</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">irreps_in1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">irreps_in2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">irreps_out</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">channels_in</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">channels_out</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">internal_weights</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">feature_mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'uu'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">path_norm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">channel_norm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">path</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#equitorch.nn.linears.SO3Linear" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>SO(3) equivariant linear layer using tensor products.</p>
<p>Equivalent to a TensorProduct where  <code class="docutils literal notranslate"><span class="pre">input2</span></code> does not have a channel dimension.</p>
<p>Supports two main modes controlled by <code class="docutils literal notranslate"><span class="pre">feature_mode</span></code>:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">'uv'</span></code>: Fully connected in channel dimension.</p>
<ul>
<li><p>Input1 shape: <code class="docutils literal notranslate"><span class="pre">(...,</span> <span class="pre">irreps_in1.dim,</span> <span class="pre">channels_in)</span></code></p></li>
<li><p>Input2 shape: <code class="docutils literal notranslate"><span class="pre">(...,</span> <span class="pre">irreps_in2.dim)</span></code></p></li>
<li><p>Weight shape: <code class="docutils literal notranslate"><span class="pre">(num_paths,</span> <span class="pre">channels_in,</span> <span class="pre">channels_out)</span></code></p></li>
<li><p>Output shape: <code class="docutils literal notranslate"><span class="pre">(...,</span> <span class="pre">irreps_out.dim,</span> <span class="pre">channels_out)</span></code></p></li>
</ul>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">'uu'</span></code>: Depthwise/elementwise in channel dimension.</p>
<ul>
<li><p>Input1 shape: <code class="docutils literal notranslate"><span class="pre">(...,</span> <span class="pre">irreps_in1.dim,</span> <span class="pre">channels)</span></code></p></li>
<li><p>Input2 shape: <code class="docutils literal notranslate"><span class="pre">(...,</span> <span class="pre">irreps_in2.dim)</span></code></p></li>
<li><p>Weight shape: <code class="docutils literal notranslate"><span class="pre">(num_paths,</span> <span class="pre">channels)</span></code></p></li>
<li><p>Output shape: <code class="docutils literal notranslate"><span class="pre">(...,</span> <span class="pre">irreps_out.dim,</span> <span class="pre">channels)</span></code></p></li>
</ul>
</li>
</ul>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>irreps_in1</strong> (<a class="reference internal" href="equitorch.irreps.html#equitorch.irreps.Irreps" title="equitorch.irreps.Irreps"><em>Irreps</em></a>) – Irreducible representations of the main input tensor (<code class="docutils literal notranslate"><span class="pre">input1</span></code>).</p></li>
<li><p><strong>irreps_in2</strong> (<a class="reference internal" href="equitorch.irreps.html#equitorch.irreps.Irreps" title="equitorch.irreps.Irreps"><em>Irreps</em></a>) – Irreducible representations of the second input tensor (<code class="docutils literal notranslate"><span class="pre">input2</span></code>),
often representing weights like spherical harmonics.</p></li>
<li><p><strong>irreps_out</strong> (<a class="reference internal" href="equitorch.irreps.html#equitorch.irreps.Irreps" title="equitorch.irreps.Irreps"><em>Irreps</em></a>) – Irreducible representations of the output tensor.</p></li>
<li><p><strong>channels_in</strong> (<em>int</em><em>, </em><em>optional</em>) – Number of channels for the main input (<code class="docutils literal notranslate"><span class="pre">input1</span></code>).
Required if <code class="docutils literal notranslate"><span class="pre">internal_weights=True</span></code>.</p></li>
<li><p><strong>channels_out</strong> (<em>int</em><em>, </em><em>optional</em>) – Number of channels for the output.
Required if <code class="docutils literal notranslate"><span class="pre">internal_weights=True</span></code>.</p></li>
<li><p><strong>internal_weights</strong> (<em>bool</em><em>, </em><em>optional</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, the module manages its own weight parameter.
If <code class="docutils literal notranslate"><span class="pre">False</span></code>, weights must be provided during the forward pass. Defaults to <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p></li>
<li><p><strong>feature_mode</strong> (<em>str</em><em>, </em><em>optional</em>) – <p>Controls the type of linear operation: <code class="docutils literal notranslate"><span class="pre">{'uu',</span> <span class="pre">'uv'}</span></code>.
Defaults to <code class="docutils literal notranslate"><span class="pre">'uu'</span></code>.</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">'uu'</span></code>: Depthwise/elementwise linear. Assumes <code class="docutils literal notranslate"><span class="pre">channels_in</span> <span class="pre">==</span> <span class="pre">channels_out</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">'uv'</span></code>: Fully connected linear.</p></li>
</ul>
</p></li>
<li><p><strong>path_norm</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether to apply path normalization to the weights.
Normalizes by the square root of the number of paths to each output irrep. Defaults to <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p></li>
<li><p><strong>channel_norm</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether to apply channel normalization (specific to <code class="docutils literal notranslate"><span class="pre">'uv'</span></code> mode).
Divides weights by (sqrt{text{channels_in}}). Note: This interacts with <code class="docutils literal notranslate"><span class="pre">path_norm</span></code>.
Defaults to <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p><strong>path</strong> (<em>list</em><em>, </em><em>optional</em>) – Manually specify the coupling paths.
If <code class="docutils literal notranslate"><span class="pre">None</span></code>, all allowed paths are used. Defaults to <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="equitorch.nn.linears.SO3Linear.weight">
<span class="sig-name descname"><span class="pre">weight</span></span><a class="headerlink" href="#equitorch.nn.linears.SO3Linear.weight" title="Link to this definition"></a></dt>
<dd><p>The learnable weights of the module if <code class="docutils literal notranslate"><span class="pre">internal_weights=True</span></code>.
Shape depends on <code class="docutils literal notranslate"><span class="pre">feature_mode</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.nn.Parameter or None</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="equitorch.nn.linears.SO3Linear.tp_info_forward">
<span class="sig-name descname"><span class="pre">tp_info_forward</span></span><a class="headerlink" href="#equitorch.nn.linears.SO3Linear.tp_info_forward" title="Link to this definition"></a></dt>
<dd><p>Constant information for the forward pass computation.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="equitorch.structs.html#equitorch.structs.TensorProductInfo" title="equitorch.structs.TensorProductInfo">TensorProductInfo</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="equitorch.nn.linears.SO3Linear.tp_info_backward1">
<span class="sig-name descname"><span class="pre">tp_info_backward1</span></span><a class="headerlink" href="#equitorch.nn.linears.SO3Linear.tp_info_backward1" title="Link to this definition"></a></dt>
<dd><p>Constant information for the backward pass w.r.t. <code class="docutils literal notranslate"><span class="pre">input1</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="equitorch.structs.html#equitorch.structs.TensorProductInfo" title="equitorch.structs.TensorProductInfo">TensorProductInfo</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="equitorch.nn.linears.SO3Linear.tp_info_backward2">
<span class="sig-name descname"><span class="pre">tp_info_backward2</span></span><a class="headerlink" href="#equitorch.nn.linears.SO3Linear.tp_info_backward2" title="Link to this definition"></a></dt>
<dd><p>Constant information for the backward pass w.r.t. <code class="docutils literal notranslate"><span class="pre">input2</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="equitorch.structs.html#equitorch.structs.TensorProductInfo" title="equitorch.structs.TensorProductInfo">TensorProductInfo</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="equitorch.nn.linears.SO3Linear.num_paths">
<span class="sig-name descname"><span class="pre">num_paths</span></span><a class="headerlink" href="#equitorch.nn.linears.SO3Linear.num_paths" title="Link to this definition"></a></dt>
<dd><p>Number of coupling paths determined by the irreps.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="equitorch.nn.linears.SO3Linear.weight_numel">
<span class="sig-name descname"><span class="pre">weight_numel</span></span><a class="headerlink" href="#equitorch.nn.linears.SO3Linear.weight_numel" title="Link to this definition"></a></dt>
<dd><p>Total number of elements in the weight tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="id1">
<span class="sig-name descname"><span class="pre">tp_info_forward</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="equitorch.structs.html#equitorch.structs.TensorProductInfo" title="equitorch.structs.TensorProductInfo"><span class="pre">TensorProductInfo</span></a></em><a class="headerlink" href="#id1" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="id2">
<span class="sig-name descname"><span class="pre">tp_info_backward1</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="equitorch.structs.html#equitorch.structs.TensorProductInfo" title="equitorch.structs.TensorProductInfo"><span class="pre">TensorProductInfo</span></a></em><a class="headerlink" href="#id2" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="id3">
<span class="sig-name descname"><span class="pre">tp_info_backward2</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="equitorch.structs.html#equitorch.structs.TensorProductInfo" title="equitorch.structs.TensorProductInfo"><span class="pre">TensorProductInfo</span></a></em><a class="headerlink" href="#id3" title="Link to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="equitorch.nn.linears.IrrepWiseLinear">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">equitorch.nn.linears.</span></span><span class="sig-name descname"><span class="pre">IrrepWiseLinear</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">irreps</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">channels_in</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">channels_out</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">internal_weights</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">channel_norm</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#equitorch.nn.linears.IrrepWiseLinear" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Irrep-wise linear layer (channel mixing).</p>
<p>Applies a separate linear transformation to the channels associated with each irrep type.
This operation does not change the spherical tensor structure (the irreps).</p>
<ul class="simple">
<li><p>Input shape: <code class="docutils literal notranslate"><span class="pre">(...,</span> <span class="pre">irreps.dim,</span> <span class="pre">channels_in)</span></code></p></li>
<li><p>Weight shape: <code class="docutils literal notranslate"><span class="pre">(num_paths,</span> <span class="pre">channels_in,</span> <span class="pre">channels_out)</span></code> where <code class="docutils literal notranslate"><span class="pre">num_paths</span></code> is the number of unique irreps in <code class="docutils literal notranslate"><span class="pre">irreps</span></code>.</p></li>
<li><p>Output shape: <code class="docutils literal notranslate"><span class="pre">(...,</span> <span class="pre">irreps.dim,</span> <span class="pre">channels_out)</span></code></p></li>
</ul>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>irreps</strong> (<a class="reference internal" href="equitorch.irreps.html#equitorch.irreps.Irreps" title="equitorch.irreps.Irreps"><em>Irreps</em></a><em> or </em><em>str</em>) – Irreducible representations of the input tensor.</p></li>
<li><p><strong>channels_in</strong> (<em>int</em>) – Number of input channels.</p></li>
<li><p><strong>channels_out</strong> (<em>int</em>) – Number of output channels.</p></li>
<li><p><strong>internal_weights</strong> (<em>bool</em><em>, </em><em>optional</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, the module manages its own weight parameter.
If <code class="docutils literal notranslate"><span class="pre">False</span></code>, weights must be provided during the forward pass. Defaults to <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p></li>
<li><p><strong>channel_norm</strong> (<em>bool</em><em>, </em><em>optional</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, divides the output by (sqrt{text{channels_in}}).
Defaults to <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="equitorch.nn.linears.IrrepWiseLinear.weight">
<span class="sig-name descname"><span class="pre">weight</span></span><a class="headerlink" href="#equitorch.nn.linears.IrrepWiseLinear.weight" title="Link to this definition"></a></dt>
<dd><p>The learnable weights of the module if <code class="docutils literal notranslate"><span class="pre">internal_weights=True</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.nn.Parameter or None</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="equitorch.nn.linears.IrrepWiseLinear.irreps_info">
<span class="sig-name descname"><span class="pre">irreps_info</span></span><a class="headerlink" href="#equitorch.nn.linears.IrrepWiseLinear.irreps_info" title="Link to this definition"></a></dt>
<dd><p>Constant information about the input irreps.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="equitorch.structs.html#equitorch.structs.IrrepsInfo" title="equitorch.structs.IrrepsInfo">IrrepsInfo</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="equitorch.nn.linears.IrrepWiseLinear.num_paths">
<span class="sig-name descname"><span class="pre">num_paths</span></span><a class="headerlink" href="#equitorch.nn.linears.IrrepWiseLinear.num_paths" title="Link to this definition"></a></dt>
<dd><p>Number of unique irreps in the input.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="equitorch.nn.linears.IrrepWiseLinear.weight_numel">
<span class="sig-name descname"><span class="pre">weight_numel</span></span><a class="headerlink" href="#equitorch.nn.linears.IrrepWiseLinear.weight_numel" title="Link to this definition"></a></dt>
<dd><p>Total number of elements in the weight tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="id4">
<span class="sig-name descname"><span class="pre">irreps_info</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="equitorch.structs.html#equitorch.structs.IrrepsInfo" title="equitorch.structs.IrrepsInfo"><span class="pre">IrrepsInfo</span></a></em><a class="headerlink" href="#id4" title="Link to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="equitorch.nn.linears.IrrepsLinear">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">equitorch.nn.linears.</span></span><span class="sig-name descname"><span class="pre">IrrepsLinear</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">irreps_in</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">irreps_out</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">channels_in</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">channels_out</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">internal_weights</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">path_norm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">channel_norm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">path</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#equitorch.nn.linears.IrrepsLinear" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Equivariant linear layer that preserves the spherical tensor structure but mixes channels.</p>
<p>This layer applies a linear transformation across channels while respecting the
equivariance constraints imposed by the input and output irreps. It only allows
paths where the input and output irreps are the same ((l_{in} = l_{out}) and ((p_{in} = p_{out}) or (p_{out}=0))).
This is often used for channel mixing in equivariant networks.</p>
<ul class="simple">
<li><p>Input shape: <code class="docutils literal notranslate"><span class="pre">(...,</span> <span class="pre">irreps_in.dim,</span> <span class="pre">channels_in)</span></code></p></li>
<li><p>Weight shape: <code class="docutils literal notranslate"><span class="pre">(num_paths,</span> <span class="pre">channels_in,</span> <span class="pre">channels_out)</span></code> where <code class="docutils literal notranslate"><span class="pre">num_paths</span></code> is the number of allowed paths.</p></li>
<li><p>Output shape: <code class="docutils literal notranslate"><span class="pre">(...,</span> <span class="pre">irreps_out.dim,</span> <span class="pre">channels_out)</span></code></p></li>
</ul>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>irreps_in</strong> (<a class="reference internal" href="equitorch.irreps.html#equitorch.irreps.Irreps" title="equitorch.irreps.Irreps"><em>Irreps</em></a>) – Irreducible representations of the input tensor.</p></li>
<li><p><strong>irreps_out</strong> (<a class="reference internal" href="equitorch.irreps.html#equitorch.irreps.Irreps" title="equitorch.irreps.Irreps"><em>Irreps</em></a>) – Irreducible representations of the output tensor.</p></li>
<li><p><strong>channels_in</strong> (<em>int</em>) – Number of input channels.</p></li>
<li><p><strong>channels_out</strong> (<em>int</em>) – Number of output channels.</p></li>
<li><p><strong>internal_weights</strong> (<em>bool</em><em>, </em><em>optional</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, the module manages its own weight parameter.
If <code class="docutils literal notranslate"><span class="pre">False</span></code>, weights must be provided during the forward pass. Defaults to <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p></li>
<li><p><strong>path_norm</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether to apply path normalization to the weights.
Normalizes by the square root of the number of paths to each output irrep. Defaults to <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p></li>
<li><p><strong>channel_norm</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether to apply channel normalization.
Divides weights by (sqrt{text{channels_in}}).
Defaults to <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p><strong>path</strong> (<em>list</em><em>, </em><em>optional</em>) – Manually specify the coupling paths.
If <code class="docutils literal notranslate"><span class="pre">None</span></code>, all allowed paths are used. Defaults to <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="equitorch.nn.linears.IrrepsLinear.weight">
<span class="sig-name descname"><span class="pre">weight</span></span><a class="headerlink" href="#equitorch.nn.linears.IrrepsLinear.weight" title="Link to this definition"></a></dt>
<dd><p>The learnable weights of the module if <code class="docutils literal notranslate"><span class="pre">internal_weights=True</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.nn.Parameter or None</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="equitorch.nn.linears.IrrepsLinear.forward_info">
<span class="sig-name descname"><span class="pre">forward_info</span></span><a class="headerlink" href="#equitorch.nn.linears.IrrepsLinear.forward_info" title="Link to this definition"></a></dt>
<dd><p>Constant information for the forward pass computation.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="equitorch.structs.html#equitorch.structs.IrrepsLinearInfo" title="equitorch.structs.IrrepsLinearInfo">IrrepsLinearInfo</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="equitorch.nn.linears.IrrepsLinear.backward_info">
<span class="sig-name descname"><span class="pre">backward_info</span></span><a class="headerlink" href="#equitorch.nn.linears.IrrepsLinear.backward_info" title="Link to this definition"></a></dt>
<dd><p>Constant information for the backward pass computation.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="equitorch.structs.html#equitorch.structs.IrrepsLinearInfo" title="equitorch.structs.IrrepsLinearInfo">IrrepsLinearInfo</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="equitorch.nn.linears.IrrepsLinear.num_paths">
<span class="sig-name descname"><span class="pre">num_paths</span></span><a class="headerlink" href="#equitorch.nn.linears.IrrepsLinear.num_paths" title="Link to this definition"></a></dt>
<dd><p>Number of allowed coupling paths.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="equitorch.nn.linears.IrrepsLinear.weight_numel">
<span class="sig-name descname"><span class="pre">weight_numel</span></span><a class="headerlink" href="#equitorch.nn.linears.IrrepsLinear.weight_numel" title="Link to this definition"></a></dt>
<dd><p>Total number of elements in the weight tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="id5">
<span class="sig-name descname"><span class="pre">forward_info</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="equitorch.structs.html#equitorch.structs.IrrepsLinearInfo" title="equitorch.structs.IrrepsLinearInfo"><span class="pre">IrrepsLinearInfo</span></a></em><a class="headerlink" href="#id5" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="id6">
<span class="sig-name descname"><span class="pre">backward_info</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="equitorch.structs.html#equitorch.structs.IrrepsLinearInfo" title="equitorch.structs.IrrepsLinearInfo"><span class="pre">IrrepsLinearInfo</span></a></em><a class="headerlink" href="#id6" title="Link to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="equitorch.nn.linears.SO2Linear">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">equitorch.nn.linears.</span></span><span class="sig-name descname"><span class="pre">SO2Linear</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">irreps_in</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">irreps_out</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">channels_in</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">channels_out</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">internal_weights</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">feature_mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'uu'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">path_norm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">channel_norm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">path</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#equitorch.nn.linears.SO2Linear" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>SO(2) equivariant linear layer using tensor products.</p>
<p>This layer applies an SO(2) equivariant linear transformation, as proposed in <a class="reference external" href="https://arxiv.org/abs/2302.03655">Reducing SO(3) Convolutions to SO(2) for Efficient Equivariant GNNs</a>.
It supports two main modes controlled by <code class="docutils literal notranslate"><span class="pre">feature_mode</span></code>:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">'uv'</span></code>: Fully connected linear layer.</p>
<ul>
<li><p>Input shape: <code class="docutils literal notranslate"><span class="pre">(...,</span> <span class="pre">irreps_in.dim,</span> <span class="pre">channels_in)</span></code></p></li>
<li><p>Weight shape: <code class="docutils literal notranslate"><span class="pre">(num_weights,</span> <span class="pre">channels_in,</span> <span class="pre">channels_out)</span></code></p></li>
<li><p>Output shape: <code class="docutils literal notranslate"><span class="pre">(...,</span> <span class="pre">irreps_out.dim,</span> <span class="pre">channels_out)</span></code></p></li>
</ul>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">'uu'</span></code>: Depthwise/elementwise linear layer.</p>
<ul>
<li><p>Input shape: <code class="docutils literal notranslate"><span class="pre">(...,</span> <span class="pre">irreps_in.dim,</span> <span class="pre">channels)</span></code></p></li>
<li><p>Weight shape: <code class="docutils literal notranslate"><span class="pre">(num_weights,</span> <span class="pre">channels_out)</span></code></p></li>
<li><p>Output shape: <code class="docutils literal notranslate"><span class="pre">(...,</span> <span class="pre">irreps_out.dim,</span> <span class="pre">channels_out)</span></code></p></li>
</ul>
</li>
</ul>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>irreps_in</strong> (<a class="reference internal" href="equitorch.irreps.html#equitorch.irreps.Irreps" title="equitorch.irreps.Irreps"><em>Irreps</em></a><em> or </em><em>str</em>) – Irreducible representations of the input tensor.</p></li>
<li><p><strong>irreps_out</strong> (<a class="reference internal" href="equitorch.irreps.html#equitorch.irreps.Irreps" title="equitorch.irreps.Irreps"><em>Irreps</em></a><em> or </em><em>str</em>) – Irreducible representations of the output tensor.</p></li>
<li><p><strong>channels_in</strong> (<em>int</em><em>, </em><em>optional</em>) – Number of channels for the input.
Required if <code class="docutils literal notranslate"><span class="pre">internal_weights=True</span></code>.</p></li>
<li><p><strong>channels_out</strong> (<em>int</em><em>, </em><em>optional</em>) – Number of channels for the output.
Required if <code class="docutils literal notranslate"><span class="pre">internal_weights=True</span></code>.</p></li>
<li><p><strong>internal_weights</strong> (<em>bool</em><em>, </em><em>optional</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, the module manages its own weight parameter.
If <code class="docutils literal notranslate"><span class="pre">False</span></code>, weights must be provided during the forward pass. Defaults to <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p></li>
<li><p><strong>feature_mode</strong> (<em>str</em><em>, </em><em>optional</em>) – <p>Controls the type of linear operation: <code class="docutils literal notranslate"><span class="pre">{'uu',</span> <span class="pre">'uv'}</span></code>.
Defaults to <code class="docutils literal notranslate"><span class="pre">'uu'</span></code>.</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">'uu'</span></code>: Depthwise/elementwise linear. Assumes <code class="docutils literal notranslate"><span class="pre">channels_in</span> <span class="pre">==</span> <span class="pre">channels_out</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">'uv'</span></code>: Fully connected linear.</p></li>
</ul>
</p></li>
<li><p><strong>path_norm</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether to apply path normalization to the weights.
Normalizes by the square root of the number of paths to each output irrep. Defaults to <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p></li>
<li><p><strong>channel_norm</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether to apply channel normalization (specific to <code class="docutils literal notranslate"><span class="pre">'uv'</span></code> mode).
Divides weights by (sqrt{text{channels_in}}). Note: This interacts with <code class="docutils literal notranslate"><span class="pre">path_norm</span></code>.
Defaults to <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p><strong>path</strong> (<em>list</em><em>, </em><em>optional</em>) – Manually specify the coupling paths.
If <code class="docutils literal notranslate"><span class="pre">None</span></code>, all allowed paths are used. Defaults to <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="equitorch.nn.linears.SO2Linear.weight">
<span class="sig-name descname"><span class="pre">weight</span></span><a class="headerlink" href="#equitorch.nn.linears.SO2Linear.weight" title="Link to this definition"></a></dt>
<dd><p>The learnable weights of the module if <code class="docutils literal notranslate"><span class="pre">internal_weights=True</span></code>.
Shape depends on <code class="docutils literal notranslate"><span class="pre">feature_mode</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.nn.Parameter or None</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="equitorch.nn.linears.SO2Linear.info_forward">
<span class="sig-name descname"><span class="pre">info_forward</span></span><a class="headerlink" href="#equitorch.nn.linears.SO2Linear.info_forward" title="Link to this definition"></a></dt>
<dd><p>Constant information for the forward pass computation.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="equitorch.structs.html#equitorch.structs.SparseProductInfo" title="equitorch.structs.SparseProductInfo">SparseProductInfo</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="equitorch.nn.linears.SO2Linear.info_backward1">
<span class="sig-name descname"><span class="pre">info_backward1</span></span><a class="headerlink" href="#equitorch.nn.linears.SO2Linear.info_backward1" title="Link to this definition"></a></dt>
<dd><p>Constant information for the first backward pass.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="equitorch.structs.html#equitorch.structs.SparseProductInfo" title="equitorch.structs.SparseProductInfo">SparseProductInfo</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="equitorch.nn.linears.SO2Linear.info_backward2">
<span class="sig-name descname"><span class="pre">info_backward2</span></span><a class="headerlink" href="#equitorch.nn.linears.SO2Linear.info_backward2" title="Link to this definition"></a></dt>
<dd><p>Constant information for the second backward pass.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="equitorch.structs.html#equitorch.structs.SparseProductInfo" title="equitorch.structs.SparseProductInfo">SparseProductInfo</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="equitorch.nn.linears.SO2Linear.num_paths">
<span class="sig-name descname"><span class="pre">num_paths</span></span><a class="headerlink" href="#equitorch.nn.linears.SO2Linear.num_paths" title="Link to this definition"></a></dt>
<dd><p>Number of coupling paths determined by the irreps.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="equitorch.nn.linears.SO2Linear.weight_numel">
<span class="sig-name descname"><span class="pre">weight_numel</span></span><a class="headerlink" href="#equitorch.nn.linears.SO2Linear.weight_numel" title="Link to this definition"></a></dt>
<dd><p>Total number of elements in the weight tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="module-equitorch.nn.norm">
<span id="equitorch-nn-norm-module"></span><h2>equitorch.nn.norm module<a class="headerlink" href="#module-equitorch.nn.norm" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="equitorch.nn.norm.SquaredNorm">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">equitorch.nn.norm.</span></span><span class="sig-name descname"><span class="pre">SquaredNorm</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">irreps</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="equitorch.irreps.html#equitorch.irreps.Irreps" title="equitorch.irreps.Irreps"><span class="pre">Irreps</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">scaled</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#equitorch.nn.norm.SquaredNorm" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Computes the squared L2 norm for each irrep block in an input tensor.</p>
<div class="math notranslate nohighlight">
\[\text{Output}_k = \sum_{m \in \text{irrep}_k} (\text{input}_{km}^2)\]</div>
<p>Optionally scales the output by <span class="math notranslate nohighlight">\(1/\text{irrep}_k\text{.dim}\)</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>irreps</strong> (<a class="reference internal" href="equitorch.irreps.html#equitorch.irreps.Irreps" title="equitorch.irreps.Irreps"><em>Irreps</em></a>) – Irreducible representations of the input tensor.</p></li>
<li><p><strong>scaled</strong> (<em>bool</em><em>, </em><em>optional</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, scales the output of each <code class="docutils literal notranslate"><span class="pre">irrep_k</span></code>
by <span class="math notranslate nohighlight">\(1/\text{irrep}_k\text{.dim}\)</span>.
Defaults to <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="equitorch.nn.norm.SquaredNorm.irreps_info">
<span class="sig-name descname"><span class="pre">irreps_info</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="equitorch.structs.html#equitorch.structs.IrrepsInfo" title="equitorch.structs.IrrepsInfo"><span class="pre">IrrepsInfo</span></a></em><a class="headerlink" href="#equitorch.nn.norm.SquaredNorm.irreps_info" title="Link to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="equitorch.nn.norm.Norm">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">equitorch.nn.norm.</span></span><span class="sig-name descname"><span class="pre">Norm</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">irreps</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="equitorch.irreps.html#equitorch.irreps.Irreps" title="equitorch.irreps.Irreps"><span class="pre">Irreps</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">scaled</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#equitorch.nn.norm.Norm" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Computes the L2 norm for each irrep block in an input tensor.</p>
<div class="math notranslate nohighlight">
\[\text{Output}_k = \sqrt{\sum_{m \in \text{irrep}_k} (\text{input}_{km}^2)}\]</div>
<p>Optionally scales the output by <span class="math notranslate nohighlight">\(\sqrt{1/\text{irrep}_k\text{.dim}}\)</span>.
Gradient at zero vector is zero.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>irreps</strong> (<a class="reference internal" href="equitorch.irreps.html#equitorch.irreps.Irreps" title="equitorch.irreps.Irreps"><em>Irreps</em></a>) – Irreducible representations of the input tensor.</p></li>
<li><p><strong>scaled</strong> (<em>bool</em><em>, </em><em>optional</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, scales the output of each <code class="docutils literal notranslate"><span class="pre">irrep_k</span></code>
by <span class="math notranslate nohighlight">\(\sqrt{1/\text{irrep}_k\text{.dim}}\)</span>.
Defaults to <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="equitorch.nn.norm.Norm.irreps_info">
<span class="sig-name descname"><span class="pre">irreps_info</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="equitorch.structs.html#equitorch.structs.IrrepsInfo" title="equitorch.structs.IrrepsInfo"><span class="pre">IrrepsInfo</span></a></em><a class="headerlink" href="#equitorch.nn.norm.Norm.irreps_info" title="Link to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="equitorch.nn.norm.MeanSquaredNorm">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">equitorch.nn.norm.</span></span><span class="sig-name descname"><span class="pre">MeanSquaredNorm</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">irreps</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="equitorch.irreps.html#equitorch.irreps.Irreps" title="equitorch.irreps.Irreps"><span class="pre">Irreps</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">scaled</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Literal</span><span class="p"><span class="pre">[</span></span><span class="m"><span class="pre">0</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">-</span></span><span class="m"><span class="pre">1</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="m"><span class="pre">2</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">-1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#equitorch.nn.norm.MeanSquaredNorm" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Computes the mean of squared L2 norms over a specified dimension (batch or channel)
for each irrep block.</p>
<p>If <code class="docutils literal notranslate"><span class="pre">dim=0</span></code> (batch mean):</p>
<div class="math notranslate nohighlight">
\[\text{Output}_{ic} = \frac{1}{N} \sum_n \left( \sum_{m \in \text{irrep}_i} (\text{input}_n(im)c^2) \right)\]</div>
<p>If <code class="docutils literal notranslate"><span class="pre">dim=-1</span></code> (channel mean):</p>
<div class="math notranslate nohighlight">
\[\text{Output}_{ni} = \frac{1}{C} \sum_c \left( \sum_{m \in \text{irrep}_i} (\text{input}_n(im)c^2) \right)\]</div>
<p>Optionally scales the output by <span class="math notranslate nohighlight">\(1/\text{irrep}_i\text{.dim}\)</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>irreps</strong> (<a class="reference internal" href="equitorch.irreps.html#equitorch.irreps.Irreps" title="equitorch.irreps.Irreps"><em>Irreps</em></a>) – Irreducible representations of the input tensor.</p></li>
<li><p><strong>scaled</strong> (<em>bool</em><em>, </em><em>optional</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, scales the output of each <code class="docutils literal notranslate"><span class="pre">irrep_i</span></code>
by <span class="math notranslate nohighlight">\(1/\text{dim}_\text{irrep_i}\)</span>.
Defaults to <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p></li>
<li><p><strong>dim</strong> (<em>int</em><em>, </em><em>optional</em>) – Dimension over which to compute the mean.
Allowed values: <code class="docutils literal notranslate"><span class="pre">0</span></code> (batch), <code class="docutils literal notranslate"><span class="pre">-1</span></code> or <code class="docutils literal notranslate"><span class="pre">2</span></code> (channel).
Defaults to <code class="docutils literal notranslate"><span class="pre">-1</span></code>.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="equitorch.nn.norm.MeanSquaredNorm.irreps_info">
<span class="sig-name descname"><span class="pre">irreps_info</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="equitorch.structs.html#equitorch.structs.IrrepsInfo" title="equitorch.structs.IrrepsInfo"><span class="pre">IrrepsInfo</span></a></em><a class="headerlink" href="#equitorch.nn.norm.MeanSquaredNorm.irreps_info" title="Link to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="module-equitorch.nn.normalization">
<span id="equitorch-nn-normalization-module"></span><h2>equitorch.nn.normalization module<a class="headerlink" href="#module-equitorch.nn.normalization" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="equitorch.nn.normalization.BatchRMSNorm">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">equitorch.nn.normalization.</span></span><span class="sig-name descname"><span class="pre">BatchRMSNorm</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">irreps</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="equitorch.irreps.html#equitorch.irreps.Irreps" title="equitorch.irreps.Irreps"><span class="pre">Irreps</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">channels</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1e-05</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">momentum</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">affine</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scaled</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#equitorch.nn.normalization.BatchRMSNorm" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Applies Batch Root Mean Square Normalization for equivariant features.</p>
<div class="math notranslate nohighlight">
\[x'_{nimc} = \gamma_{ic} \cdot (x_{nimc} / \sigma_{ic})\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[\sigma_{ic} = \sqrt{E[\text{SquaredNorm}(x_{nic})] + \epsilon}\]</div>
<p>The <a class="reference internal" href="#equitorch.nn.norm.SquaredNorm" title="equitorch.nn.norm.SquaredNorm"><code class="xref py py-class docutils literal notranslate"><span class="pre">SquaredNorm</span></code></a> can be scaled by (1/text{irrep}_itext{.dim})
depending on the <code class="docutils literal notranslate"><span class="pre">scaled</span></code> argument.
Running statistics are used during evaluation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>irreps</strong> (<a class="reference internal" href="equitorch.irreps.html#equitorch.irreps.Irreps" title="equitorch.irreps.Irreps"><em>Irreps</em></a>) – Irreducible representations of the input tensor.</p></li>
<li><p><strong>channels</strong> (<em>int</em>) – Number of channels in the input tensor (size of the last dimension).</p></li>
<li><p><strong>eps</strong> (<em>float</em><em>, </em><em>optional</em>) – A value added to the denominator for numerical stability. (epsilon)
Defaults to 1e-5.</p></li>
<li><p><strong>momentum</strong> (<em>float</em><em>, </em><em>optional</em>) – The value used for the running_mean computation.
Defaults to 0.1.</p></li>
<li><p><strong>affine</strong> (<em>bool</em><em>, </em><em>optional</em>) – If True, this module has learnable affine parameters (weight (gamma_{ic})).
Defaults to True.</p></li>
<li><p><strong>scaled</strong> (<em>bool</em><em>, </em><em>optional</em>) – If True, the <a class="reference internal" href="#equitorch.nn.norm.SquaredNorm" title="equitorch.nn.norm.SquaredNorm"><code class="xref py py-class docutils literal notranslate"><span class="pre">SquaredNorm</span></code></a> used for calculating statistics
is scaled by (1/text{irrep}_itext{.dim}). Defaults to True.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="equitorch.nn.normalization.BatchRMSNorm.irreps_info">
<span class="sig-name descname"><span class="pre">irreps_info</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="equitorch.structs.html#equitorch.structs.IrrepsInfo" title="equitorch.structs.IrrepsInfo"><span class="pre">IrrepsInfo</span></a></em><a class="headerlink" href="#equitorch.nn.normalization.BatchRMSNorm.irreps_info" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="equitorch.nn.normalization.BatchRMSNorm.reset_running_stats">
<span class="sig-name descname"><span class="pre">reset_running_stats</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#equitorch.nn.normalization.BatchRMSNorm.reset_running_stats" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="equitorch.nn.normalization.BatchRMSNorm.reset_parameters">
<span class="sig-name descname"><span class="pre">reset_parameters</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#equitorch.nn.normalization.BatchRMSNorm.reset_parameters" title="Link to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="equitorch.nn.normalization.LayerRMSNorm">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">equitorch.nn.normalization.</span></span><span class="sig-name descname"><span class="pre">LayerRMSNorm</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">irreps</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="equitorch.irreps.html#equitorch.irreps.Irreps" title="equitorch.irreps.Irreps"><span class="pre">Irreps</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">channels</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1e-05</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">affine</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scaled</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#equitorch.nn.normalization.LayerRMSNorm" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Applies Irrep-wise Layer Root Mean Square Normalization.</p>
<p>Computes statistics independently for each irrep instance within each sample.
Normalizes using the RMS value calculated across channels and irrep components
for that specific sample and irrep instance.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>irreps</strong> (<a class="reference internal" href="equitorch.irreps.html#equitorch.irreps.Irreps" title="equitorch.irreps.Irreps"><em>Irreps</em></a>) – Irreducible representations of the input tensor.</p></li>
<li><p><strong>channels</strong> (<em>int</em>) – Number of channels in the input tensor (size of the last dimension).</p></li>
<li><p><strong>eps</strong> (<em>float</em><em>, </em><em>optional</em>) – A value added to the denominator for numerical stability. (epsilon)
Defaults to 1e-5.</p></li>
<li><p><strong>affine</strong> (<em>bool</em><em>, </em><em>optional</em>) – If True, this module has learnable affine parameters (weight (gamma_{ic})).
Defaults to True.</p></li>
<li><p><strong>scaled</strong> (<em>bool</em><em>, </em><em>optional</em>) – If True, the statistics calculation considers the norm
to be scaled by (1/text{irrep}_itext{.dim}).
Defaults to True.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="equitorch.nn.normalization.LayerRMSNorm.irreps_info">
<span class="sig-name descname"><span class="pre">irreps_info</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="equitorch.structs.html#equitorch.structs.IrrepsInfo" title="equitorch.structs.IrrepsInfo"><span class="pre">IrrepsInfo</span></a></em><a class="headerlink" href="#equitorch.nn.normalization.LayerRMSNorm.irreps_info" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="equitorch.nn.normalization.LayerRMSNorm.reset_parameters">
<span class="sig-name descname"><span class="pre">reset_parameters</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#equitorch.nn.normalization.LayerRMSNorm.reset_parameters" title="Link to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="module-equitorch.nn.others">
<span id="equitorch-nn-others-module"></span><h2>equitorch.nn.others module<a class="headerlink" href="#module-equitorch.nn.others" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="equitorch.nn.others.SplitIrreps">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">equitorch.nn.others.</span></span><span class="sig-name descname"><span class="pre">SplitIrreps</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">irreps</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="equitorch.irreps.html#equitorch.irreps.Irreps" title="equitorch.irreps.Irreps"><span class="pre">Irreps</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">split_num_irreps</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">-2</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#equitorch.nn.others.SplitIrreps" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>A module that splits input tensors according to specified irrep segments.</p>
<p>The splitting is done based on the dimensions of the irreducible representations.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>irreps</strong> (<a class="reference internal" href="equitorch.irreps.html#equitorch.irreps.Irreps" title="equitorch.irreps.Irreps"><em>Irreps</em></a>) – The irreducible representations specification.</p></li>
<li><p><strong>split_num_irreps</strong> (<em>Iterable</em><em>[</em><em>int</em><em>]</em>) – Number of irreps in each split segment.
Must sum to total irreps. May contain at most one -1 or <code class="docutils literal notranslate"><span class="pre">...</span></code>
to represent the remaining irreps.</p></li>
<li><p><strong>dim</strong> (<em>int</em><em>, </em><em>optional</em>) – The dimension along which to split the input tensor.
Defaults to -2.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Split 3 scalar irreps (dim=1 each) and 2 vector irreps (dim=3 each)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">irreps</span> <span class="o">=</span> <span class="n">Irreps</span><span class="p">(</span><span class="s2">&quot;3x0e + 2x1o&quot;</span><span class="p">)</span>  <span class="c1"># 3 scalars + 2 vectors</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">split</span> <span class="o">=</span> <span class="n">SplitIrreps</span><span class="p">(</span><span class="n">irreps</span><span class="p">,</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>  <span class="c1"># Split first 2 irreps, then remaining</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">irreps</span><span class="o">.</span><span class="n">dim</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>  <span class="c1"># batch=5, dim=9 (3*1 + 2*3), channels=10</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">splits</span> <span class="o">=</span> <span class="n">split</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># Returns list of 2 tensors</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">splits</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span>
<span class="go">torch.Size([5, 2, 10])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">splits</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span>
<span class="go">torch.Size([5, 7, 10])</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Using ... for automatic size calculation</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">split</span> <span class="o">=</span> <span class="n">SplitIrreps</span><span class="p">(</span><span class="n">irreps</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="o">...</span><span class="p">])</span>  <span class="c1"># First irrep, then remaining</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">splits</span> <span class="o">=</span> <span class="n">split</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">splits</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span>
<span class="go">torch.Size([5, 1, 10])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">splits</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span>
<span class="go">torch.Size([5, 8, 10])</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="equitorch.nn.others.Separable">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">equitorch.nn.others.</span></span><span class="sig-name descname"><span class="pre">Separable</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">irreps</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="equitorch.irreps.html#equitorch.irreps.Irreps" title="equitorch.irreps.Irreps"><span class="pre">Irreps</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">split_num_irreps</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sub_modules</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><span class="pre">Module</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cat_after</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">-2</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#equitorch.nn.others.Separable" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>A module that applies different transformations to different parts of input tensor
according to irreducible representations (irreps), with optional concatenation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>irreps</strong> (<a class="reference internal" href="equitorch.irreps.html#equitorch.irreps.Irreps" title="equitorch.irreps.Irreps"><em>Irreps</em></a>) – The irreducible representations specification for the input tensor.</p></li>
<li><p><strong>split_num_irreps</strong> (<em>Iterable</em><em>[</em><em>int</em><em>]</em>) – Number of irreps in each split segment.
May contain at most one -1 or <code class="docutils literal notranslate"><span class="pre">...</span></code> to represent remaining irreps.
Length must match length of <code class="docutils literal notranslate"><span class="pre">sub_modules</span></code>.</p></li>
<li><p><strong>sub_modules</strong> (<em>Iterable</em><em>[</em><em>Callable</em><em>]</em>) – Transformation modules for each split segment.
Use <code class="docutils literal notranslate"><span class="pre">None</span></code> for identity operation.</p></li>
<li><p><strong>cat_after</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether to concatenate results after transformation.
Defaults to True.</p></li>
<li><p><strong>dim</strong> (<em>int</em><em>, </em><em>optional</em>) – The dimension along which to split and concatenate tensors.
Defaults to -2.</p></li>
</ul>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>ValueError</strong> – <ul class="simple">
<li><p>If lengths of <code class="docutils literal notranslate"><span class="pre">split_num_irreps</span></code> and <code class="docutils literal notranslate"><span class="pre">sub_modules</span></code> don’t match.
    - If sum of <code class="docutils literal notranslate"><span class="pre">split_num_irreps</span></code> doesn’t match total irreps.
    - If invalid <code class="docutils literal notranslate"><span class="pre">split_num_irreps</span></code> specification.</p></li>
</ul>
</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="module-equitorch.nn.radials">
<span id="equitorch-nn-radials-module"></span><h2>equitorch.nn.radials module<a class="headerlink" href="#module-equitorch.nn.radials" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="equitorch.nn.radials.BesselBasis">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">equitorch.nn.radials.</span></span><span class="sig-name descname"><span class="pre">BesselBasis</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">r_max</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_basis</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">trainable</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#equitorch.nn.radials.BesselBasis" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="equitorch.nn.radials.BesselBasis.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">r_max</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_basis</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">trainable</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#equitorch.nn.radials.BesselBasis.__init__" title="Link to this definition"></a></dt>
<dd><p>Radial Bessel Basis, as proposed in DimeNet: <a class="reference external" href="https://arxiv.org/abs/2003.03123">https://arxiv.org/abs/2003.03123</a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>r_max</strong> (<em>float</em>) – Cutoff radius</p></li>
<li><p><strong>num_basis</strong> (<em>int</em>) – Number of Bessel Basis functions</p></li>
<li><p><strong>trainable</strong> (<em>bool</em>) – Train the <span class="math notranslate nohighlight">\(n \pi\)</span> part or not.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="equitorch.nn.radials.BesselBasis.r_max">
<span class="sig-name descname"><span class="pre">r_max</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">float</span></em><a class="headerlink" href="#equitorch.nn.radials.BesselBasis.r_max" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="equitorch.nn.radials.BesselBasis.prefactor">
<span class="sig-name descname"><span class="pre">prefactor</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">float</span></em><a class="headerlink" href="#equitorch.nn.radials.BesselBasis.prefactor" title="Link to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="module-equitorch.nn.rotations">
<span id="equitorch-nn-rotations-module"></span><h2>equitorch.nn.rotations module<a class="headerlink" href="#module-equitorch.nn.rotations" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="equitorch.nn.rotations.AnglesToMatrix">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">equitorch.nn.rotations.</span></span><span class="sig-name descname"><span class="pre">AnglesToMatrix</span></span><a class="headerlink" href="#equitorch.nn.rotations.AnglesToMatrix" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Module to convert Euler angles (ZYZ convention) to rotation matrices.</p>
<p>The ZYZ Euler angles (alpha, beta, gamma) correspond to the rotation matrix:</p>
<div class="math notranslate nohighlight">
\[R(\alpha, \beta, \gamma) = R_z(\alpha) R_y(\beta) R_z(\gamma)\]</div>
<p>which is explicitly:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{pmatrix}
-\sin(\alpha)\sin(\gamma) + \cos(\alpha)\cos(\beta)\cos(\gamma) &amp; -\sin(\alpha)\cos(\beta)\cos(\gamma) - \sin(\gamma)\cos(\alpha) &amp; \sin(\beta)\cos(\gamma) \\
\sin(\alpha)\cos(\gamma) + \sin(\gamma)\cos(\alpha)\cos(\beta) &amp; -\sin(\alpha)\sin(\gamma)\cos(\beta) + \cos(\alpha)\cos(\gamma) &amp; \sin(\beta)\sin(\gamma) \\
-\sin(\beta)\cos(\alpha) &amp; \sin(\alpha)\sin(\beta) &amp; \cos(\beta)
\end{pmatrix}\end{split}\]</div>
<p>Wraps the functional version <a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.rotations.angles_to_matrix" title="equitorch.nn.functional.rotations.angles_to_matrix"><code class="xref py py-func docutils literal notranslate"><span class="pre">angles_to_matrix()</span></code></a>.</p>
</dd></dl>

</section>
<section id="module-equitorch.nn.sphericals">
<span id="equitorch-nn-sphericals-module"></span><h2>equitorch.nn.sphericals module<a class="headerlink" href="#module-equitorch.nn.sphericals" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="equitorch.nn.sphericals.SphericalHarmonics">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">equitorch.nn.sphericals.</span></span><span class="sig-name descname"><span class="pre">SphericalHarmonics</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">l_max</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">normalize_input</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">integral_normalize</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#equitorch.nn.sphericals.SphericalHarmonics" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Computes spherical harmonics from input Cartesian coordinates.
Wraps the functional <a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.sphericals.spherical_harmonics" title="equitorch.nn.functional.sphericals.spherical_harmonics"><code class="xref py py-func docutils literal notranslate"><span class="pre">spherical_harmonics()</span></code></a>.</p>
<p>Spherical harmonics are a set of orthogonal functions defined on the surface of a sphere.
They are solutions to Laplace’s equation in spherical coordinates.</p>
<p>If <cite>integral_normalize</cite> is True, the output is scaled by (1 / sqrt{4pi}).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>l_max</strong> (<em>int</em>) – The maximum degree of the spherical harmonics.</p></li>
<li><p><strong>normalize_input</strong> (<em>bool</em><em>, </em><em>optional</em>) – If True, normalizes the input xyz vector before
computing spherical harmonics. Defaults to True.</p></li>
<li><p><strong>integral_normalize</strong> (<em>bool</em><em>, </em><em>optional</em>) – If True, applies normalization for integration over the sphere.
Defaults to False.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="equitorch.nn.sphericals.XYZToSpherical">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">equitorch.nn.sphericals.</span></span><span class="sig-name descname"><span class="pre">XYZToSpherical</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">normalize_input</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">with_r</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1e-14</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">-1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#equitorch.nn.sphericals.XYZToSpherical" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Module to convert Cartesian coordinates ((x, y, z)) to spherical ((theta, phi, r)).
Wraps the functional <a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.sphericals.xyz_to_spherical" title="equitorch.nn.functional.sphericals.xyz_to_spherical"><code class="xref py py-func docutils literal notranslate"><span class="pre">xyz_to_spherical()</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>normalize_input</strong> (<em>bool</em><em>, </em><em>optional</em>) – If True, normalizes the input <cite>xyz</cite> vector before
computing angles. Defaults to True.</p></li>
<li><p><strong>with_r</strong> (<em>bool</em><em>, </em><em>optional</em>) – If True, returns (r) along with (theta) and (phi).
Defaults to False.</p></li>
<li><p><strong>eps</strong> (<em>float</em><em>, </em><em>optional</em>) – Small (epsilon) for numerical stability. Defaults to 1e-14.</p></li>
<li><p><strong>dim</strong> (<em>int</em><em>, </em><em>optional</em>) – Dimension of Cartesian coordinates. Defaults to -1.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="equitorch.nn.sphericals.SphericalToXYZ">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">equitorch.nn.sphericals.</span></span><span class="sig-name descname"><span class="pre">SphericalToXYZ</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">-1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#equitorch.nn.sphericals.SphericalToXYZ" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Module to convert spherical coordinates ((theta, phi, r)) to Cartesian ((x, y, z)).
Wraps the functional <a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.sphericals.spherical_to_xyz" title="equitorch.nn.functional.sphericals.spherical_to_xyz"><code class="xref py py-func docutils literal notranslate"><span class="pre">spherical_to_xyz()</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>dim</strong> (<em>int</em><em>, </em><em>optional</em>) – Dimension along which to stack output (x,y,z). Defaults to -1.</p>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="equitorch.nn.sphericals.XYZToSinCos">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">equitorch.nn.sphericals.</span></span><span class="sig-name descname"><span class="pre">XYZToSinCos</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">max_m</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">normalize_input</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">component_normalize</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1e-14</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">-1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#equitorch.nn.sphericals.XYZToSinCos" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Module to convert Cartesian coordinates ((x, y, z)) to sin/cos embeddings
of the spherical angles (theta) and (phi).
Wraps the functional <a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.sphericals.xyz_to_sincos" title="equitorch.nn.functional.sphericals.xyz_to_sincos"><code class="xref py py-func docutils literal notranslate"><span class="pre">xyz_to_sincos()</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>max_m</strong> (<em>int</em>) – The maximum multiple of the angles to compute (sin) / (cos) for.</p></li>
<li><p><strong>normalize_input</strong> (<em>bool</em><em>, </em><em>optional</em>) – If True, normalizes <cite>xyz</cite> before extracting angles.
Defaults to True.</p></li>
<li><p><strong>component_normalize</strong> – (bool, optional): False</p></li>
<li><p><strong>eps</strong> (<em>float</em><em>, </em><em>optional</em>) – Small (epsilon) for numerical stability. Defaults to 1e-14.</p></li>
<li><p><strong>dim</strong> (<em>int</em><em>, </em><em>optional</em>) – Dimension of Cartesian coordinates in <cite>xyz</cite>. Defaults to -1.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
<section id="module-equitorch.nn.tensor_products">
<span id="equitorch-nn-tensor-products-module"></span><h2>equitorch.nn.tensor_products module<a class="headerlink" href="#module-equitorch.nn.tensor_products" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="equitorch.nn.tensor_products.TensorProduct">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">equitorch.nn.tensor_products.</span></span><span class="sig-name descname"><span class="pre">TensorProduct</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">irreps_in1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">irreps_in2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">irreps_out</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">channels_in1</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">channels_in2</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">channels_out</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">internal_weights</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">feature_mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'uuu'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">path_norm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">channel_norm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">path</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#equitorch.nn.tensor_products.TensorProduct" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Computes the tensor product of two equivariant feature tensors.</p>
<p>Supports two main modes controlled by <code class="docutils literal notranslate"><span class="pre">feature_mode</span></code>:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">'uvw'</span></code>: Fully connected tensor product.</p>
<ul>
<li><p>Input1 shape: <code class="docutils literal notranslate"><span class="pre">(...,</span> <span class="pre">irreps_in1.dim,</span> <span class="pre">channels_in1)</span></code></p></li>
<li><p>Input2 shape: <code class="docutils literal notranslate"><span class="pre">(...,</span> <span class="pre">irreps_in2.dim,</span> <span class="pre">channels_in2)</span></code></p></li>
<li><p>Weight shape: <code class="docutils literal notranslate"><span class="pre">(num_paths,</span> <span class="pre">channels_in1,</span> <span class="pre">channels_in2,</span> <span class="pre">channels_out)</span></code></p></li>
<li><p>Output shape: <code class="docutils literal notranslate"><span class="pre">(...,</span> <span class="pre">irreps_out.dim,</span> <span class="pre">channels_out)</span></code></p></li>
</ul>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">'uuu'</span></code>: Depthwise/elementwise tensor product.
with <code class="docutils literal notranslate"><span class="pre">uuu</span></code> instructions (often used for self-interaction).</p>
<ul>
<li><p>Input1 shape: <code class="docutils literal notranslate"><span class="pre">(...,</span> <span class="pre">irreps_in1.dim,</span> <span class="pre">channels)</span></code></p></li>
<li><p>Input2 shape: <code class="docutils literal notranslate"><span class="pre">(...,</span> <span class="pre">irreps_in2.dim,</span> <span class="pre">channels)</span></code></p></li>
<li><p>Weight shape: <code class="docutils literal notranslate"><span class="pre">(num_paths,</span> <span class="pre">channels_out)</span></code> (where <code class="docutils literal notranslate"><span class="pre">channels_out</span></code> usually equals <code class="docutils literal notranslate"><span class="pre">channels</span></code>)</p></li>
<li><p>Output shape: <code class="docutils literal notranslate"><span class="pre">(...,</span> <span class="pre">irreps_out.dim,</span> <span class="pre">channels_out)</span></code></p></li>
</ul>
</li>
</ul>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>irreps_in1</strong> (<a class="reference internal" href="equitorch.irreps.html#equitorch.irreps.Irreps" title="equitorch.irreps.Irreps"><em>Irreps</em></a>) – Irreducible representations of the first input tensor.</p></li>
<li><p><strong>irreps_in2</strong> (<a class="reference internal" href="equitorch.irreps.html#equitorch.irreps.Irreps" title="equitorch.irreps.Irreps"><em>Irreps</em></a>) – Irreducible representations of the second input tensor.</p></li>
<li><p><strong>irreps_out</strong> (<a class="reference internal" href="equitorch.irreps.html#equitorch.irreps.Irreps" title="equitorch.irreps.Irreps"><em>Irreps</em></a>) – Irreducible representations of the output tensor.</p></li>
<li><p><strong>channels_in1</strong> (<em>int</em><em>, </em><em>optional</em>) – Number of channels for the first input.
Required if <code class="docutils literal notranslate"><span class="pre">internal_weights=True</span></code> or <code class="docutils literal notranslate"><span class="pre">feature_mode='uvw'</span></code>.</p></li>
<li><p><strong>channels_in2</strong> (<em>int</em><em>, </em><em>optional</em>) – Number of channels for the second input.
Required if <code class="docutils literal notranslate"><span class="pre">internal_weights=True</span></code> or <code class="docutils literal notranslate"><span class="pre">feature_mode='uvw'</span></code>.</p></li>
<li><p><strong>channels_out</strong> (<em>int</em><em>, </em><em>optional</em>) – Number of channels for the output.
Required if <code class="docutils literal notranslate"><span class="pre">internal_weights=True</span></code>.</p></li>
<li><p><strong>internal_weights</strong> (<em>bool</em><em>, </em><em>default=True</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, the module manages its own weight parameter.
If <code class="docutils literal notranslate"><span class="pre">False</span></code>, weights must be provided during the forward pass.</p></li>
<li><p><strong>feature_mode</strong> (<em>{'uuu'</em><em>, </em><em>'uvw'}</em><em>, </em><em>default='uuu'</em>) – <p>Controls the type of tensor product:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">'uuu'</span></code>: Depthwise/elementwise product. Assumes <code class="docutils literal notranslate"><span class="pre">channels_in1</span> <span class="pre">==</span> <span class="pre">channels_in2</span> <span class="pre">==</span> <span class="pre">channels_out</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">'uvw'</span></code>: Fully connected product.</p></li>
</ul>
</p></li>
<li><p><strong>path_norm</strong> (<em>bool</em><em>, </em><em>default=True</em>) – Whether to apply path normalization to the weights.</p></li>
<li><p><strong>channel_norm</strong> (<em>bool</em><em>, </em><em>default=False</em>) – Whether to apply channel normalization (specific to <code class="docutils literal notranslate"><span class="pre">'uvw'</span></code> mode).
Divides weights by <span class="math notranslate nohighlight">\(\sqrt{\text{channels_in1} \times \text{channels_in2}}\)</span>.</p></li>
<li><p><strong>path</strong> (<em>list</em><em>, </em><em>optional</em>) – Manually specify the coupling paths.
If <code class="docutils literal notranslate"><span class="pre">None</span></code>, all allowed paths are used.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="equitorch.nn.tensor_products.TensorProduct.weight">
<span class="sig-name descname"><span class="pre">weight</span></span><a class="headerlink" href="#equitorch.nn.tensor_products.TensorProduct.weight" title="Link to this definition"></a></dt>
<dd><p>The learnable weights of the module if <code class="docutils literal notranslate"><span class="pre">internal_weights=True</span></code>.
Shape depends on <code class="docutils literal notranslate"><span class="pre">feature_mode</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.nn.Parameter or None</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="equitorch.nn.tensor_products.TensorProduct.tp_info_forward">
<span class="sig-name descname"><span class="pre">tp_info_forward</span></span><a class="headerlink" href="#equitorch.nn.tensor_products.TensorProduct.tp_info_forward" title="Link to this definition"></a></dt>
<dd><p>Constant information for the forward pass computation.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="equitorch.structs.html#equitorch.structs.TensorProductInfo" title="equitorch.structs.TensorProductInfo">TensorProductInfo</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="equitorch.nn.tensor_products.TensorProduct.tp_info_backward1">
<span class="sig-name descname"><span class="pre">tp_info_backward1</span></span><a class="headerlink" href="#equitorch.nn.tensor_products.TensorProduct.tp_info_backward1" title="Link to this definition"></a></dt>
<dd><p>Constant information for the backward pass w.r.t. input1.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="equitorch.structs.html#equitorch.structs.TensorProductInfo" title="equitorch.structs.TensorProductInfo">TensorProductInfo</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="equitorch.nn.tensor_products.TensorProduct.tp_info_backward2">
<span class="sig-name descname"><span class="pre">tp_info_backward2</span></span><a class="headerlink" href="#equitorch.nn.tensor_products.TensorProduct.tp_info_backward2" title="Link to this definition"></a></dt>
<dd><p>Constant information for the backward pass w.r.t. input2.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="equitorch.structs.html#equitorch.structs.TensorProductInfo" title="equitorch.structs.TensorProductInfo">TensorProductInfo</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="equitorch.nn.tensor_products.TensorProduct.num_paths">
<span class="sig-name descname"><span class="pre">num_paths</span></span><a class="headerlink" href="#equitorch.nn.tensor_products.TensorProduct.num_paths" title="Link to this definition"></a></dt>
<dd><p>Number of coupling paths determined by the irreps.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="equitorch.nn.tensor_products.TensorProduct.weight_numel">
<span class="sig-name descname"><span class="pre">weight_numel</span></span><a class="headerlink" href="#equitorch.nn.tensor_products.TensorProduct.weight_numel" title="Link to this definition"></a></dt>
<dd><p>Total number of elements in the weight tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="id7">
<span class="sig-name descname"><span class="pre">tp_info_forward</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="equitorch.structs.html#equitorch.structs.TensorProductInfo" title="equitorch.structs.TensorProductInfo"><span class="pre">TensorProductInfo</span></a></em><a class="headerlink" href="#id7" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="id8">
<span class="sig-name descname"><span class="pre">tp_info_backward1</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="equitorch.structs.html#equitorch.structs.TensorProductInfo" title="equitorch.structs.TensorProductInfo"><span class="pre">TensorProductInfo</span></a></em><a class="headerlink" href="#id8" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="id9">
<span class="sig-name descname"><span class="pre">tp_info_backward2</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="equitorch.structs.html#equitorch.structs.TensorProductInfo" title="equitorch.structs.TensorProductInfo"><span class="pre">TensorProductInfo</span></a></em><a class="headerlink" href="#id9" title="Link to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="equitorch.nn.tensor_products.TensorDot">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">equitorch.nn.tensor_products.</span></span><span class="sig-name descname"><span class="pre">TensorDot</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">irreps</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">feature_mode</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scaled</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#equitorch.nn.tensor_products.TensorDot" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Computes the equivariant irrep-wise dot product of two feature tensors.</p>
<p>Supports two main modes controlled by <code class="docutils literal notranslate"><span class="pre">feature_mode</span></code>:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">'uv'</span></code>: Channel-cartesian dot product.</p>
<ul>
<li><p>Input1 shape: <code class="docutils literal notranslate"><span class="pre">(...,</span> <span class="pre">irreps.dim,</span> <span class="pre">channels1)</span></code></p></li>
<li><p>Input2 shape: <code class="docutils literal notranslate"><span class="pre">(...,</span> <span class="pre">irreps.dim,</span> <span class="pre">channels2)</span></code></p></li>
<li><p>Output shape: <code class="docutils literal notranslate"><span class="pre">(...,</span> <span class="pre">len(irreps),</span> <span class="pre">channels1,</span> <span class="pre">channels2)</span></code></p></li>
</ul>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">'uu'</span></code>: Channel-wise dot product. Sums over the channel dimension after the dot product.</p>
<ul>
<li><p>Input1 shape: <code class="docutils literal notranslate"><span class="pre">(...,</span> <span class="pre">irreps.dim,</span> <span class="pre">channels)</span></code></p></li>
<li><p>Input2 shape: <code class="docutils literal notranslate"><span class="pre">(...,</span> <span class="pre">irreps.dim,</span> <span class="pre">channels)</span></code></p></li>
<li><p>Output shape: <code class="docutils literal notranslate"><span class="pre">(...,</span> <span class="pre">len(irreps),</span> <span class="pre">channels)</span></code></p></li>
</ul>
</li>
</ul>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>irreps</strong> (<a class="reference internal" href="equitorch.irreps.html#equitorch.irreps.Irreps" title="equitorch.irreps.Irreps"><em>Irreps</em></a><em> or </em><em>str</em>) – Irreducible representations of the input tensors.
Both inputs must have the same irreps.</p></li>
<li><p><strong>feature_mode</strong> (<em>{'uv'</em><em>, </em><em>'uu'}</em>) – <p>Controls how the channel dimension is handled:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">'uv'</span></code>: Channel-cartesian dot product.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">'uu'</span></code>: Channel-wise dot product.</p></li>
</ul>
</p></li>
<li><p><strong>scaled</strong> (<em>bool</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, scales the dot product by <span class="math notranslate nohighlight">\(1 / \sqrt{\text{irrep.dim}}\)</span>.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="equitorch.nn.tensor_products.TensorDot.irreps_info">
<span class="sig-name descname"><span class="pre">irreps_info</span></span><a class="headerlink" href="#equitorch.nn.tensor_products.TensorDot.irreps_info" title="Link to this definition"></a></dt>
<dd><p>Constant information about the input irreps.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="equitorch.structs.html#equitorch.structs.IrrepsInfo" title="equitorch.structs.IrrepsInfo">IrrepsInfo</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="id10">
<span class="sig-name descname"><span class="pre">irreps_info</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="equitorch.structs.html#equitorch.structs.IrrepsInfo" title="equitorch.structs.IrrepsInfo"><span class="pre">IrrepsInfo</span></a></em><a class="headerlink" href="#id10" title="Link to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="module-equitorch.nn.wigner_d">
<span id="equitorch-nn-wigner-d-module"></span><h2>equitorch.nn.wigner_d module<a class="headerlink" href="#module-equitorch.nn.wigner_d" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="equitorch.nn.wigner_d.SparseWignerRotation">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">equitorch.nn.wigner_d.</span></span><span class="sig-name descname"><span class="pre">SparseWignerRotation</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">irreps</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="equitorch.irreps.html#equitorch.irreps.Irreps" title="equitorch.irreps.Irreps"><span class="pre">Irreps</span></a></span></em><span class="sig-paren">)</span><a class="headerlink" href="#equitorch.nn.wigner_d.SparseWignerRotation" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Applies a sparse Wigner D-matrix rotation to input features.</p>
<p>This module computes the rotation based on Euler angles (<span class="math notranslate nohighlight">\(\alpha, \beta, \gamma\)</span>)
provided as precomputed sin/cos tensors. It utilizes sparse matrix operations for the rotation.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>It is currently suggested to use <a class="reference internal" href="#equitorch.nn.wigner_d.DenseWignerRotation" title="equitorch.nn.wigner_d.DenseWignerRotation"><code class="xref py py-class docutils literal notranslate"><span class="pre">DenseWignerRotation</span></code></a> or <a class="reference internal" href="#equitorch.nn.wigner_d.WignerD" title="equitorch.nn.wigner_d.WignerD"><code class="xref py py-class docutils literal notranslate"><span class="pre">WignerD</span></code></a>
for applying rotations, at least when gradients with respect to angles are not required.</p>
</div>
<p>The Wigner D-matrix <span class="math notranslate nohighlight">\(D^l_{m'm}(R)\)</span> transforms spherical tensors under rotation <span class="math notranslate nohighlight">\(R\)</span>.
This module applies such transformations for a given <a class="reference internal" href="equitorch.irreps.html#equitorch.irreps.Irreps" title="equitorch.irreps.Irreps"><code class="xref py py-class docutils literal notranslate"><span class="pre">Irreps</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>irreps</strong> (<a class="reference internal" href="equitorch.irreps.html#equitorch.irreps.Irreps" title="equitorch.irreps.Irreps"><em>Irreps</em></a>) – The irreducible representations defining the input and output feature space.</p>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="equitorch.nn.wigner_d.SparseWignerRotation.info">
<span class="sig-name descname"><span class="pre">info</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="equitorch.structs.html#equitorch.structs.WignerRotationInfo" title="equitorch.structs.WignerRotationInfo"><span class="pre">WignerRotationInfo</span></a></em><a class="headerlink" href="#equitorch.nn.wigner_d.SparseWignerRotation.info" title="Link to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="equitorch.nn.wigner_d.DenseWignerRotation">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">equitorch.nn.wigner_d.</span></span><span class="sig-name descname"><span class="pre">DenseWignerRotation</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">irreps</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="equitorch.irreps.html#equitorch.irreps.Irreps" title="equitorch.irreps.Irreps"><span class="pre">Irreps</span></a></span></em><span class="sig-paren">)</span><a class="headerlink" href="#equitorch.nn.wigner_d.DenseWignerRotation" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Applies a dense Wigner D-matrix rotation to input features.</p>
<p>This module takes a precomputed dense Wigner D-matrix and applies it to the input features.
The Wigner D-matrix <span class="math notranslate nohighlight">\(D(R)\)</span> itself should be computed separately, for example, using the <a class="reference internal" href="#equitorch.nn.wigner_d.WignerD" title="equitorch.nn.wigner_d.WignerD"><code class="xref py py-class docutils literal notranslate"><span class="pre">WignerD</span></code></a> module.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>irreps</strong> (<a class="reference internal" href="equitorch.irreps.html#equitorch.irreps.Irreps" title="equitorch.irreps.Irreps"><em>Irreps</em></a>) – The irreducible representations defining the input and output feature space.
This is used for validation and representation purposes.</p>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="equitorch.nn.wigner_d.WignerD">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">equitorch.nn.wigner_d.</span></span><span class="sig-name descname"><span class="pre">WignerD</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">irreps</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="equitorch.irreps.html#equitorch.irreps.Irreps" title="equitorch.irreps.Irreps"><span class="pre">Irreps</span></a></span></em><span class="sig-paren">)</span><a class="headerlink" href="#equitorch.nn.wigner_d.WignerD" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Computes the dense Wigner D-matrix <span class="math notranslate nohighlight">\(D(R)\)</span> for given <a class="reference internal" href="equitorch.irreps.html#equitorch.irreps.Irreps" title="equitorch.irreps.Irreps"><code class="xref py py-class docutils literal notranslate"><span class="pre">Irreps</span></code></a> and Euler angles <span class="math notranslate nohighlight">\((\alpha, \beta, \gamma)\)</span>.</p>
<p>The Wigner D-matrix is constructed based on the ZYZ Euler angle convention:</p>
<div class="math notranslate nohighlight">
\[D(\alpha, \beta, \gamma) = D_z(\alpha) D_y(\beta) D_z(\gamma)\]</div>
<p>This module caches the necessary sparse rotation information and an identity matrix
to efficiently compute the dense D-matrix using the <a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.wigner_d.wigner_d_matrix" title="equitorch.nn.functional.wigner_d.wigner_d_matrix"><code class="xref py py-func docutils literal notranslate"><span class="pre">wigner_d_matrix()</span></code></a> functional.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>irreps</strong> (<a class="reference internal" href="equitorch.irreps.html#equitorch.irreps.Irreps" title="equitorch.irreps.Irreps"><em>Irreps</em></a>) – The irreducible representations for which to compute the D-matrix.
The resulting D-matrix will have dimensions <code class="docutils literal notranslate"><span class="pre">(irreps.dim,</span> <span class="pre">irreps.dim)</span></code>.</p>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="equitorch.nn.wigner_d.WignerD.info">
<span class="sig-name descname"><span class="pre">info</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="equitorch.structs.html#equitorch.structs.WignerRotationInfo" title="equitorch.structs.WignerRotationInfo"><span class="pre">WignerRotationInfo</span></a></em><a class="headerlink" href="#equitorch.nn.wigner_d.WignerD.info" title="Link to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="equitorch.nn.wigner_d.AlignToZWignerD">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">equitorch.nn.wigner_d.</span></span><span class="sig-name descname"><span class="pre">AlignToZWignerD</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">irreps</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="equitorch.irreps.html#equitorch.irreps.Irreps" title="equitorch.irreps.Irreps"><span class="pre">Irreps</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">normalized</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1e-14</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#equitorch.nn.wigner_d.AlignToZWignerD" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Computes the Wigner D-matrix <span class="math notranslate nohighlight">\(D(R_{align})\)</span> that rotates a given vector <span class="math notranslate nohighlight">\(\vec{v} = (x, y, z)\)</span> onto the z-axis.</p>
<p>The rotation <span class="math notranslate nohighlight">\(R_{align}\)</span> is defined by Euler angles <span class="math notranslate nohighlight">\((0, -\theta, -\phi)\)</span>, where <span class="math notranslate nohighlight">\(\theta\)</span> and <span class="math notranslate nohighlight">\(\phi\)</span> are the
polar and azimuthal angles of the vector <span class="math notranslate nohighlight">\(\vec{v}\)</span>, respectively. This means:</p>
<div class="math notranslate nohighlight">
\[R_{align} \vec{v} = ||\vec{v}|| \hat{z}\]</div>
<p>The Wigner D-matrix is then <span class="math notranslate nohighlight">\(D(0, -\theta, -\phi)\)</span>.</p>
<p>This module caches the necessary sparse rotation information and an identity matrix.
It utilizes the <a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.wigner_d.align_to_z_wigner_d" title="equitorch.nn.functional.wigner_d.align_to_z_wigner_d"><code class="xref py py-func docutils literal notranslate"><span class="pre">align_to_z_wigner_d()</span></code></a> functional.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>irreps</strong> (<a class="reference internal" href="equitorch.irreps.html#equitorch.irreps.Irreps" title="equitorch.irreps.Irreps"><em>Irreps</em></a>) – The irreducible representations for which to compute the D-matrix.</p></li>
<li><p><strong>normalized</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether to normalize the input <code class="docutils literal notranslate"><span class="pre">xyz</span></code> vector
before calculating angles for rotation. If <code class="docutils literal notranslate"><span class="pre">True</span></code>, effectively rotates <span class="math notranslate nohighlight">\(\hat{v}\)</span>.
Defaults to <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p></li>
<li><p><strong>eps</strong> (<em>float</em><em>, </em><em>optional</em>) – Small <span class="math notranslate nohighlight">\(\epsilon\)</span> value for numerical stability in angle calculation.
Defaults to <code class="docutils literal notranslate"><span class="pre">1e-14</span></code>.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="equitorch.nn.wigner_d.AlignToZWignerD.info">
<span class="sig-name descname"><span class="pre">info</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="equitorch.structs.html#equitorch.structs.WignerRotationInfo" title="equitorch.structs.WignerRotationInfo"><span class="pre">WignerRotationInfo</span></a></em><a class="headerlink" href="#equitorch.nn.wigner_d.AlignToZWignerD.info" title="Link to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="module-equitorch.nn">
<span id="module-contents"></span><h2>Module contents<a class="headerlink" href="#module-equitorch.nn" title="Link to this heading"></a></h2>
<p>Contains equivariant neural network modules and functionalities.</p>
<dl class="py class">
<dt class="sig sig-object py" id="equitorch.nn.SO3Linear">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">equitorch.nn.</span></span><span class="sig-name descname"><span class="pre">SO3Linear</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">irreps_in1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">irreps_in2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">irreps_out</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">channels_in</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">channels_out</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">internal_weights</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">feature_mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'uu'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">path_norm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">channel_norm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">path</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#equitorch.nn.SO3Linear" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>SO(3) equivariant linear layer using tensor products.</p>
<p>Equivalent to a TensorProduct where  <code class="docutils literal notranslate"><span class="pre">input2</span></code> does not have a channel dimension.</p>
<p>Supports two main modes controlled by <code class="docutils literal notranslate"><span class="pre">feature_mode</span></code>:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">'uv'</span></code>: Fully connected in channel dimension.</p>
<ul>
<li><p>Input1 shape: <code class="docutils literal notranslate"><span class="pre">(...,</span> <span class="pre">irreps_in1.dim,</span> <span class="pre">channels_in)</span></code></p></li>
<li><p>Input2 shape: <code class="docutils literal notranslate"><span class="pre">(...,</span> <span class="pre">irreps_in2.dim)</span></code></p></li>
<li><p>Weight shape: <code class="docutils literal notranslate"><span class="pre">(num_paths,</span> <span class="pre">channels_in,</span> <span class="pre">channels_out)</span></code></p></li>
<li><p>Output shape: <code class="docutils literal notranslate"><span class="pre">(...,</span> <span class="pre">irreps_out.dim,</span> <span class="pre">channels_out)</span></code></p></li>
</ul>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">'uu'</span></code>: Depthwise/elementwise in channel dimension.</p>
<ul>
<li><p>Input1 shape: <code class="docutils literal notranslate"><span class="pre">(...,</span> <span class="pre">irreps_in1.dim,</span> <span class="pre">channels)</span></code></p></li>
<li><p>Input2 shape: <code class="docutils literal notranslate"><span class="pre">(...,</span> <span class="pre">irreps_in2.dim)</span></code></p></li>
<li><p>Weight shape: <code class="docutils literal notranslate"><span class="pre">(num_paths,</span> <span class="pre">channels)</span></code></p></li>
<li><p>Output shape: <code class="docutils literal notranslate"><span class="pre">(...,</span> <span class="pre">irreps_out.dim,</span> <span class="pre">channels)</span></code></p></li>
</ul>
</li>
</ul>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>irreps_in1</strong> (<a class="reference internal" href="equitorch.irreps.html#equitorch.irreps.Irreps" title="equitorch.irreps.Irreps"><em>Irreps</em></a>) – Irreducible representations of the main input tensor (<code class="docutils literal notranslate"><span class="pre">input1</span></code>).</p></li>
<li><p><strong>irreps_in2</strong> (<a class="reference internal" href="equitorch.irreps.html#equitorch.irreps.Irreps" title="equitorch.irreps.Irreps"><em>Irreps</em></a>) – Irreducible representations of the second input tensor (<code class="docutils literal notranslate"><span class="pre">input2</span></code>),
often representing weights like spherical harmonics.</p></li>
<li><p><strong>irreps_out</strong> (<a class="reference internal" href="equitorch.irreps.html#equitorch.irreps.Irreps" title="equitorch.irreps.Irreps"><em>Irreps</em></a>) – Irreducible representations of the output tensor.</p></li>
<li><p><strong>channels_in</strong> (<em>int</em><em>, </em><em>optional</em>) – Number of channels for the main input (<code class="docutils literal notranslate"><span class="pre">input1</span></code>).
Required if <code class="docutils literal notranslate"><span class="pre">internal_weights=True</span></code>.</p></li>
<li><p><strong>channels_out</strong> (<em>int</em><em>, </em><em>optional</em>) – Number of channels for the output.
Required if <code class="docutils literal notranslate"><span class="pre">internal_weights=True</span></code>.</p></li>
<li><p><strong>internal_weights</strong> (<em>bool</em><em>, </em><em>optional</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, the module manages its own weight parameter.
If <code class="docutils literal notranslate"><span class="pre">False</span></code>, weights must be provided during the forward pass. Defaults to <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p></li>
<li><p><strong>feature_mode</strong> (<em>str</em><em>, </em><em>optional</em>) – <p>Controls the type of linear operation: <code class="docutils literal notranslate"><span class="pre">{'uu',</span> <span class="pre">'uv'}</span></code>.
Defaults to <code class="docutils literal notranslate"><span class="pre">'uu'</span></code>.</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">'uu'</span></code>: Depthwise/elementwise linear. Assumes <code class="docutils literal notranslate"><span class="pre">channels_in</span> <span class="pre">==</span> <span class="pre">channels_out</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">'uv'</span></code>: Fully connected linear.</p></li>
</ul>
</p></li>
<li><p><strong>path_norm</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether to apply path normalization to the weights.
Normalizes by the square root of the number of paths to each output irrep. Defaults to <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p></li>
<li><p><strong>channel_norm</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether to apply channel normalization (specific to <code class="docutils literal notranslate"><span class="pre">'uv'</span></code> mode).
Divides weights by (sqrt{text{channels_in}}). Note: This interacts with <code class="docutils literal notranslate"><span class="pre">path_norm</span></code>.
Defaults to <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p><strong>path</strong> (<em>list</em><em>, </em><em>optional</em>) – Manually specify the coupling paths.
If <code class="docutils literal notranslate"><span class="pre">None</span></code>, all allowed paths are used. Defaults to <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="equitorch.nn.SO3Linear.weight">
<span class="sig-name descname"><span class="pre">weight</span></span><a class="headerlink" href="#equitorch.nn.SO3Linear.weight" title="Link to this definition"></a></dt>
<dd><p>The learnable weights of the module if <code class="docutils literal notranslate"><span class="pre">internal_weights=True</span></code>.
Shape depends on <code class="docutils literal notranslate"><span class="pre">feature_mode</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.nn.Parameter or None</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="equitorch.nn.SO3Linear.tp_info_forward">
<span class="sig-name descname"><span class="pre">tp_info_forward</span></span><a class="headerlink" href="#equitorch.nn.SO3Linear.tp_info_forward" title="Link to this definition"></a></dt>
<dd><p>Constant information for the forward pass computation.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="equitorch.structs.html#equitorch.structs.TensorProductInfo" title="equitorch.structs.TensorProductInfo">TensorProductInfo</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="equitorch.nn.SO3Linear.tp_info_backward1">
<span class="sig-name descname"><span class="pre">tp_info_backward1</span></span><a class="headerlink" href="#equitorch.nn.SO3Linear.tp_info_backward1" title="Link to this definition"></a></dt>
<dd><p>Constant information for the backward pass w.r.t. <code class="docutils literal notranslate"><span class="pre">input1</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="equitorch.structs.html#equitorch.structs.TensorProductInfo" title="equitorch.structs.TensorProductInfo">TensorProductInfo</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="equitorch.nn.SO3Linear.tp_info_backward2">
<span class="sig-name descname"><span class="pre">tp_info_backward2</span></span><a class="headerlink" href="#equitorch.nn.SO3Linear.tp_info_backward2" title="Link to this definition"></a></dt>
<dd><p>Constant information for the backward pass w.r.t. <code class="docutils literal notranslate"><span class="pre">input2</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="equitorch.structs.html#equitorch.structs.TensorProductInfo" title="equitorch.structs.TensorProductInfo">TensorProductInfo</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="equitorch.nn.SO3Linear.num_paths">
<span class="sig-name descname"><span class="pre">num_paths</span></span><a class="headerlink" href="#equitorch.nn.SO3Linear.num_paths" title="Link to this definition"></a></dt>
<dd><p>Number of coupling paths determined by the irreps.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="equitorch.nn.SO3Linear.weight_numel">
<span class="sig-name descname"><span class="pre">weight_numel</span></span><a class="headerlink" href="#equitorch.nn.SO3Linear.weight_numel" title="Link to this definition"></a></dt>
<dd><p>Total number of elements in the weight tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="id11">
<span class="sig-name descname"><span class="pre">tp_info_forward</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="equitorch.structs.html#equitorch.structs.TensorProductInfo" title="equitorch.structs.TensorProductInfo"><span class="pre">TensorProductInfo</span></a></em><a class="headerlink" href="#id11" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="id12">
<span class="sig-name descname"><span class="pre">tp_info_backward1</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="equitorch.structs.html#equitorch.structs.TensorProductInfo" title="equitorch.structs.TensorProductInfo"><span class="pre">TensorProductInfo</span></a></em><a class="headerlink" href="#id12" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="id13">
<span class="sig-name descname"><span class="pre">tp_info_backward2</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="equitorch.structs.html#equitorch.structs.TensorProductInfo" title="equitorch.structs.TensorProductInfo"><span class="pre">TensorProductInfo</span></a></em><a class="headerlink" href="#id13" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="equitorch.nn.SO3Linear.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#equitorch.nn.SO3Linear.training" title="Link to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="equitorch.nn.IrrepWiseLinear">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">equitorch.nn.</span></span><span class="sig-name descname"><span class="pre">IrrepWiseLinear</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">irreps</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">channels_in</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">channels_out</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">internal_weights</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">channel_norm</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#equitorch.nn.IrrepWiseLinear" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Irrep-wise linear layer (channel mixing).</p>
<p>Applies a separate linear transformation to the channels associated with each irrep type.
This operation does not change the spherical tensor structure (the irreps).</p>
<ul class="simple">
<li><p>Input shape: <code class="docutils literal notranslate"><span class="pre">(...,</span> <span class="pre">irreps.dim,</span> <span class="pre">channels_in)</span></code></p></li>
<li><p>Weight shape: <code class="docutils literal notranslate"><span class="pre">(num_paths,</span> <span class="pre">channels_in,</span> <span class="pre">channels_out)</span></code> where <code class="docutils literal notranslate"><span class="pre">num_paths</span></code> is the number of unique irreps in <code class="docutils literal notranslate"><span class="pre">irreps</span></code>.</p></li>
<li><p>Output shape: <code class="docutils literal notranslate"><span class="pre">(...,</span> <span class="pre">irreps.dim,</span> <span class="pre">channels_out)</span></code></p></li>
</ul>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>irreps</strong> (<a class="reference internal" href="equitorch.irreps.html#equitorch.irreps.Irreps" title="equitorch.irreps.Irreps"><em>Irreps</em></a><em> or </em><em>str</em>) – Irreducible representations of the input tensor.</p></li>
<li><p><strong>channels_in</strong> (<em>int</em>) – Number of input channels.</p></li>
<li><p><strong>channels_out</strong> (<em>int</em>) – Number of output channels.</p></li>
<li><p><strong>internal_weights</strong> (<em>bool</em><em>, </em><em>optional</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, the module manages its own weight parameter.
If <code class="docutils literal notranslate"><span class="pre">False</span></code>, weights must be provided during the forward pass. Defaults to <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p></li>
<li><p><strong>channel_norm</strong> (<em>bool</em><em>, </em><em>optional</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, divides the output by (sqrt{text{channels_in}}).
Defaults to <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="equitorch.nn.IrrepWiseLinear.weight">
<span class="sig-name descname"><span class="pre">weight</span></span><a class="headerlink" href="#equitorch.nn.IrrepWiseLinear.weight" title="Link to this definition"></a></dt>
<dd><p>The learnable weights of the module if <code class="docutils literal notranslate"><span class="pre">internal_weights=True</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.nn.Parameter or None</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="equitorch.nn.IrrepWiseLinear.irreps_info">
<span class="sig-name descname"><span class="pre">irreps_info</span></span><a class="headerlink" href="#equitorch.nn.IrrepWiseLinear.irreps_info" title="Link to this definition"></a></dt>
<dd><p>Constant information about the input irreps.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="equitorch.structs.html#equitorch.structs.IrrepsInfo" title="equitorch.structs.IrrepsInfo">IrrepsInfo</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="equitorch.nn.IrrepWiseLinear.num_paths">
<span class="sig-name descname"><span class="pre">num_paths</span></span><a class="headerlink" href="#equitorch.nn.IrrepWiseLinear.num_paths" title="Link to this definition"></a></dt>
<dd><p>Number of unique irreps in the input.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="equitorch.nn.IrrepWiseLinear.weight_numel">
<span class="sig-name descname"><span class="pre">weight_numel</span></span><a class="headerlink" href="#equitorch.nn.IrrepWiseLinear.weight_numel" title="Link to this definition"></a></dt>
<dd><p>Total number of elements in the weight tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="id14">
<span class="sig-name descname"><span class="pre">irreps_info</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="equitorch.structs.html#equitorch.structs.IrrepsInfo" title="equitorch.structs.IrrepsInfo"><span class="pre">IrrepsInfo</span></a></em><a class="headerlink" href="#id14" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="equitorch.nn.IrrepWiseLinear.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#equitorch.nn.IrrepWiseLinear.training" title="Link to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="equitorch.nn.IrrepsLinear">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">equitorch.nn.</span></span><span class="sig-name descname"><span class="pre">IrrepsLinear</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">irreps_in</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">irreps_out</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">channels_in</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">channels_out</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">internal_weights</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">path_norm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">channel_norm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">path</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#equitorch.nn.IrrepsLinear" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Equivariant linear layer that preserves the spherical tensor structure but mixes channels.</p>
<p>This layer applies a linear transformation across channels while respecting the
equivariance constraints imposed by the input and output irreps. It only allows
paths where the input and output irreps are the same ((l_{in} = l_{out}) and ((p_{in} = p_{out}) or (p_{out}=0))).
This is often used for channel mixing in equivariant networks.</p>
<ul class="simple">
<li><p>Input shape: <code class="docutils literal notranslate"><span class="pre">(...,</span> <span class="pre">irreps_in.dim,</span> <span class="pre">channels_in)</span></code></p></li>
<li><p>Weight shape: <code class="docutils literal notranslate"><span class="pre">(num_paths,</span> <span class="pre">channels_in,</span> <span class="pre">channels_out)</span></code> where <code class="docutils literal notranslate"><span class="pre">num_paths</span></code> is the number of allowed paths.</p></li>
<li><p>Output shape: <code class="docutils literal notranslate"><span class="pre">(...,</span> <span class="pre">irreps_out.dim,</span> <span class="pre">channels_out)</span></code></p></li>
</ul>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>irreps_in</strong> (<a class="reference internal" href="equitorch.irreps.html#equitorch.irreps.Irreps" title="equitorch.irreps.Irreps"><em>Irreps</em></a>) – Irreducible representations of the input tensor.</p></li>
<li><p><strong>irreps_out</strong> (<a class="reference internal" href="equitorch.irreps.html#equitorch.irreps.Irreps" title="equitorch.irreps.Irreps"><em>Irreps</em></a>) – Irreducible representations of the output tensor.</p></li>
<li><p><strong>channels_in</strong> (<em>int</em>) – Number of input channels.</p></li>
<li><p><strong>channels_out</strong> (<em>int</em>) – Number of output channels.</p></li>
<li><p><strong>internal_weights</strong> (<em>bool</em><em>, </em><em>optional</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, the module manages its own weight parameter.
If <code class="docutils literal notranslate"><span class="pre">False</span></code>, weights must be provided during the forward pass. Defaults to <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p></li>
<li><p><strong>path_norm</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether to apply path normalization to the weights.
Normalizes by the square root of the number of paths to each output irrep. Defaults to <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p></li>
<li><p><strong>channel_norm</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether to apply channel normalization.
Divides weights by (sqrt{text{channels_in}}).
Defaults to <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p><strong>path</strong> (<em>list</em><em>, </em><em>optional</em>) – Manually specify the coupling paths.
If <code class="docutils literal notranslate"><span class="pre">None</span></code>, all allowed paths are used. Defaults to <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="equitorch.nn.IrrepsLinear.weight">
<span class="sig-name descname"><span class="pre">weight</span></span><a class="headerlink" href="#equitorch.nn.IrrepsLinear.weight" title="Link to this definition"></a></dt>
<dd><p>The learnable weights of the module if <code class="docutils literal notranslate"><span class="pre">internal_weights=True</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.nn.Parameter or None</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="equitorch.nn.IrrepsLinear.forward_info">
<span class="sig-name descname"><span class="pre">forward_info</span></span><a class="headerlink" href="#equitorch.nn.IrrepsLinear.forward_info" title="Link to this definition"></a></dt>
<dd><p>Constant information for the forward pass computation.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="equitorch.structs.html#equitorch.structs.IrrepsLinearInfo" title="equitorch.structs.IrrepsLinearInfo">IrrepsLinearInfo</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="equitorch.nn.IrrepsLinear.backward_info">
<span class="sig-name descname"><span class="pre">backward_info</span></span><a class="headerlink" href="#equitorch.nn.IrrepsLinear.backward_info" title="Link to this definition"></a></dt>
<dd><p>Constant information for the backward pass computation.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="equitorch.structs.html#equitorch.structs.IrrepsLinearInfo" title="equitorch.structs.IrrepsLinearInfo">IrrepsLinearInfo</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="equitorch.nn.IrrepsLinear.num_paths">
<span class="sig-name descname"><span class="pre">num_paths</span></span><a class="headerlink" href="#equitorch.nn.IrrepsLinear.num_paths" title="Link to this definition"></a></dt>
<dd><p>Number of allowed coupling paths.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="equitorch.nn.IrrepsLinear.weight_numel">
<span class="sig-name descname"><span class="pre">weight_numel</span></span><a class="headerlink" href="#equitorch.nn.IrrepsLinear.weight_numel" title="Link to this definition"></a></dt>
<dd><p>Total number of elements in the weight tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="id15">
<span class="sig-name descname"><span class="pre">forward_info</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="equitorch.structs.html#equitorch.structs.IrrepsLinearInfo" title="equitorch.structs.IrrepsLinearInfo"><span class="pre">IrrepsLinearInfo</span></a></em><a class="headerlink" href="#id15" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="id16">
<span class="sig-name descname"><span class="pre">backward_info</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="equitorch.structs.html#equitorch.structs.IrrepsLinearInfo" title="equitorch.structs.IrrepsLinearInfo"><span class="pre">IrrepsLinearInfo</span></a></em><a class="headerlink" href="#id16" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="equitorch.nn.IrrepsLinear.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#equitorch.nn.IrrepsLinear.training" title="Link to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="equitorch.nn.SO2Linear">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">equitorch.nn.</span></span><span class="sig-name descname"><span class="pre">SO2Linear</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">irreps_in</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">irreps_out</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">channels_in</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">channels_out</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">internal_weights</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">feature_mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'uu'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">path_norm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">channel_norm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">path</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#equitorch.nn.SO2Linear" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>SO(2) equivariant linear layer using tensor products.</p>
<p>This layer applies an SO(2) equivariant linear transformation, as proposed in <a class="reference external" href="https://arxiv.org/abs/2302.03655">Reducing SO(3) Convolutions to SO(2) for Efficient Equivariant GNNs</a>.
It supports two main modes controlled by <code class="docutils literal notranslate"><span class="pre">feature_mode</span></code>:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">'uv'</span></code>: Fully connected linear layer.</p>
<ul>
<li><p>Input shape: <code class="docutils literal notranslate"><span class="pre">(...,</span> <span class="pre">irreps_in.dim,</span> <span class="pre">channels_in)</span></code></p></li>
<li><p>Weight shape: <code class="docutils literal notranslate"><span class="pre">(num_weights,</span> <span class="pre">channels_in,</span> <span class="pre">channels_out)</span></code></p></li>
<li><p>Output shape: <code class="docutils literal notranslate"><span class="pre">(...,</span> <span class="pre">irreps_out.dim,</span> <span class="pre">channels_out)</span></code></p></li>
</ul>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">'uu'</span></code>: Depthwise/elementwise linear layer.</p>
<ul>
<li><p>Input shape: <code class="docutils literal notranslate"><span class="pre">(...,</span> <span class="pre">irreps_in.dim,</span> <span class="pre">channels)</span></code></p></li>
<li><p>Weight shape: <code class="docutils literal notranslate"><span class="pre">(num_weights,</span> <span class="pre">channels_out)</span></code></p></li>
<li><p>Output shape: <code class="docutils literal notranslate"><span class="pre">(...,</span> <span class="pre">irreps_out.dim,</span> <span class="pre">channels_out)</span></code></p></li>
</ul>
</li>
</ul>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>irreps_in</strong> (<a class="reference internal" href="equitorch.irreps.html#equitorch.irreps.Irreps" title="equitorch.irreps.Irreps"><em>Irreps</em></a><em> or </em><em>str</em>) – Irreducible representations of the input tensor.</p></li>
<li><p><strong>irreps_out</strong> (<a class="reference internal" href="equitorch.irreps.html#equitorch.irreps.Irreps" title="equitorch.irreps.Irreps"><em>Irreps</em></a><em> or </em><em>str</em>) – Irreducible representations of the output tensor.</p></li>
<li><p><strong>channels_in</strong> (<em>int</em><em>, </em><em>optional</em>) – Number of channels for the input.
Required if <code class="docutils literal notranslate"><span class="pre">internal_weights=True</span></code>.</p></li>
<li><p><strong>channels_out</strong> (<em>int</em><em>, </em><em>optional</em>) – Number of channels for the output.
Required if <code class="docutils literal notranslate"><span class="pre">internal_weights=True</span></code>.</p></li>
<li><p><strong>internal_weights</strong> (<em>bool</em><em>, </em><em>optional</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, the module manages its own weight parameter.
If <code class="docutils literal notranslate"><span class="pre">False</span></code>, weights must be provided during the forward pass. Defaults to <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p></li>
<li><p><strong>feature_mode</strong> (<em>str</em><em>, </em><em>optional</em>) – <p>Controls the type of linear operation: <code class="docutils literal notranslate"><span class="pre">{'uu',</span> <span class="pre">'uv'}</span></code>.
Defaults to <code class="docutils literal notranslate"><span class="pre">'uu'</span></code>.</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">'uu'</span></code>: Depthwise/elementwise linear. Assumes <code class="docutils literal notranslate"><span class="pre">channels_in</span> <span class="pre">==</span> <span class="pre">channels_out</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">'uv'</span></code>: Fully connected linear.</p></li>
</ul>
</p></li>
<li><p><strong>path_norm</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether to apply path normalization to the weights.
Normalizes by the square root of the number of paths to each output irrep. Defaults to <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p></li>
<li><p><strong>channel_norm</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether to apply channel normalization (specific to <code class="docutils literal notranslate"><span class="pre">'uv'</span></code> mode).
Divides weights by (sqrt{text{channels_in}}). Note: This interacts with <code class="docutils literal notranslate"><span class="pre">path_norm</span></code>.
Defaults to <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p><strong>path</strong> (<em>list</em><em>, </em><em>optional</em>) – Manually specify the coupling paths.
If <code class="docutils literal notranslate"><span class="pre">None</span></code>, all allowed paths are used. Defaults to <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="equitorch.nn.SO2Linear.weight">
<span class="sig-name descname"><span class="pre">weight</span></span><a class="headerlink" href="#equitorch.nn.SO2Linear.weight" title="Link to this definition"></a></dt>
<dd><p>The learnable weights of the module if <code class="docutils literal notranslate"><span class="pre">internal_weights=True</span></code>.
Shape depends on <code class="docutils literal notranslate"><span class="pre">feature_mode</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.nn.Parameter or None</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="equitorch.nn.SO2Linear.info_forward">
<span class="sig-name descname"><span class="pre">info_forward</span></span><a class="headerlink" href="#equitorch.nn.SO2Linear.info_forward" title="Link to this definition"></a></dt>
<dd><p>Constant information for the forward pass computation.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="equitorch.structs.html#equitorch.structs.SparseProductInfo" title="equitorch.structs.SparseProductInfo">SparseProductInfo</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="equitorch.nn.SO2Linear.info_backward1">
<span class="sig-name descname"><span class="pre">info_backward1</span></span><a class="headerlink" href="#equitorch.nn.SO2Linear.info_backward1" title="Link to this definition"></a></dt>
<dd><p>Constant information for the first backward pass.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="equitorch.structs.html#equitorch.structs.SparseProductInfo" title="equitorch.structs.SparseProductInfo">SparseProductInfo</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="equitorch.nn.SO2Linear.info_backward2">
<span class="sig-name descname"><span class="pre">info_backward2</span></span><a class="headerlink" href="#equitorch.nn.SO2Linear.info_backward2" title="Link to this definition"></a></dt>
<dd><p>Constant information for the second backward pass.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="equitorch.structs.html#equitorch.structs.SparseProductInfo" title="equitorch.structs.SparseProductInfo">SparseProductInfo</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="equitorch.nn.SO2Linear.num_paths">
<span class="sig-name descname"><span class="pre">num_paths</span></span><a class="headerlink" href="#equitorch.nn.SO2Linear.num_paths" title="Link to this definition"></a></dt>
<dd><p>Number of coupling paths determined by the irreps.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="equitorch.nn.SO2Linear.weight_numel">
<span class="sig-name descname"><span class="pre">weight_numel</span></span><a class="headerlink" href="#equitorch.nn.SO2Linear.weight_numel" title="Link to this definition"></a></dt>
<dd><p>Total number of elements in the weight tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="equitorch.nn.SplitIrreps">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">equitorch.nn.</span></span><span class="sig-name descname"><span class="pre">SplitIrreps</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">irreps</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="equitorch.irreps.html#equitorch.irreps.Irreps" title="equitorch.irreps.Irreps"><span class="pre">Irreps</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">split_num_irreps</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">-2</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#equitorch.nn.SplitIrreps" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>A module that splits input tensors according to specified irrep segments.</p>
<p>The splitting is done based on the dimensions of the irreducible representations.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>irreps</strong> (<a class="reference internal" href="equitorch.irreps.html#equitorch.irreps.Irreps" title="equitorch.irreps.Irreps"><em>Irreps</em></a>) – The irreducible representations specification.</p></li>
<li><p><strong>split_num_irreps</strong> (<em>Iterable</em><em>[</em><em>int</em><em>]</em>) – Number of irreps in each split segment.
Must sum to total irreps. May contain at most one -1 or <code class="docutils literal notranslate"><span class="pre">...</span></code>
to represent the remaining irreps.</p></li>
<li><p><strong>dim</strong> (<em>int</em><em>, </em><em>optional</em>) – The dimension along which to split the input tensor.
Defaults to -2.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Split 3 scalar irreps (dim=1 each) and 2 vector irreps (dim=3 each)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">irreps</span> <span class="o">=</span> <span class="n">Irreps</span><span class="p">(</span><span class="s2">&quot;3x0e + 2x1o&quot;</span><span class="p">)</span>  <span class="c1"># 3 scalars + 2 vectors</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">split</span> <span class="o">=</span> <span class="n">SplitIrreps</span><span class="p">(</span><span class="n">irreps</span><span class="p">,</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>  <span class="c1"># Split first 2 irreps, then remaining</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">irreps</span><span class="o">.</span><span class="n">dim</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>  <span class="c1"># batch=5, dim=9 (3*1 + 2*3), channels=10</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">splits</span> <span class="o">=</span> <span class="n">split</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># Returns list of 2 tensors</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">splits</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span>
<span class="go">torch.Size([5, 2, 10])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">splits</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span>
<span class="go">torch.Size([5, 7, 10])</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Using ... for automatic size calculation</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">split</span> <span class="o">=</span> <span class="n">SplitIrreps</span><span class="p">(</span><span class="n">irreps</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="o">...</span><span class="p">])</span>  <span class="c1"># First irrep, then remaining</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">splits</span> <span class="o">=</span> <span class="n">split</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">splits</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span>
<span class="go">torch.Size([5, 1, 10])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">splits</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span>
<span class="go">torch.Size([5, 8, 10])</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="equitorch.nn.Separable">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">equitorch.nn.</span></span><span class="sig-name descname"><span class="pre">Separable</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">irreps</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="equitorch.irreps.html#equitorch.irreps.Irreps" title="equitorch.irreps.Irreps"><span class="pre">Irreps</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">split_num_irreps</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sub_modules</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><span class="pre">Module</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cat_after</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">-2</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#equitorch.nn.Separable" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>A module that applies different transformations to different parts of input tensor
according to irreducible representations (irreps), with optional concatenation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>irreps</strong> (<a class="reference internal" href="equitorch.irreps.html#equitorch.irreps.Irreps" title="equitorch.irreps.Irreps"><em>Irreps</em></a>) – The irreducible representations specification for the input tensor.</p></li>
<li><p><strong>split_num_irreps</strong> (<em>Iterable</em><em>[</em><em>int</em><em>]</em>) – Number of irreps in each split segment.
May contain at most one -1 or <code class="docutils literal notranslate"><span class="pre">...</span></code> to represent remaining irreps.
Length must match length of <code class="docutils literal notranslate"><span class="pre">sub_modules</span></code>.</p></li>
<li><p><strong>sub_modules</strong> (<em>Iterable</em><em>[</em><em>Callable</em><em>]</em>) – Transformation modules for each split segment.
Use <code class="docutils literal notranslate"><span class="pre">None</span></code> for identity operation.</p></li>
<li><p><strong>cat_after</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether to concatenate results after transformation.
Defaults to True.</p></li>
<li><p><strong>dim</strong> (<em>int</em><em>, </em><em>optional</em>) – The dimension along which to split and concatenate tensors.
Defaults to -2.</p></li>
</ul>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>ValueError</strong> – <ul class="simple">
<li><p>If lengths of <code class="docutils literal notranslate"><span class="pre">split_num_irreps</span></code> and <code class="docutils literal notranslate"><span class="pre">sub_modules</span></code> don’t match.
    - If sum of <code class="docutils literal notranslate"><span class="pre">split_num_irreps</span></code> doesn’t match total irreps.
    - If invalid <code class="docutils literal notranslate"><span class="pre">split_num_irreps</span></code> specification.</p></li>
</ul>
</p>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="equitorch.nn.SphericalHarmonics">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">equitorch.nn.</span></span><span class="sig-name descname"><span class="pre">SphericalHarmonics</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">l_max</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">normalize_input</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">integral_normalize</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#equitorch.nn.SphericalHarmonics" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Computes spherical harmonics from input Cartesian coordinates.
Wraps the functional <a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.sphericals.spherical_harmonics" title="equitorch.nn.functional.sphericals.spherical_harmonics"><code class="xref py py-func docutils literal notranslate"><span class="pre">spherical_harmonics()</span></code></a>.</p>
<p>Spherical harmonics are a set of orthogonal functions defined on the surface of a sphere.
They are solutions to Laplace’s equation in spherical coordinates.</p>
<p>If <cite>integral_normalize</cite> is True, the output is scaled by (1 / sqrt{4pi}).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>l_max</strong> (<em>int</em>) – The maximum degree of the spherical harmonics.</p></li>
<li><p><strong>normalize_input</strong> (<em>bool</em><em>, </em><em>optional</em>) – If True, normalizes the input xyz vector before
computing spherical harmonics. Defaults to True.</p></li>
<li><p><strong>integral_normalize</strong> (<em>bool</em><em>, </em><em>optional</em>) – If True, applies normalization for integration over the sphere.
Defaults to False.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="equitorch.nn.XYZToSpherical">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">equitorch.nn.</span></span><span class="sig-name descname"><span class="pre">XYZToSpherical</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">normalize_input</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">with_r</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1e-14</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">-1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#equitorch.nn.XYZToSpherical" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Module to convert Cartesian coordinates ((x, y, z)) to spherical ((theta, phi, r)).
Wraps the functional <a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.sphericals.xyz_to_spherical" title="equitorch.nn.functional.sphericals.xyz_to_spherical"><code class="xref py py-func docutils literal notranslate"><span class="pre">xyz_to_spherical()</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>normalize_input</strong> (<em>bool</em><em>, </em><em>optional</em>) – If True, normalizes the input <cite>xyz</cite> vector before
computing angles. Defaults to True.</p></li>
<li><p><strong>with_r</strong> (<em>bool</em><em>, </em><em>optional</em>) – If True, returns (r) along with (theta) and (phi).
Defaults to False.</p></li>
<li><p><strong>eps</strong> (<em>float</em><em>, </em><em>optional</em>) – Small (epsilon) for numerical stability. Defaults to 1e-14.</p></li>
<li><p><strong>dim</strong> (<em>int</em><em>, </em><em>optional</em>) – Dimension of Cartesian coordinates. Defaults to -1.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="equitorch.nn.SphericalToXYZ">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">equitorch.nn.</span></span><span class="sig-name descname"><span class="pre">SphericalToXYZ</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">-1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#equitorch.nn.SphericalToXYZ" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Module to convert spherical coordinates ((theta, phi, r)) to Cartesian ((x, y, z)).
Wraps the functional <a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.sphericals.spherical_to_xyz" title="equitorch.nn.functional.sphericals.spherical_to_xyz"><code class="xref py py-func docutils literal notranslate"><span class="pre">spherical_to_xyz()</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>dim</strong> (<em>int</em><em>, </em><em>optional</em>) – Dimension along which to stack output (x,y,z). Defaults to -1.</p>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="equitorch.nn.XYZToSinCos">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">equitorch.nn.</span></span><span class="sig-name descname"><span class="pre">XYZToSinCos</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">max_m</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">normalize_input</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">component_normalize</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1e-14</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">-1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#equitorch.nn.XYZToSinCos" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Module to convert Cartesian coordinates ((x, y, z)) to sin/cos embeddings
of the spherical angles (theta) and (phi).
Wraps the functional <a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.sphericals.xyz_to_sincos" title="equitorch.nn.functional.sphericals.xyz_to_sincos"><code class="xref py py-func docutils literal notranslate"><span class="pre">xyz_to_sincos()</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>max_m</strong> (<em>int</em>) – The maximum multiple of the angles to compute (sin) / (cos) for.</p></li>
<li><p><strong>normalize_input</strong> (<em>bool</em><em>, </em><em>optional</em>) – If True, normalizes <cite>xyz</cite> before extracting angles.
Defaults to True.</p></li>
<li><p><strong>component_normalize</strong> – (bool, optional): False</p></li>
<li><p><strong>eps</strong> (<em>float</em><em>, </em><em>optional</em>) – Small (epsilon) for numerical stability. Defaults to 1e-14.</p></li>
<li><p><strong>dim</strong> (<em>int</em><em>, </em><em>optional</em>) – Dimension of Cartesian coordinates in <cite>xyz</cite>. Defaults to -1.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="equitorch.nn.BatchRMSNorm">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">equitorch.nn.</span></span><span class="sig-name descname"><span class="pre">BatchRMSNorm</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">irreps</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="equitorch.irreps.html#equitorch.irreps.Irreps" title="equitorch.irreps.Irreps"><span class="pre">Irreps</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">channels</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1e-05</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">momentum</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">affine</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scaled</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#equitorch.nn.BatchRMSNorm" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Applies Batch Root Mean Square Normalization for equivariant features.</p>
<div class="math notranslate nohighlight">
\[x'_{nimc} = \gamma_{ic} \cdot (x_{nimc} / \sigma_{ic})\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[\sigma_{ic} = \sqrt{E[\text{SquaredNorm}(x_{nic})] + \epsilon}\]</div>
<p>The <a class="reference internal" href="#equitorch.nn.norm.SquaredNorm" title="equitorch.nn.norm.SquaredNorm"><code class="xref py py-class docutils literal notranslate"><span class="pre">SquaredNorm</span></code></a> can be scaled by (1/text{irrep}_itext{.dim})
depending on the <code class="docutils literal notranslate"><span class="pre">scaled</span></code> argument.
Running statistics are used during evaluation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>irreps</strong> (<a class="reference internal" href="equitorch.irreps.html#equitorch.irreps.Irreps" title="equitorch.irreps.Irreps"><em>Irreps</em></a>) – Irreducible representations of the input tensor.</p></li>
<li><p><strong>channels</strong> (<em>int</em>) – Number of channels in the input tensor (size of the last dimension).</p></li>
<li><p><strong>eps</strong> (<em>float</em><em>, </em><em>optional</em>) – A value added to the denominator for numerical stability. (epsilon)
Defaults to 1e-5.</p></li>
<li><p><strong>momentum</strong> (<em>float</em><em>, </em><em>optional</em>) – The value used for the running_mean computation.
Defaults to 0.1.</p></li>
<li><p><strong>affine</strong> (<em>bool</em><em>, </em><em>optional</em>) – If True, this module has learnable affine parameters (weight (gamma_{ic})).
Defaults to True.</p></li>
<li><p><strong>scaled</strong> (<em>bool</em><em>, </em><em>optional</em>) – If True, the <a class="reference internal" href="#equitorch.nn.norm.SquaredNorm" title="equitorch.nn.norm.SquaredNorm"><code class="xref py py-class docutils literal notranslate"><span class="pre">SquaredNorm</span></code></a> used for calculating statistics
is scaled by (1/text{irrep}_itext{.dim}). Defaults to True.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="equitorch.nn.BatchRMSNorm.reset_parameters">
<span class="sig-name descname"><span class="pre">reset_parameters</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#equitorch.nn.BatchRMSNorm.reset_parameters" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="equitorch.nn.BatchRMSNorm.reset_running_stats">
<span class="sig-name descname"><span class="pre">reset_running_stats</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#equitorch.nn.BatchRMSNorm.reset_running_stats" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="equitorch.nn.BatchRMSNorm.irreps_info">
<span class="sig-name descname"><span class="pre">irreps_info</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="equitorch.structs.html#equitorch.structs.IrrepsInfo" title="equitorch.structs.IrrepsInfo"><span class="pre">IrrepsInfo</span></a></em><a class="headerlink" href="#equitorch.nn.BatchRMSNorm.irreps_info" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="equitorch.nn.BatchRMSNorm.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#equitorch.nn.BatchRMSNorm.training" title="Link to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="equitorch.nn.LayerRMSNorm">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">equitorch.nn.</span></span><span class="sig-name descname"><span class="pre">LayerRMSNorm</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">irreps</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="equitorch.irreps.html#equitorch.irreps.Irreps" title="equitorch.irreps.Irreps"><span class="pre">Irreps</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">channels</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1e-05</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">affine</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scaled</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#equitorch.nn.LayerRMSNorm" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Applies Irrep-wise Layer Root Mean Square Normalization.</p>
<p>Computes statistics independently for each irrep instance within each sample.
Normalizes using the RMS value calculated across channels and irrep components
for that specific sample and irrep instance.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>irreps</strong> (<a class="reference internal" href="equitorch.irreps.html#equitorch.irreps.Irreps" title="equitorch.irreps.Irreps"><em>Irreps</em></a>) – Irreducible representations of the input tensor.</p></li>
<li><p><strong>channels</strong> (<em>int</em>) – Number of channels in the input tensor (size of the last dimension).</p></li>
<li><p><strong>eps</strong> (<em>float</em><em>, </em><em>optional</em>) – A value added to the denominator for numerical stability. (epsilon)
Defaults to 1e-5.</p></li>
<li><p><strong>affine</strong> (<em>bool</em><em>, </em><em>optional</em>) – If True, this module has learnable affine parameters (weight (gamma_{ic})).
Defaults to True.</p></li>
<li><p><strong>scaled</strong> (<em>bool</em><em>, </em><em>optional</em>) – If True, the statistics calculation considers the norm
to be scaled by (1/text{irrep}_itext{.dim}).
Defaults to True.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="equitorch.nn.LayerRMSNorm.reset_parameters">
<span class="sig-name descname"><span class="pre">reset_parameters</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#equitorch.nn.LayerRMSNorm.reset_parameters" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="equitorch.nn.LayerRMSNorm.irreps_info">
<span class="sig-name descname"><span class="pre">irreps_info</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="equitorch.structs.html#equitorch.structs.IrrepsInfo" title="equitorch.structs.IrrepsInfo"><span class="pre">IrrepsInfo</span></a></em><a class="headerlink" href="#equitorch.nn.LayerRMSNorm.irreps_info" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="equitorch.nn.LayerRMSNorm.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#equitorch.nn.LayerRMSNorm.training" title="Link to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="equitorch.nn.initialize_tensor_product">
<span class="sig-prename descclassname"><span class="pre">equitorch.nn.</span></span><span class="sig-name descname"><span class="pre">initialize_tensor_product</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">weight</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">feature_mode</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gain</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">channel_normed</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#equitorch.nn.initialize_tensor_product" title="Link to this definition"></a></dt>
<dd><p>Initialize weights for tensor product operations.</p>
<p>This function initializes weights for tensor product operations with different
feature modes. The initialization uses a uniform distribution with bounds
calculated based on the feature mode and whether channel normalization is used.</p>
<p>For ‘uvw’ mode:</p>
<div class="math notranslate nohighlight">
\[\begin{split}a = \begin{cases}
\sqrt{3} \cdot \text{gain}, &amp; \text{if channel_normed} = \text{True} \\
\sqrt{\frac{3}{\text{fan_in}}} \cdot \text{gain}, &amp; \text{otherwise}
\end{cases}\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\text{fan_in} = \text{weight.shape[-2]} \cdot \text{weight.shape[-3]}\)</span></p>
<p>For ‘uuu’ mode:</p>
<div class="math notranslate nohighlight">
\[a = \sqrt{3} \cdot \text{gain}\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>weight</strong> (<em>torch.Tensor</em>) – The weight tensor to initialize.</p></li>
<li><p><strong>feature_mode</strong> (<em>str</em>) – The feature mode for initialization. Must be one of [‘uvw’, ‘uuu’].</p></li>
<li><p><strong>gain</strong> (<em>float</em><em>, </em><em>optional</em>) – The gain factor to apply. Default is 1.</p></li>
<li><p><strong>channel_normed</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether channel normalization is used. Default is False.</p></li>
</ul>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>ValueError</strong> – If an unknown feature_mode is provided.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="equitorch.nn.initialize_so3_so2_linear">
<span class="sig-prename descclassname"><span class="pre">equitorch.nn.</span></span><span class="sig-name descname"><span class="pre">initialize_so3_so2_linear</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">weight</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">feature_mode</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gain</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">channel_normed</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#equitorch.nn.initialize_so3_so2_linear" title="Link to this definition"></a></dt>
<dd><p>Initialize weights for SO(3) or SO(2) linear operations.</p>
<p>This function initializes weights for SO(3) or SO(2) linear operations with different
feature modes. The initialization uses a uniform distribution with bounds
calculated based on the feature mode and whether channel normalization is used.</p>
<p>For ‘uv’ mode:</p>
<div class="math notranslate nohighlight">
\[\begin{split}a = \begin{cases}
\sqrt{3} \cdot \text{gain}, &amp; \text{if channel_normed} = \text{True} \\
\sqrt{\frac{3}{\text{fan_in}}} \cdot \text{gain}, &amp; \text{otherwise}
\end{cases}\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\text{fan_in} = \text{weight.shape[-2]}\)</span></p>
<p>For ‘uu’ mode:</p>
<div class="math notranslate nohighlight">
\[a = \sqrt{3} \cdot \text{gain}\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>weight</strong> (<em>torch.Tensor</em>) – The weight tensor to initialize.</p></li>
<li><p><strong>feature_mode</strong> (<em>str</em>) – The feature mode for initialization. Must be one of [‘uv’, ‘uu’].</p></li>
<li><p><strong>gain</strong> (<em>float</em><em>, </em><em>optional</em>) – The gain factor to apply. Default is 1.</p></li>
<li><p><strong>channel_normed</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether channel normalization is used. Default is False.</p></li>
</ul>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>ValueError</strong> – If an unknown feature_mode is provided.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="equitorch.nn.initialize_linear">
<span class="sig-prename descclassname"><span class="pre">equitorch.nn.</span></span><span class="sig-name descname"><span class="pre">initialize_linear</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">weight</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gain</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">channel_normed</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#equitorch.nn.initialize_linear" title="Link to this definition"></a></dt>
<dd><p>Initialize weights for standard linear operations.</p>
<p>This function initializes weights for standard linear operations using a uniform
distribution with bounds calculated based on whether channel normalization is used.</p>
<div class="math notranslate nohighlight">
\[\begin{split}a = \begin{cases}
\sqrt{3} \cdot \text{gain}, &amp; \text{if channel_normed} = \text{True} \\
\sqrt{\frac{3}{\text{fan_in}}} \cdot \text{gain}, &amp; \text{otherwise}
\end{cases}\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\text{fan_in} = \text{weight.shape[-2]}\)</span></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>weight</strong> (<em>torch.Tensor</em>) – The weight tensor to initialize.</p></li>
<li><p><strong>gain</strong> (<em>float</em><em>, </em><em>optional</em>) – The gain factor to apply. Default is 1.</p></li>
<li><p><strong>channel_normed</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether channel normalization is used. Default is False.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="equitorch.nn.TensorProduct">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">equitorch.nn.</span></span><span class="sig-name descname"><span class="pre">TensorProduct</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">irreps_in1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">irreps_in2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">irreps_out</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">channels_in1</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">channels_in2</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">channels_out</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">internal_weights</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">feature_mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'uuu'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">path_norm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">channel_norm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">path</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#equitorch.nn.TensorProduct" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Computes the tensor product of two equivariant feature tensors.</p>
<p>Supports two main modes controlled by <code class="docutils literal notranslate"><span class="pre">feature_mode</span></code>:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">'uvw'</span></code>: Fully connected tensor product.</p>
<ul>
<li><p>Input1 shape: <code class="docutils literal notranslate"><span class="pre">(...,</span> <span class="pre">irreps_in1.dim,</span> <span class="pre">channels_in1)</span></code></p></li>
<li><p>Input2 shape: <code class="docutils literal notranslate"><span class="pre">(...,</span> <span class="pre">irreps_in2.dim,</span> <span class="pre">channels_in2)</span></code></p></li>
<li><p>Weight shape: <code class="docutils literal notranslate"><span class="pre">(num_paths,</span> <span class="pre">channels_in1,</span> <span class="pre">channels_in2,</span> <span class="pre">channels_out)</span></code></p></li>
<li><p>Output shape: <code class="docutils literal notranslate"><span class="pre">(...,</span> <span class="pre">irreps_out.dim,</span> <span class="pre">channels_out)</span></code></p></li>
</ul>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">'uuu'</span></code>: Depthwise/elementwise tensor product.
with <code class="docutils literal notranslate"><span class="pre">uuu</span></code> instructions (often used for self-interaction).</p>
<ul>
<li><p>Input1 shape: <code class="docutils literal notranslate"><span class="pre">(...,</span> <span class="pre">irreps_in1.dim,</span> <span class="pre">channels)</span></code></p></li>
<li><p>Input2 shape: <code class="docutils literal notranslate"><span class="pre">(...,</span> <span class="pre">irreps_in2.dim,</span> <span class="pre">channels)</span></code></p></li>
<li><p>Weight shape: <code class="docutils literal notranslate"><span class="pre">(num_paths,</span> <span class="pre">channels_out)</span></code> (where <code class="docutils literal notranslate"><span class="pre">channels_out</span></code> usually equals <code class="docutils literal notranslate"><span class="pre">channels</span></code>)</p></li>
<li><p>Output shape: <code class="docutils literal notranslate"><span class="pre">(...,</span> <span class="pre">irreps_out.dim,</span> <span class="pre">channels_out)</span></code></p></li>
</ul>
</li>
</ul>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>irreps_in1</strong> (<a class="reference internal" href="equitorch.irreps.html#equitorch.irreps.Irreps" title="equitorch.irreps.Irreps"><em>Irreps</em></a>) – Irreducible representations of the first input tensor.</p></li>
<li><p><strong>irreps_in2</strong> (<a class="reference internal" href="equitorch.irreps.html#equitorch.irreps.Irreps" title="equitorch.irreps.Irreps"><em>Irreps</em></a>) – Irreducible representations of the second input tensor.</p></li>
<li><p><strong>irreps_out</strong> (<a class="reference internal" href="equitorch.irreps.html#equitorch.irreps.Irreps" title="equitorch.irreps.Irreps"><em>Irreps</em></a>) – Irreducible representations of the output tensor.</p></li>
<li><p><strong>channels_in1</strong> (<em>int</em><em>, </em><em>optional</em>) – Number of channels for the first input.
Required if <code class="docutils literal notranslate"><span class="pre">internal_weights=True</span></code> or <code class="docutils literal notranslate"><span class="pre">feature_mode='uvw'</span></code>.</p></li>
<li><p><strong>channels_in2</strong> (<em>int</em><em>, </em><em>optional</em>) – Number of channels for the second input.
Required if <code class="docutils literal notranslate"><span class="pre">internal_weights=True</span></code> or <code class="docutils literal notranslate"><span class="pre">feature_mode='uvw'</span></code>.</p></li>
<li><p><strong>channels_out</strong> (<em>int</em><em>, </em><em>optional</em>) – Number of channels for the output.
Required if <code class="docutils literal notranslate"><span class="pre">internal_weights=True</span></code>.</p></li>
<li><p><strong>internal_weights</strong> (<em>bool</em><em>, </em><em>default=True</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, the module manages its own weight parameter.
If <code class="docutils literal notranslate"><span class="pre">False</span></code>, weights must be provided during the forward pass.</p></li>
<li><p><strong>feature_mode</strong> (<em>{'uuu'</em><em>, </em><em>'uvw'}</em><em>, </em><em>default='uuu'</em>) – <p>Controls the type of tensor product:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">'uuu'</span></code>: Depthwise/elementwise product. Assumes <code class="docutils literal notranslate"><span class="pre">channels_in1</span> <span class="pre">==</span> <span class="pre">channels_in2</span> <span class="pre">==</span> <span class="pre">channels_out</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">'uvw'</span></code>: Fully connected product.</p></li>
</ul>
</p></li>
<li><p><strong>path_norm</strong> (<em>bool</em><em>, </em><em>default=True</em>) – Whether to apply path normalization to the weights.</p></li>
<li><p><strong>channel_norm</strong> (<em>bool</em><em>, </em><em>default=False</em>) – Whether to apply channel normalization (specific to <code class="docutils literal notranslate"><span class="pre">'uvw'</span></code> mode).
Divides weights by <span class="math notranslate nohighlight">\(\sqrt{\text{channels_in1} \times \text{channels_in2}}\)</span>.</p></li>
<li><p><strong>path</strong> (<em>list</em><em>, </em><em>optional</em>) – Manually specify the coupling paths.
If <code class="docutils literal notranslate"><span class="pre">None</span></code>, all allowed paths are used.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="equitorch.nn.TensorProduct.weight">
<span class="sig-name descname"><span class="pre">weight</span></span><a class="headerlink" href="#equitorch.nn.TensorProduct.weight" title="Link to this definition"></a></dt>
<dd><p>The learnable weights of the module if <code class="docutils literal notranslate"><span class="pre">internal_weights=True</span></code>.
Shape depends on <code class="docutils literal notranslate"><span class="pre">feature_mode</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.nn.Parameter or None</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="equitorch.nn.TensorProduct.tp_info_forward">
<span class="sig-name descname"><span class="pre">tp_info_forward</span></span><a class="headerlink" href="#equitorch.nn.TensorProduct.tp_info_forward" title="Link to this definition"></a></dt>
<dd><p>Constant information for the forward pass computation.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="equitorch.structs.html#equitorch.structs.TensorProductInfo" title="equitorch.structs.TensorProductInfo">TensorProductInfo</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="equitorch.nn.TensorProduct.tp_info_backward1">
<span class="sig-name descname"><span class="pre">tp_info_backward1</span></span><a class="headerlink" href="#equitorch.nn.TensorProduct.tp_info_backward1" title="Link to this definition"></a></dt>
<dd><p>Constant information for the backward pass w.r.t. input1.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="equitorch.structs.html#equitorch.structs.TensorProductInfo" title="equitorch.structs.TensorProductInfo">TensorProductInfo</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="equitorch.nn.TensorProduct.tp_info_backward2">
<span class="sig-name descname"><span class="pre">tp_info_backward2</span></span><a class="headerlink" href="#equitorch.nn.TensorProduct.tp_info_backward2" title="Link to this definition"></a></dt>
<dd><p>Constant information for the backward pass w.r.t. input2.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="equitorch.structs.html#equitorch.structs.TensorProductInfo" title="equitorch.structs.TensorProductInfo">TensorProductInfo</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="equitorch.nn.TensorProduct.num_paths">
<span class="sig-name descname"><span class="pre">num_paths</span></span><a class="headerlink" href="#equitorch.nn.TensorProduct.num_paths" title="Link to this definition"></a></dt>
<dd><p>Number of coupling paths determined by the irreps.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="equitorch.nn.TensorProduct.weight_numel">
<span class="sig-name descname"><span class="pre">weight_numel</span></span><a class="headerlink" href="#equitorch.nn.TensorProduct.weight_numel" title="Link to this definition"></a></dt>
<dd><p>Total number of elements in the weight tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="id18">
<span class="sig-name descname"><span class="pre">tp_info_forward</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="equitorch.structs.html#equitorch.structs.TensorProductInfo" title="equitorch.structs.TensorProductInfo"><span class="pre">TensorProductInfo</span></a></em><a class="headerlink" href="#id18" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="id19">
<span class="sig-name descname"><span class="pre">tp_info_backward1</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="equitorch.structs.html#equitorch.structs.TensorProductInfo" title="equitorch.structs.TensorProductInfo"><span class="pre">TensorProductInfo</span></a></em><a class="headerlink" href="#id19" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="id20">
<span class="sig-name descname"><span class="pre">tp_info_backward2</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="equitorch.structs.html#equitorch.structs.TensorProductInfo" title="equitorch.structs.TensorProductInfo"><span class="pre">TensorProductInfo</span></a></em><a class="headerlink" href="#id20" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="equitorch.nn.TensorProduct.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#equitorch.nn.TensorProduct.training" title="Link to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="equitorch.nn.TensorDot">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">equitorch.nn.</span></span><span class="sig-name descname"><span class="pre">TensorDot</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">irreps</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">feature_mode</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scaled</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#equitorch.nn.TensorDot" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Computes the equivariant irrep-wise dot product of two feature tensors.</p>
<p>Supports two main modes controlled by <code class="docutils literal notranslate"><span class="pre">feature_mode</span></code>:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">'uv'</span></code>: Channel-cartesian dot product.</p>
<ul>
<li><p>Input1 shape: <code class="docutils literal notranslate"><span class="pre">(...,</span> <span class="pre">irreps.dim,</span> <span class="pre">channels1)</span></code></p></li>
<li><p>Input2 shape: <code class="docutils literal notranslate"><span class="pre">(...,</span> <span class="pre">irreps.dim,</span> <span class="pre">channels2)</span></code></p></li>
<li><p>Output shape: <code class="docutils literal notranslate"><span class="pre">(...,</span> <span class="pre">len(irreps),</span> <span class="pre">channels1,</span> <span class="pre">channels2)</span></code></p></li>
</ul>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">'uu'</span></code>: Channel-wise dot product. Sums over the channel dimension after the dot product.</p>
<ul>
<li><p>Input1 shape: <code class="docutils literal notranslate"><span class="pre">(...,</span> <span class="pre">irreps.dim,</span> <span class="pre">channels)</span></code></p></li>
<li><p>Input2 shape: <code class="docutils literal notranslate"><span class="pre">(...,</span> <span class="pre">irreps.dim,</span> <span class="pre">channels)</span></code></p></li>
<li><p>Output shape: <code class="docutils literal notranslate"><span class="pre">(...,</span> <span class="pre">len(irreps),</span> <span class="pre">channels)</span></code></p></li>
</ul>
</li>
</ul>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>irreps</strong> (<a class="reference internal" href="equitorch.irreps.html#equitorch.irreps.Irreps" title="equitorch.irreps.Irreps"><em>Irreps</em></a><em> or </em><em>str</em>) – Irreducible representations of the input tensors.
Both inputs must have the same irreps.</p></li>
<li><p><strong>feature_mode</strong> (<em>{'uv'</em><em>, </em><em>'uu'}</em>) – <p>Controls how the channel dimension is handled:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">'uv'</span></code>: Channel-cartesian dot product.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">'uu'</span></code>: Channel-wise dot product.</p></li>
</ul>
</p></li>
<li><p><strong>scaled</strong> (<em>bool</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, scales the dot product by <span class="math notranslate nohighlight">\(1 / \sqrt{\text{irrep.dim}}\)</span>.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="equitorch.nn.TensorDot.irreps_info">
<span class="sig-name descname"><span class="pre">irreps_info</span></span><a class="headerlink" href="#equitorch.nn.TensorDot.irreps_info" title="Link to this definition"></a></dt>
<dd><p>Constant information about the input irreps.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="equitorch.structs.html#equitorch.structs.IrrepsInfo" title="equitorch.structs.IrrepsInfo">IrrepsInfo</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="id21">
<span class="sig-name descname"><span class="pre">irreps_info</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="equitorch.structs.html#equitorch.structs.IrrepsInfo" title="equitorch.structs.IrrepsInfo"><span class="pre">IrrepsInfo</span></a></em><a class="headerlink" href="#id21" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="equitorch.nn.TensorDot.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#equitorch.nn.TensorDot.training" title="Link to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="equitorch.nn.SparseWignerRotation">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">equitorch.nn.</span></span><span class="sig-name descname"><span class="pre">SparseWignerRotation</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">irreps</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="equitorch.irreps.html#equitorch.irreps.Irreps" title="equitorch.irreps.Irreps"><span class="pre">Irreps</span></a></span></em><span class="sig-paren">)</span><a class="headerlink" href="#equitorch.nn.SparseWignerRotation" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Applies a sparse Wigner D-matrix rotation to input features.</p>
<p>This module computes the rotation based on Euler angles (<span class="math notranslate nohighlight">\(\alpha, \beta, \gamma\)</span>)
provided as precomputed sin/cos tensors. It utilizes sparse matrix operations for the rotation.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>It is currently suggested to use <a class="reference internal" href="#equitorch.nn.DenseWignerRotation" title="equitorch.nn.DenseWignerRotation"><code class="xref py py-class docutils literal notranslate"><span class="pre">DenseWignerRotation</span></code></a> or <a class="reference internal" href="#equitorch.nn.WignerD" title="equitorch.nn.WignerD"><code class="xref py py-class docutils literal notranslate"><span class="pre">WignerD</span></code></a>
for applying rotations, at least when gradients with respect to angles are not required.</p>
</div>
<p>The Wigner D-matrix <span class="math notranslate nohighlight">\(D^l_{m'm}(R)\)</span> transforms spherical tensors under rotation <span class="math notranslate nohighlight">\(R\)</span>.
This module applies such transformations for a given <a class="reference internal" href="equitorch.irreps.html#equitorch.irreps.Irreps" title="equitorch.irreps.Irreps"><code class="xref py py-class docutils literal notranslate"><span class="pre">Irreps</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>irreps</strong> (<a class="reference internal" href="equitorch.irreps.html#equitorch.irreps.Irreps" title="equitorch.irreps.Irreps"><em>Irreps</em></a>) – The irreducible representations defining the input and output feature space.</p>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="equitorch.nn.SparseWignerRotation.info">
<span class="sig-name descname"><span class="pre">info</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="equitorch.structs.html#equitorch.structs.WignerRotationInfo" title="equitorch.structs.WignerRotationInfo"><span class="pre">WignerRotationInfo</span></a></em><a class="headerlink" href="#equitorch.nn.SparseWignerRotation.info" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="equitorch.nn.SparseWignerRotation.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#equitorch.nn.SparseWignerRotation.training" title="Link to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="equitorch.nn.DenseWignerRotation">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">equitorch.nn.</span></span><span class="sig-name descname"><span class="pre">DenseWignerRotation</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">irreps</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="equitorch.irreps.html#equitorch.irreps.Irreps" title="equitorch.irreps.Irreps"><span class="pre">Irreps</span></a></span></em><span class="sig-paren">)</span><a class="headerlink" href="#equitorch.nn.DenseWignerRotation" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Applies a dense Wigner D-matrix rotation to input features.</p>
<p>This module takes a precomputed dense Wigner D-matrix and applies it to the input features.
The Wigner D-matrix <span class="math notranslate nohighlight">\(D(R)\)</span> itself should be computed separately, for example, using the <a class="reference internal" href="#equitorch.nn.WignerD" title="equitorch.nn.WignerD"><code class="xref py py-class docutils literal notranslate"><span class="pre">WignerD</span></code></a> module.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>irreps</strong> (<a class="reference internal" href="equitorch.irreps.html#equitorch.irreps.Irreps" title="equitorch.irreps.Irreps"><em>Irreps</em></a>) – The irreducible representations defining the input and output feature space.
This is used for validation and representation purposes.</p>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="equitorch.nn.WignerD">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">equitorch.nn.</span></span><span class="sig-name descname"><span class="pre">WignerD</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">irreps</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="equitorch.irreps.html#equitorch.irreps.Irreps" title="equitorch.irreps.Irreps"><span class="pre">Irreps</span></a></span></em><span class="sig-paren">)</span><a class="headerlink" href="#equitorch.nn.WignerD" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Computes the dense Wigner D-matrix <span class="math notranslate nohighlight">\(D(R)\)</span> for given <a class="reference internal" href="equitorch.irreps.html#equitorch.irreps.Irreps" title="equitorch.irreps.Irreps"><code class="xref py py-class docutils literal notranslate"><span class="pre">Irreps</span></code></a> and Euler angles <span class="math notranslate nohighlight">\((\alpha, \beta, \gamma)\)</span>.</p>
<p>The Wigner D-matrix is constructed based on the ZYZ Euler angle convention:</p>
<div class="math notranslate nohighlight">
\[D(\alpha, \beta, \gamma) = D_z(\alpha) D_y(\beta) D_z(\gamma)\]</div>
<p>This module caches the necessary sparse rotation information and an identity matrix
to efficiently compute the dense D-matrix using the <a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.wigner_d.wigner_d_matrix" title="equitorch.nn.functional.wigner_d.wigner_d_matrix"><code class="xref py py-func docutils literal notranslate"><span class="pre">wigner_d_matrix()</span></code></a> functional.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>irreps</strong> (<a class="reference internal" href="equitorch.irreps.html#equitorch.irreps.Irreps" title="equitorch.irreps.Irreps"><em>Irreps</em></a>) – The irreducible representations for which to compute the D-matrix.
The resulting D-matrix will have dimensions <code class="docutils literal notranslate"><span class="pre">(irreps.dim,</span> <span class="pre">irreps.dim)</span></code>.</p>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="equitorch.nn.WignerD.info">
<span class="sig-name descname"><span class="pre">info</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="equitorch.structs.html#equitorch.structs.WignerRotationInfo" title="equitorch.structs.WignerRotationInfo"><span class="pre">WignerRotationInfo</span></a></em><a class="headerlink" href="#equitorch.nn.WignerD.info" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="equitorch.nn.WignerD.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#equitorch.nn.WignerD.training" title="Link to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="equitorch.nn.AlignToZWignerD">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">equitorch.nn.</span></span><span class="sig-name descname"><span class="pre">AlignToZWignerD</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">irreps</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="equitorch.irreps.html#equitorch.irreps.Irreps" title="equitorch.irreps.Irreps"><span class="pre">Irreps</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">normalized</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1e-14</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#equitorch.nn.AlignToZWignerD" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Computes the Wigner D-matrix <span class="math notranslate nohighlight">\(D(R_{align})\)</span> that rotates a given vector <span class="math notranslate nohighlight">\(\vec{v} = (x, y, z)\)</span> onto the z-axis.</p>
<p>The rotation <span class="math notranslate nohighlight">\(R_{align}\)</span> is defined by Euler angles <span class="math notranslate nohighlight">\((0, -\theta, -\phi)\)</span>, where <span class="math notranslate nohighlight">\(\theta\)</span> and <span class="math notranslate nohighlight">\(\phi\)</span> are the
polar and azimuthal angles of the vector <span class="math notranslate nohighlight">\(\vec{v}\)</span>, respectively. This means:</p>
<div class="math notranslate nohighlight">
\[R_{align} \vec{v} = ||\vec{v}|| \hat{z}\]</div>
<p>The Wigner D-matrix is then <span class="math notranslate nohighlight">\(D(0, -\theta, -\phi)\)</span>.</p>
<p>This module caches the necessary sparse rotation information and an identity matrix.
It utilizes the <a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.wigner_d.align_to_z_wigner_d" title="equitorch.nn.functional.wigner_d.align_to_z_wigner_d"><code class="xref py py-func docutils literal notranslate"><span class="pre">align_to_z_wigner_d()</span></code></a> functional.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>irreps</strong> (<a class="reference internal" href="equitorch.irreps.html#equitorch.irreps.Irreps" title="equitorch.irreps.Irreps"><em>Irreps</em></a>) – The irreducible representations for which to compute the D-matrix.</p></li>
<li><p><strong>normalized</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether to normalize the input <code class="docutils literal notranslate"><span class="pre">xyz</span></code> vector
before calculating angles for rotation. If <code class="docutils literal notranslate"><span class="pre">True</span></code>, effectively rotates <span class="math notranslate nohighlight">\(\hat{v}\)</span>.
Defaults to <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p></li>
<li><p><strong>eps</strong> (<em>float</em><em>, </em><em>optional</em>) – Small <span class="math notranslate nohighlight">\(\epsilon\)</span> value for numerical stability in angle calculation.
Defaults to <code class="docutils literal notranslate"><span class="pre">1e-14</span></code>.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="equitorch.nn.AlignToZWignerD.info">
<span class="sig-name descname"><span class="pre">info</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="equitorch.structs.html#equitorch.structs.WignerRotationInfo" title="equitorch.structs.WignerRotationInfo"><span class="pre">WignerRotationInfo</span></a></em><a class="headerlink" href="#equitorch.nn.AlignToZWignerD.info" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="equitorch.nn.AlignToZWignerD.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#equitorch.nn.AlignToZWignerD.training" title="Link to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="equitorch.nn.PolynomialCutoff">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">equitorch.nn.</span></span><span class="sig-name descname"><span class="pre">PolynomialCutoff</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">r_max</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">r_min</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">p</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">6</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#equitorch.nn.PolynomialCutoff" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Polynomial cutoff, as proposed in <a class="reference external" href="https://arxiv.org/abs/2003.03123">DimeNet</a>.</p>
<p>The polynomial cutoff function is defined as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}f(r) = \begin{cases}
1, &amp; r &lt; r_{\text{min}} \\
1 - \frac{(p+1)(p+2)}{2}u^p + p(p+2)u^{p+1} - \frac{p(p+1)}{2}u^{p+2}, &amp; r_{\text{min}} \leq r \leq r_{\text{max}} \\
0, &amp; r &gt; r_{\text{max}}
\end{cases}\end{split}\]</div>
<p>where (u = frac{r - r_{text{min}}}{r_{text{max}} - r_{text{min}}}) and (r) is the input distance.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>r_max</strong> (<em>float</em>) – The cutoff distance (r_{text{max}}) where the function reaches zero.</p></li>
<li><p><strong>r_min</strong> (<em>float</em><em>, </em><em>optional</em>) – The starting distance (r_{text{min}}) where the function begins to decrease from 1.
Must be less than or equal to <code class="docutils literal notranslate"><span class="pre">r_max</span></code>. Defaults to <code class="docutils literal notranslate"><span class="pre">0.</span></code>.</p></li>
<li><p><strong>p</strong> (<em>float</em><em>, </em><em>optional</em>) – The power parameter (p) controlling the smoothness of the cutoff.
Must be greater than or equal to <code class="docutils literal notranslate"><span class="pre">2.0</span></code>. Defaults to <code class="docutils literal notranslate"><span class="pre">6.</span></code>.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="equitorch.nn.CosineCutoff">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">equitorch.nn.</span></span><span class="sig-name descname"><span class="pre">CosineCutoff</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">r_max</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">r_min</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#equitorch.nn.CosineCutoff" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>The cosine cutoff function.</p>
<p>The cosine cutoff function is defined as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}f(r) = \begin{cases}
1, &amp; r &lt; r_{\text{min}} \\
\frac{1}{2}\left[1 + \cos\left(\pi \cdot u\right)\right], &amp; r_{\text{min}} \leq r \leq r_{\text{max}} \\
0, &amp; r &gt; r_{\text{max}}
\end{cases}\end{split}\]</div>
<p>where (u = frac{r - r_{text{min}}}{r_{text{max}} - r_{text{min}}}) and (r) is the input distance.</p>
<p>This cutoff function smoothly decreases from 1 to 0 in the range
[r_{text{min}}, r_{text{max}}] using a cosine function.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>r_max</strong> (<em>float</em>) – The cutoff distance (r_{text{max}}) where the function reaches zero.</p></li>
<li><p><strong>r_min</strong> (<em>float</em><em>, </em><em>optional</em>) – The starting distance (r_{text{min}}) where the function begins to decrease from 1.
Must be less than <code class="docutils literal notranslate"><span class="pre">r_max</span></code>. Defaults to <code class="docutils literal notranslate"><span class="pre">0.</span></code>.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="equitorch.nn.MollifierCutoff">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">equitorch.nn.</span></span><span class="sig-name descname"><span class="pre">MollifierCutoff</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">r_max</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">r_min</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1e-07</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#equitorch.nn.MollifierCutoff" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>The mollifier cutoff function.</p>
<p>The mollifier cutoff function is defined as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}f(r) = \begin{cases}
1, &amp; r &lt; r_{\text{min}} \\
\exp\left(1 - \frac{1}{1 - u^2 + \epsilon}\right), &amp; r_{\text{min}} \leq r \leq r_{\text{max}} \\
0, &amp; r &gt; r_{\text{max}}
\end{cases}\end{split}\]</div>
<p>where (u = frac{r - r_{text{min}}}{r_{text{max}} - r_{text{min}}}) and (r) is the input distance.</p>
<p>This cutoff function smoothly decreases from 1 to 0 in the range
[r_{text{min}}, r_{text{max}}] using a mollifier (bump) function.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>r_max</strong> (<em>float</em>) – The cutoff distance (r_{text{max}}) where the function reaches zero.</p></li>
<li><p><strong>r_min</strong> (<em>float</em><em>, </em><em>optional</em>) – The starting distance (r_{text{min}}) where the function begins to decrease from 1.
Must be less than <code class="docutils literal notranslate"><span class="pre">r_max</span></code>. Defaults to <code class="docutils literal notranslate"><span class="pre">0.</span></code>.</p></li>
<li><p><strong>eps</strong> (<em>float</em><em>, </em><em>optional</em>) – Small epsilon value (epsilon) to prevent division by zero.
Defaults to <code class="docutils literal notranslate"><span class="pre">1e-7</span></code>.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="equitorch.nn.Gate">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">equitorch.nn.</span></span><span class="sig-name descname"><span class="pre">Gate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">irreps</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="equitorch.irreps.html#equitorch.irreps.Irreps" title="equitorch.irreps.Irreps"><span class="pre">Irreps</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">irrep_wise</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#equitorch.nn.Gate" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Applies element-wise gates to equivariant features.</p>
<p>This module implements gating nonlinearities for features represented by <a class="reference internal" href="equitorch.irreps.html#equitorch.irreps.Irreps" title="equitorch.irreps.Irreps"><code class="xref py py-class docutils literal notranslate"><span class="pre">Irreps</span></code></a>.
It can operate in two primary modes based on how the gate values are provided:</p>
<ol class="arabic simple">
<li><p><strong>Separate Gates (``gate`` argument provided):</strong>
The module takes two distinct inputs: <code class="docutils literal notranslate"><span class="pre">input</span></code> (the features to be gated) and <code class="docutils literal notranslate"><span class="pre">gate</span></code> (the gate scalars).</p>
<ul class="simple">
<li><p>If <code class="docutils literal notranslate"><span class="pre">irrep_wise=True</span></code> (default): Each gate scalar in the <code class="docutils literal notranslate"><span class="pre">gate</span></code> tensor is applied to its corresponding
irrep block within the <code class="docutils literal notranslate"><span class="pre">input</span></code> features. The <code class="docutils literal notranslate"><span class="pre">gate</span></code> tensor should have a shape compatible with
<code class="docutils literal notranslate"><span class="pre">(...,</span> <span class="pre">num_gates,</span> <span class="pre">channels)</span></code>, where <code class="docutils literal notranslate"><span class="pre">num_gates</span></code> is the number of irreps in <code class="docutils literal notranslate"><span class="pre">irreps</span></code>.</p></li>
<li><p>If <code class="docutils literal notranslate"><span class="pre">irrep_wise=False</span></code>: A single gate scalar (or a set of scalars broadcastable across irreps)
is applied to all irrep blocks in the <code class="docutils literal notranslate"><span class="pre">input</span></code> features. The <code class="docutils literal notranslate"><span class="pre">gate</span></code> tensor should have a shape
compatible with <code class="docutils literal notranslate"><span class="pre">(...,</span> <span class="pre">1,</span> <span class="pre">channels)</span></code>.</p></li>
</ul>
</li>
<li><p><strong>Concatenated Input (``gate=None``):</strong>
The module takes a single <code class="docutils literal notranslate"><span class="pre">input</span></code> tensor where the features and their corresponding gate scalars
are concatenated along the spherical dimension (<code class="docutils literal notranslate"><span class="pre">dim=-2</span></code>). The last <code class="docutils literal notranslate"><span class="pre">num_gates</span></code> slices along this
dimension are interpreted as the gate scalars. The <code class="docutils literal notranslate"><span class="pre">input</span></code> tensor shape is expected to be
<code class="docutils literal notranslate"><span class="pre">(...,</span> <span class="pre">irreps.dim</span> <span class="pre">+</span> <span class="pre">num_gates,</span> <span class="pre">channels)</span></code>.
The module internally splits this tensor into features and gates, optionally applies an activation
function to the extracted gates, and then proceeds with the gating operation as described in mode 1.</p></li>
</ol>
<p>An optional activation function can be applied to the gate scalars before they modulate the features.</p>
<p class="rubric">Example</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">equitorch.irreps</span><span class="w"> </span><span class="kn">import</span> <span class="n">Irreps</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">equitorch.nn</span><span class="w"> </span><span class="kn">import</span> <span class="n">Gate</span>

<span class="n">irreps</span> <span class="o">=</span> <span class="n">Irreps</span><span class="p">(</span><span class="s2">&quot;1x0e + 2x1o&quot;</span><span class="p">)</span> <span class="c1"># Example: one scalar, two l=1 odd irreps</span>
<span class="n">gate_module</span> <span class="o">=</span> <span class="n">Gate</span><span class="p">(</span><span class="n">irreps</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Tanh</span><span class="p">())</span>

<span class="n">batch_size</span><span class="p">,</span> <span class="n">channels</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">8</span>
<span class="n">num_gates</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">irreps</span><span class="p">)</span> <span class="c1"># This will be 2 for the example irreps</span>

<span class="c1"># Mode 1: Separate input and gate tensors (irrep_wise=True)</span>
<span class="n">features</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">irreps</span><span class="o">.</span><span class="n">dim</span><span class="p">,</span> <span class="n">channels</span><span class="p">)</span>
<span class="n">gates</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_gates</span><span class="p">,</span> <span class="n">channels</span><span class="p">)</span>
<span class="n">output_separate</span> <span class="o">=</span> <span class="n">gate_module</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">gates</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Output shape (separate gates): </span><span class="si">{</span><span class="n">output_separate</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Mode 2: Concatenated input tensor</span>
<span class="c1"># irreps.dim for &quot;1x0e + 2x1o&quot; is 1*1 + 2*3 = 7</span>
<span class="c1"># num_gates is 2</span>
<span class="n">concatenated_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">irreps</span><span class="o">.</span><span class="n">dim</span> <span class="o">+</span> <span class="n">num_gates</span><span class="p">,</span> <span class="n">channels</span><span class="p">)</span>
<span class="n">output_concatenated</span> <span class="o">=</span> <span class="n">gate_module</span><span class="p">(</span><span class="n">concatenated_input</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Output shape (concatenated input): </span><span class="si">{</span><span class="n">output_concatenated</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>irreps</strong> (<a class="reference internal" href="equitorch.irreps.html#equitorch.irreps.Irreps" title="equitorch.irreps.Irreps"><em>Irreps</em></a>) – The irreducible representations of the feature part of the input tensor
(i.e., the part that will be gated).</p></li>
<li><p><strong>activation</strong> (<em>torch.nn.Module</em><em>, </em><em>optional</em>) – An activation function to be applied to the
gate scalars before the gating operation. Defaults to <code class="docutils literal notranslate"><span class="pre">None</span></code> (no activation).</p></li>
<li><p><strong>irrep_wise</strong> (<em>bool</em><em>, </em><em>optional</em>) – Determines how gates are applied.
If <code class="docutils literal notranslate"><span class="pre">True</span></code> (default), gates are applied irrep-by-irrep. This requires the <code class="docutils literal notranslate"><span class="pre">gate</span></code>
tensor (if provided separately) to have a shape like <code class="docutils literal notranslate"><span class="pre">(...,</span> <span class="pre">num_gates,</span> <span class="pre">channels)</span></code>.
If <code class="docutils literal notranslate"><span class="pre">False</span></code>, a single gate (or a broadcastable set) is applied across all irreps.
This requires the <code class="docutils literal notranslate"><span class="pre">gate</span></code> tensor (if provided separately) to have a shape like
<code class="docutils literal notranslate"><span class="pre">(...,</span> <span class="pre">1,</span> <span class="pre">channels)</span></code>. <code class="docutils literal notranslate"><span class="pre">num_gates</span></code> corresponds to <code class="docutils literal notranslate"><span class="pre">len(irreps)</span></code>.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="equitorch.nn.Gate.irreps_info">
<span class="sig-name descname"><span class="pre">irreps_info</span></span><a class="headerlink" href="#equitorch.nn.Gate.irreps_info" title="Link to this definition"></a></dt>
<dd><p>Cached information about the input feature irreps, used for efficient gating.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="equitorch.structs.html#equitorch.structs.IrrepsInfo" title="equitorch.structs.IrrepsInfo">IrrepsInfo</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="equitorch.nn.Gate.num_gates">
<span class="sig-name descname"><span class="pre">num_gates</span></span><a class="headerlink" href="#equitorch.nn.Gate.num_gates" title="Link to this definition"></a></dt>
<dd><p>The number of distinct gate scalars, equal to <code class="docutils literal notranslate"><span class="pre">len(irreps)</span></code>.
This dictates the expected size of the gate dimension in the <code class="docutils literal notranslate"><span class="pre">gate</span></code> tensor
or the number of gate slices in a concatenated input.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="id23">
<span class="sig-name descname"><span class="pre">irreps_info</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="equitorch.structs.html#equitorch.structs.IrrepsInfo" title="equitorch.structs.IrrepsInfo"><span class="pre">IrrepsInfo</span></a></em><a class="headerlink" href="#id23" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="equitorch.nn.Gate.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#equitorch.nn.Gate.training" title="Link to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="equitorch.nn.SinCos">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">equitorch.nn.</span></span><span class="sig-name descname"><span class="pre">SinCos</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">max_m</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">with_ones</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">component_normalize</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#equitorch.nn.SinCos" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Module wrapper for the <a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.angular.sincos" title="equitorch.nn.functional.angular.sincos"><code class="xref py py-func docutils literal notranslate"><span class="pre">sincos()</span></code></a> function.</p>
<p>Computes the sin/cos expansion of an angle (a):</p>
<div class="math notranslate nohighlight">
\[[1.0, \sin(a), \cos(a), \sin(2a), \cos(2a), \dots, \sin(\text{max_m} \cdot a), \cos(\text{max_m} \cdot a)]\]</div>
<p>or</p>
<div class="math notranslate nohighlight">
\[[1.0, \sqrt{2}\sin(a), \sqrt{2}\cos(a), \sqrt{2}\sin(2a), \sqrt{2}\cos(2a), \dots, \sqrt{2}\sin(\text{max_m} \cdot a), \sqrt{2}\cos(\text{max_m} \cdot a)]\]</div>
<p>The leading 1.0 is excluded if <cite>with_ones</cite> is <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>max_m</strong> (<em>int</em>) – The maximum multiple of the angle (a) to compute (sin) and (cos) for.</p></li>
<li><p><strong>with_ones</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether to include the leading 1.0 in the expansion. Defaults to <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p></li>
<li><p><strong>component_normalize</strong> (<em>bool</em><em>, </em><em>optional</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, multiplies the (sin) and (cos) values by (sqrt{2})
such that the expectation of the squared norm over ([0, 2pi]) is 1.
Defaults to <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="equitorch.nn.BesselBasis">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">equitorch.nn.</span></span><span class="sig-name descname"><span class="pre">BesselBasis</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">r_max</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_basis</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">trainable</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#equitorch.nn.BesselBasis" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="equitorch.nn.BesselBasis.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">r_max</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_basis</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">trainable</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#equitorch.nn.BesselBasis.__init__" title="Link to this definition"></a></dt>
<dd><p>Radial Bessel Basis, as proposed in DimeNet: <a class="reference external" href="https://arxiv.org/abs/2003.03123">https://arxiv.org/abs/2003.03123</a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>r_max</strong> (<em>float</em>) – Cutoff radius</p></li>
<li><p><strong>num_basis</strong> (<em>int</em>) – Number of Bessel Basis functions</p></li>
<li><p><strong>trainable</strong> (<em>bool</em>) – Train the <span class="math notranslate nohighlight">\(n \pi\)</span> part or not.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="equitorch.nn.BesselBasis.r_max">
<span class="sig-name descname"><span class="pre">r_max</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">float</span></em><a class="headerlink" href="#equitorch.nn.BesselBasis.r_max" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="equitorch.nn.BesselBasis.prefactor">
<span class="sig-name descname"><span class="pre">prefactor</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">float</span></em><a class="headerlink" href="#equitorch.nn.BesselBasis.prefactor" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="equitorch.nn.BesselBasis.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#equitorch.nn.BesselBasis.training" title="Link to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="equitorch.nn.SquaredNorm">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">equitorch.nn.</span></span><span class="sig-name descname"><span class="pre">SquaredNorm</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">irreps</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="equitorch.irreps.html#equitorch.irreps.Irreps" title="equitorch.irreps.Irreps"><span class="pre">Irreps</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">scaled</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#equitorch.nn.SquaredNorm" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Computes the squared L2 norm for each irrep block in an input tensor.</p>
<div class="math notranslate nohighlight">
\[\text{Output}_k = \sum_{m \in \text{irrep}_k} (\text{input}_{km}^2)\]</div>
<p>Optionally scales the output by <span class="math notranslate nohighlight">\(1/\text{irrep}_k\text{.dim}\)</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>irreps</strong> (<a class="reference internal" href="equitorch.irreps.html#equitorch.irreps.Irreps" title="equitorch.irreps.Irreps"><em>Irreps</em></a>) – Irreducible representations of the input tensor.</p></li>
<li><p><strong>scaled</strong> (<em>bool</em><em>, </em><em>optional</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, scales the output of each <code class="docutils literal notranslate"><span class="pre">irrep_k</span></code>
by <span class="math notranslate nohighlight">\(1/\text{irrep}_k\text{.dim}\)</span>.
Defaults to <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="equitorch.nn.SquaredNorm.irreps_info">
<span class="sig-name descname"><span class="pre">irreps_info</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="equitorch.structs.html#equitorch.structs.IrrepsInfo" title="equitorch.structs.IrrepsInfo"><span class="pre">IrrepsInfo</span></a></em><a class="headerlink" href="#equitorch.nn.SquaredNorm.irreps_info" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="equitorch.nn.SquaredNorm.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#equitorch.nn.SquaredNorm.training" title="Link to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="equitorch.nn.Norm">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">equitorch.nn.</span></span><span class="sig-name descname"><span class="pre">Norm</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">irreps</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="equitorch.irreps.html#equitorch.irreps.Irreps" title="equitorch.irreps.Irreps"><span class="pre">Irreps</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">scaled</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#equitorch.nn.Norm" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Computes the L2 norm for each irrep block in an input tensor.</p>
<div class="math notranslate nohighlight">
\[\text{Output}_k = \sqrt{\sum_{m \in \text{irrep}_k} (\text{input}_{km}^2)}\]</div>
<p>Optionally scales the output by <span class="math notranslate nohighlight">\(\sqrt{1/\text{irrep}_k\text{.dim}}\)</span>.
Gradient at zero vector is zero.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>irreps</strong> (<a class="reference internal" href="equitorch.irreps.html#equitorch.irreps.Irreps" title="equitorch.irreps.Irreps"><em>Irreps</em></a>) – Irreducible representations of the input tensor.</p></li>
<li><p><strong>scaled</strong> (<em>bool</em><em>, </em><em>optional</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, scales the output of each <code class="docutils literal notranslate"><span class="pre">irrep_k</span></code>
by <span class="math notranslate nohighlight">\(\sqrt{1/\text{irrep}_k\text{.dim}}\)</span>.
Defaults to <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="equitorch.nn.Norm.irreps_info">
<span class="sig-name descname"><span class="pre">irreps_info</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="equitorch.structs.html#equitorch.structs.IrrepsInfo" title="equitorch.structs.IrrepsInfo"><span class="pre">IrrepsInfo</span></a></em><a class="headerlink" href="#equitorch.nn.Norm.irreps_info" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="equitorch.nn.Norm.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#equitorch.nn.Norm.training" title="Link to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="equitorch.nn.MeanSquaredNorm">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">equitorch.nn.</span></span><span class="sig-name descname"><span class="pre">MeanSquaredNorm</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">irreps</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="equitorch.irreps.html#equitorch.irreps.Irreps" title="equitorch.irreps.Irreps"><span class="pre">Irreps</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">scaled</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Literal</span><span class="p"><span class="pre">[</span></span><span class="m"><span class="pre">0</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">-</span></span><span class="m"><span class="pre">1</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="m"><span class="pre">2</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">-1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#equitorch.nn.MeanSquaredNorm" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Computes the mean of squared L2 norms over a specified dimension (batch or channel)
for each irrep block.</p>
<p>If <code class="docutils literal notranslate"><span class="pre">dim=0</span></code> (batch mean):</p>
<div class="math notranslate nohighlight">
\[\text{Output}_{ic} = \frac{1}{N} \sum_n \left( \sum_{m \in \text{irrep}_i} (\text{input}_n(im)c^2) \right)\]</div>
<p>If <code class="docutils literal notranslate"><span class="pre">dim=-1</span></code> (channel mean):</p>
<div class="math notranslate nohighlight">
\[\text{Output}_{ni} = \frac{1}{C} \sum_c \left( \sum_{m \in \text{irrep}_i} (\text{input}_n(im)c^2) \right)\]</div>
<p>Optionally scales the output by <span class="math notranslate nohighlight">\(1/\text{irrep}_i\text{.dim}\)</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>irreps</strong> (<a class="reference internal" href="equitorch.irreps.html#equitorch.irreps.Irreps" title="equitorch.irreps.Irreps"><em>Irreps</em></a>) – Irreducible representations of the input tensor.</p></li>
<li><p><strong>scaled</strong> (<em>bool</em><em>, </em><em>optional</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, scales the output of each <code class="docutils literal notranslate"><span class="pre">irrep_i</span></code>
by <span class="math notranslate nohighlight">\(1/\text{dim}_\text{irrep_i}\)</span>.
Defaults to <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p></li>
<li><p><strong>dim</strong> (<em>int</em><em>, </em><em>optional</em>) – Dimension over which to compute the mean.
Allowed values: <code class="docutils literal notranslate"><span class="pre">0</span></code> (batch), <code class="docutils literal notranslate"><span class="pre">-1</span></code> or <code class="docutils literal notranslate"><span class="pre">2</span></code> (channel).
Defaults to <code class="docutils literal notranslate"><span class="pre">-1</span></code>.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="equitorch.nn.MeanSquaredNorm.irreps_info">
<span class="sig-name descname"><span class="pre">irreps_info</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="equitorch.structs.html#equitorch.structs.IrrepsInfo" title="equitorch.structs.IrrepsInfo"><span class="pre">IrrepsInfo</span></a></em><a class="headerlink" href="#equitorch.nn.MeanSquaredNorm.irreps_info" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="equitorch.nn.MeanSquaredNorm.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#equitorch.nn.MeanSquaredNorm.training" title="Link to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="equitorch.nn.Dropout">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">equitorch.nn.</span></span><span class="sig-name descname"><span class="pre">Dropout</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">p</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">irreps</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="equitorch.irreps.html#equitorch.irreps.Irreps" title="equitorch.irreps.Irreps"><span class="pre">Irreps</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">irrep_wise</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">work_on_eval</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#equitorch.nn.Dropout" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Apply dropout to equivariant features.</p>
<p>Can operate irrep-wise or on the entire feature vector (channel-wise).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>p</strong> (<em>float</em><em>, </em><em>optional</em>) – Probability of an element to be zeroed.
Default: 0.5</p></li>
<li><p><strong>irreps</strong> (<a class="reference internal" href="equitorch.irreps.html#equitorch.irreps.Irreps" title="equitorch.irreps.Irreps"><em>Irreps</em></a><em>, </em><em>optional</em>) – Irreps of the input tensor.
Required if <cite>irrep_wise</cite> is True.
Default: None</p></li>
<li><p><strong>irrep_wise</strong> (<em>bool</em><em>, </em><em>optional</em>) – If True, applies dropout independently
for each (irrep_instance, channel).
If False, applies standard 1D dropout
treating (irreps_dim, channels) as a
single feature dimension for dropout.
Default: True</p></li>
<li><p><strong>work_on_eval</strong> (<em>bool</em><em>, </em><em>optional</em>) – If True, dropout is applied even during
evaluation. Default: False</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="equitorch.nn.AnglesToMatrix">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">equitorch.nn.</span></span><span class="sig-name descname"><span class="pre">AnglesToMatrix</span></span><a class="headerlink" href="#equitorch.nn.AnglesToMatrix" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Module to convert Euler angles (ZYZ convention) to rotation matrices.</p>
<p>The ZYZ Euler angles (alpha, beta, gamma) correspond to the rotation matrix:</p>
<div class="math notranslate nohighlight">
\[R(\alpha, \beta, \gamma) = R_z(\alpha) R_y(\beta) R_z(\gamma)\]</div>
<p>which is explicitly:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{pmatrix}
-\sin(\alpha)\sin(\gamma) + \cos(\alpha)\cos(\beta)\cos(\gamma) &amp; -\sin(\alpha)\cos(\beta)\cos(\gamma) - \sin(\gamma)\cos(\alpha) &amp; \sin(\beta)\cos(\gamma) \\
\sin(\alpha)\cos(\gamma) + \sin(\gamma)\cos(\alpha)\cos(\beta) &amp; -\sin(\alpha)\sin(\gamma)\cos(\beta) + \cos(\alpha)\cos(\gamma) &amp; \sin(\beta)\sin(\gamma) \\
-\sin(\beta)\cos(\alpha) &amp; \sin(\alpha)\sin(\beta) &amp; \cos(\beta)
\end{pmatrix}\end{split}\]</div>
<p>Wraps the functional version <a class="reference internal" href="equitorch.nn.functional.html#equitorch.nn.functional.rotations.angles_to_matrix" title="equitorch.nn.functional.rotations.angles_to_matrix"><code class="xref py py-func docutils literal notranslate"><span class="pre">angles_to_matrix()</span></code></a>.</p>
</dd></dl>

</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="equitorch.irreps.html" class="btn btn-neutral float-left" title="equitorch.irreps package" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="equitorch.nn.functional.html" class="btn btn-neutral float-right" title="equitorch.nn.functional package" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2025, Tong Wang.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>